#!/usr/bin/env python3
"""
é‡æ–°ç”Ÿæˆæ·±åº¦ç»¼è¿°è„šæœ¬
ä½¿ç”¨å·²æœ‰çš„è®ºæ–‡æ•°æ®å’ŒçŸ¥è¯†å›¾è°±ï¼Œé‡æ–°ç”Ÿæˆç»¼è¿°æŠ¥å‘Š
"""

import sys
import json
import logging
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from deep_survey_analyzer import DeepSurveyAnalyzer
from knowledge_graph import CitationGraph

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DateTimeEncoder(json.JSONEncoder):
    """è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†datetimeå¯¹è±¡"""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)


def generate_markdown_from_result(result: dict, json_path: Path) -> str:
    """
    ä»åˆ†æç»“æœç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š

    Args:
        result: æ·±åº¦ç»¼è¿°åˆ†æç»“æœå­—å…¸
        json_path: JSONæ–‡ä»¶è·¯å¾„

    Returns:
        ç”Ÿæˆçš„Markdownæ–‡ä»¶è·¯å¾„
    """
    topic = result['topic']
    evolutionary_paths = result['evolutionary_paths']
    survey_report = result.get('survey_report', {})

    # ç”ŸæˆMarkdownå†…å®¹
    md_lines = []

    # æ ‡é¢˜
    md_lines.append(f"# Deep Survey: {topic}")
    md_lines.append("")
    md_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {result.get('timestamp', '')}")
    md_lines.append("")

    # ç»¼è¿°æ‘˜è¦ï¼ˆæ¥è‡ª survey_reportï¼‰
    if survey_report and survey_report.get('abstract'):
        md_lines.append("## ç»¼è¿°æ‘˜è¦")
        md_lines.append("")
        md_lines.append(survey_report['abstract'])
        md_lines.append("")

    # ç»Ÿè®¡ä¿¡æ¯
    md_lines.append("## ç»Ÿè®¡æ¦‚è§ˆ")
    md_lines.append("")
    md_lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
    md_lines.append("|------|------|")
    md_lines.append(f"| æ¼”åŒ–è·¯å¾„æ•° | {len(evolutionary_paths)} |")

    # è®¡ç®—æ€»è®ºæ–‡æ•°ï¼ˆå»é‡ï¼‰
    all_paper_ids = set()
    for path in evolutionary_paths:
        for paper in path.get('papers', []):
            all_paper_ids.add(paper['paper_id'])
    total_papers = len(all_paper_ids)
    md_lines.append(f"| ç›¸å…³è®ºæ–‡æ€»æ•° | {total_papers} |")

    # è®¡ç®—æ€»å¼•ç”¨æ•°
    total_citations = sum(path.get('total_citations', 0) for path in evolutionary_paths)
    md_lines.append(f"| æ€»å¼•ç”¨æ•° | {total_citations} |")

    # å‰ªæç»Ÿè®¡
    if 'pruning_stats' in result:
        stats = result['pruning_stats']
        md_lines.append(f"| åŸå§‹è®ºæ–‡æ•° | {stats.get('original_papers', 'N/A')} |")
        md_lines.append(f"| å‰ªæä¿ç•™ç‡ | {stats.get('retention_rate', 0)*100:.1f}% |")

    md_lines.append("")

    # æ¼”åŒ–è·¯å¾„è¯¦ç»†åˆ†æ
    md_lines.append("## æ¼”åŒ–è·¯å¾„è¯¦ç»†åˆ†æ")
    md_lines.append("")

    for i, path in enumerate(evolutionary_paths, 1):
        pattern_type = path.get('pattern_type', 'Unknown')
        md_lines.append(f"### è·¯å¾„ {i}: {pattern_type}")
        md_lines.append("")

        # è·¯å¾„æ ‡é¢˜
        if path.get('title'):
            md_lines.append(f"**{path['title']}**")
            md_lines.append("")

        # è·¯å¾„æ¦‚è§ˆ
        md_lines.append("#### ğŸ“Š è·¯å¾„æ¦‚è§ˆ")
        md_lines.append("")
        md_lines.append(f"- **æ¨¡å¼ç±»å‹**: {pattern_type}")
        md_lines.append(f"- **è®ºæ–‡æ•°é‡**: {len(path.get('papers', []))}")
        md_lines.append(f"- **æ€»å¼•ç”¨æ•°**: {path.get('total_citations', 0)}")
        md_lines.append(f"- **ç»“æ„**: {path.get('visual_structure', 'N/A')}")
        md_lines.append("")

        # Star ç±»å‹ç‰¹æœ‰ä¿¡æ¯
        if path.get('thread_type') == 'star':
            if path.get('center_paper'):
                md_lines.append(f"- **ä¸­å¿ƒè®ºæ–‡**: {path['center_paper']}")
            if path.get('routes_count'):
                md_lines.append(f"- **åˆ†æ”¯è·¯çº¿æ•°**: {path['routes_count']}")
            md_lines.append("")

        # æ¼”åŒ–å™äº‹
        if path.get('narrative'):
            md_lines.append("#### ğŸ“ æ¼”åŒ–å™äº‹")
            md_lines.append("")
            md_lines.append(path['narrative'])
            md_lines.append("")

        # Chain ç±»å‹ï¼šæ˜¾ç¤ºå…³ç³»é“¾
        if path.get('thread_type') == 'chain' and path.get('relation_chain'):
            md_lines.append("#### ğŸ”— æ¼”åŒ–å…³ç³»é“¾")
            md_lines.append("")
            relation_chain = path['relation_chain']
            for j, rel in enumerate(relation_chain):
                from_paper = rel.get('from_paper', {})
                to_paper = rel.get('to_paper', {})
                relation = rel.get('narrative_relation', rel.get('relation_type', 'Unknown'))

                from_title = from_paper.get('title', 'Unknown')[:50]
                to_title = to_paper.get('title', 'Unknown')[:50]
                from_year = from_paper.get('year', '?')
                to_year = to_paper.get('year', '?')

                md_lines.append(f"{j+1}. **{from_title}** ({from_year}) --{relation}--> **{to_title}** ({to_year})")
            md_lines.append("")

        # Star ç±»å‹ï¼šæ˜¾ç¤ºå„æ¡è·¯çº¿
        if path.get('thread_type') == 'star' and path.get('routes'):
            md_lines.append("#### ğŸŒŸ åˆ†æ”¯è·¯çº¿")
            md_lines.append("")
            for route_idx, route in enumerate(path['routes'], 1):
                relation_type = route.get('relation_type', 'Unknown')
                route_papers = route.get('papers', [])
                md_lines.append(f"**è·¯çº¿ {route_idx}** ({relation_type}):")
                for paper in route_papers[:3]:  # åªæ˜¾ç¤ºå‰3ç¯‡
                    paper_title = paper.get('title', 'Unknown')[:60]
                    paper_year = paper.get('year', '?')
                    md_lines.append(f"  - {paper_title} ({paper_year})")
                if len(route_papers) > 3:
                    md_lines.append(f"  - ... å…± {len(route_papers)} ç¯‡è®ºæ–‡")
            md_lines.append("")

        # æ ¸å¿ƒè®ºæ–‡åˆ—è¡¨
        if path.get('papers'):
            md_lines.append("#### â­ æ ¸å¿ƒè®ºæ–‡")
            md_lines.append("")
            md_lines.append("| æ ‡é¢˜ | å¹´ä»½ | å¼•ç”¨æ•° | è®ºæ–‡ID |")
            md_lines.append("|------|------|--------|--------|")
            for paper in path['papers'][:10]:  # æœ€å¤šæ˜¾ç¤º10ç¯‡
                title = paper.get('title', 'Unknown')
                title = title[:60] + "..." if len(title) > 60 else title
                year = paper.get('year', 'N/A')
                citations = paper.get('cited_by_count', 0)
                paper_id = paper.get('paper_id', 'N/A')
                role = paper.get('role', '')
                role_marker = f" ({role})" if role else ""
                md_lines.append(f"| {title}{role_marker} | {year} | {citations} | `{paper_id}` |")
            md_lines.append("")

        md_lines.append("---")
        md_lines.append("")

    # ç ”ç©¶è¶‹åŠ¿ä¸å±•æœ›
    md_lines.append("## ç ”ç©¶è¶‹åŠ¿ä¸å±•æœ›")
    md_lines.append("")

    if len(evolutionary_paths) >= 1:
        md_lines.append("### æ¼”åŒ–æ¨¡å¼åˆ†æ")
        md_lines.append("")

        # ç»Ÿè®¡ä¸åŒç±»å‹çš„è·¯å¾„
        chain_count = sum(1 for p in evolutionary_paths if p.get('thread_type') == 'chain')
        star_count = sum(1 for p in evolutionary_paths if p.get('thread_type') == 'star')

        if chain_count > 0:
            md_lines.append(f"- **çº¿æ€§é“¾æ¡æ¨¡å¼** ({chain_count} æ¡): ä½“ç°äº†æŠ€æœ¯çš„æ¸è¿›å¼æ¼”åŒ–ï¼Œåç»­ç ”ç©¶é€æ­¥å…‹æœå‰äººå±€é™")
        if star_count > 0:
            md_lines.append(f"- **æ˜Ÿå‹çˆ†å‘æ¨¡å¼** ({star_count} ä¸ª): å±•ç¤ºäº†åŸºç¡€æ€§å·¥ä½œå¦‚ä½•æ¿€å‘å¤šä¸ªç ”ç©¶æ–¹å‘")
        md_lines.append("")

    # æ–¹æ³•è®ºè¯´æ˜
    md_lines.append("## æ–¹æ³•è®ºè¯´æ˜")
    md_lines.append("")
    md_lines.append("1. **å…³ç³»å‰ªæ**: åŸºäºè®ºæ–‡é—´çš„è¯­ä¹‰å…³ç³»ï¼ˆOvercomesã€Extendsã€Adaptsç­‰ï¼‰è¿›è¡Œå›¾è°±å‰ªæ")
    md_lines.append("2. **æ¼”åŒ–è·¯å¾„è¯†åˆ«**: è¯†åˆ«çº¿æ€§é“¾æ¡å’Œæ˜Ÿå‹çˆ†å‘ä¸¤ç§æ ¸å¿ƒæ¼”åŒ–æ¨¡å¼")
    md_lines.append("3. **LLMè¾…åŠ©å™äº‹**: ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆæµç•…çš„æ¼”åŒ–å™äº‹æè¿°")
    md_lines.append("4. **è´¨é‡ç­›é€‰**: åŸºäºå¼•ç”¨æ•°å’Œå…³ç³»å¼ºåº¦ç­›é€‰é«˜è´¨é‡æ¼”åŒ–è·¯å¾„")
    md_lines.append("")

    # ç»“è®º
    md_lines.append("## ç»“è®º")
    md_lines.append("")
    md_lines.append(f"æœ¬ç»¼è¿°é€šè¿‡çŸ¥è¯†å›¾è°±åˆ†æè¯†åˆ«å‡º {topic} é¢†åŸŸçš„ {len(evolutionary_paths)} æ¡å…³é”®æ¼”åŒ–è·¯å¾„ï¼Œ")
    md_lines.append(f"æ¶µç›– {total_papers} ç¯‡é«˜è´¨é‡è®ºæ–‡ï¼Œæ€»å¼•ç”¨æ•°è¾¾ {total_citations}ã€‚")
    md_lines.append(f"è¿™äº›æ¼”åŒ–è·¯å¾„æ­ç¤ºäº†è¯¥é¢†åŸŸçš„æŠ€æœ¯æ¼”è¿›è„‰ç»œå’Œå¤šå…ƒåŒ–å‘å±•è¶‹åŠ¿ã€‚")
    md_lines.append("")

    # ç”ŸæˆMarkdownæ–‡æœ¬
    markdown_text = "\n".join(md_lines)

    # ä¿å­˜æ–‡ä»¶
    md_file = json_path.parent / json_path.name.replace('.json', '.md')
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(markdown_text)

    return str(md_file)


def regenerate_survey(
    papers_file: str,
    graph_file: str,
    topic: str,
    output_file: str = None
):
    """
    é‡æ–°ç”Ÿæˆæ·±åº¦ç»¼è¿°

    Args:
        papers_file: è®ºæ–‡æ•°æ®JSONæ–‡ä»¶è·¯å¾„
        graph_file: çŸ¥è¯†å›¾è°±JSONæ–‡ä»¶è·¯å¾„
        topic: ç ”ç©¶ä¸»é¢˜
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    logger.info("="*80)
    logger.info("ğŸ”„ å¼€å§‹é‡æ–°ç”Ÿæˆæ·±åº¦ç»¼è¿°")
    logger.info("="*80)

    # 1. åŠ è½½è®ºæ–‡æ•°æ®
    logger.info(f"\nğŸ“‚ åŠ è½½è®ºæ–‡æ•°æ®: {papers_file}")
    with open(papers_file, 'r', encoding='utf-8') as f:
        papers_data = json.load(f)
    logger.info(f"  âœ… åŠ è½½äº† {len(papers_data)} ç¯‡è®ºæ–‡")

    # 2. åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®
    logger.info(f"\nğŸ“‚ åŠ è½½çŸ¥è¯†å›¾è°±: {graph_file}")
    with open(graph_file, 'r', encoding='utf-8') as f:
        graph_data = json.load(f)

    # 3. é‡å»ºçŸ¥è¯†å›¾è°±
    logger.info("\nğŸ”¨ é‡å»ºçŸ¥è¯†å›¾è°±...")
    citation_graph = CitationGraph()

    # æ·»åŠ èŠ‚ç‚¹
    for paper in papers_data:
        # æå–deep_analysisä¸­çš„ä¿¡æ¯
        deep_analysis = paper.get('deep_analysis', {})

        # æ·»åŠ deep_analysisä¿¡æ¯ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬å­—æ®µåï¼‰
        if deep_analysis:
            paper['rag_analysis'] = {
                'problem': deep_analysis.get('problem', ''),
                'method': deep_analysis.get('method', ''),
                'contribution': deep_analysis.get('contribution', ''),
                'limitation': deep_analysis.get('limitation', ''),
                'future_work': deep_analysis.get('future_work', ''),
            }

        citation_graph.add_paper_node(paper)

    # æ·»åŠ è¾¹
    edges_data = graph_data.get('edges', [])
    for edge in edges_data:
        # å…¼å®¹ä¸åŒçš„è¾¹æ•°æ®ç»“æ„
        source = edge.get('from') or edge.get('source')
        target = edge.get('to') or edge.get('target')
        edge_type = edge.get('edge_type') or edge.get('type', 'CITES')

        if source and target:
            citation_graph.add_citation_edge(source, target, edge_type=edge_type)

    logger.info(f"  âœ… çŸ¥è¯†å›¾è°±é‡å»ºå®Œæˆ")
    logger.info(f"     - èŠ‚ç‚¹æ•°: {citation_graph.graph.number_of_nodes()}")
    logger.info(f"     - è¾¹æ•°: {citation_graph.graph.number_of_edges()}")

    # 4. åˆå§‹åŒ–æ·±åº¦ç»¼è¿°åˆ†æå™¨ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„ç‰ˆæœ¬ï¼‰
    logger.info("\nğŸ”§ åˆå§‹åŒ–æ·±åº¦ç»¼è¿°åˆ†æå™¨...")
    config = {
        'embedding_model': 'all-MiniLM-L6-v2',
        'use_modelscope': True,
        'relevance_threshold': 0.3,  # ä½¿ç”¨æ–°çš„ç›¸å…³æ€§è¿‡æ»¤åŠŸèƒ½
        'llm_config_path': './config/config.yaml'
    }

    analyzer = DeepSurveyAnalyzer(config=config)
    logger.info("  âœ… åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    # 5. æ‰§è¡Œæ·±åº¦ç»¼è¿°åˆ†æ
    logger.info(f"\nğŸ“Š å¼€å§‹åˆ†æä¸»é¢˜: '{topic}'")
    logger.info("="*80)

    try:
        result = analyzer.analyze(citation_graph.graph, topic)

        logger.info("\nâœ… æ·±åº¦ç»¼è¿°åˆ†æå®Œæˆ!")
        logger.info("="*80)

        # 6. æ˜¾ç¤ºç»“æœæ‘˜è¦
        logger.info("\nğŸ“ˆ åˆ†æç»“æœæ‘˜è¦:")
        logger.info(f"  - åŸå§‹è®ºæ–‡æ•°: {result['summary']['original_papers']}")
        logger.info(f"  - å‰ªæåè®ºæ–‡æ•°: {result['summary']['pruned_papers']}")
        logger.info(f"  - è¯†åˆ«æ¼”åŒ–è·¯å¾„æ•°: {result['summary']['total_threads']}")

        # æ˜¾ç¤ºå‰ªæç»Ÿè®¡
        if 'pruning_stats' in result:
            stats = result['pruning_stats']
            logger.info(f"\nğŸ“Š å›¾è°±å‰ªæç»Ÿè®¡:")
            logger.info(f"  - å‰ªææ¨¡å¼: {stats.get('pruning_mode', 'N/A')}")
            logger.info(f"  - ä¿ç•™ç‡: {stats.get('retention_rate', 0)*100:.1f}%")
            logger.info(f"  - å¼ºå…³ç³»è¾¹: {stats.get('strong_edges', 0)}")
            logger.info(f"  - å‰”é™¤å¼±å…³ç³»è¾¹: {stats.get('weak_edges_removed', 0)}")
            if stats.get('strong_components_count', 0) > 0:
                logger.info(f"  - å¼ºå…³ç³»è¿é€šåˆ†é‡æ•°: {stats['strong_components_count']}")
                logger.info(f"  - æœ€å¤§è¿é€šåˆ†é‡å¤§å°: {stats.get('largest_component_size', 0)}")

        # æ˜¾ç¤ºæ¼”åŒ–è·¯å¾„è¯¦æƒ…
        logger.info("\nğŸ“š æ¼”åŒ–è·¯å¾„è¯¦æƒ…:")
        for i, path in enumerate(result.get('evolutionary_paths', []), 1):
            logger.info(f"\n  Thread {i}: {path.get('pattern_type', 'Unknown')}")
            logger.info(f"    ğŸ“ æ ‡é¢˜: {path.get('title', 'N/A')}")
            logger.info(f"    ğŸ“„ è®ºæ–‡æ•°é‡: {len(path.get('papers', []))}")
            logger.info(f"    ğŸ“ˆ æ€»å¼•ç”¨æ•°: {path.get('total_citations', 0)}")
            logger.info(f"    ğŸ”— ç»“æ„: {path.get('visual_structure', 'N/A')}")

            # æ˜¾ç¤ºä»£è¡¨æ€§è®ºæ–‡
            papers = path.get('papers', [])[:3]
            if papers:
                logger.info(f"    â­ ä»£è¡¨æ€§è®ºæ–‡:")
                for paper in papers:
                    logger.info(f"       - {paper.get('title', 'N/A')[:60]}... ({paper.get('year', 'N/A')})")

        # æ˜¾ç¤ºç»¼è¿°æŠ¥å‘Šæ‘˜è¦
        if 'survey_report' in result and result['survey_report']:
            report = result['survey_report']
            logger.info(f"\nğŸ“– ç»¼è¿°æŠ¥å‘Š:")
            logger.info(f"  - æ ‡é¢˜: {report.get('title', 'N/A')}")
            if 'abstract' in report:
                abstract = report['abstract']
                logger.info(f"  - æ‘˜è¦: {abstract[:200]}..." if len(abstract) > 200 else f"  - æ‘˜è¦: {abstract}")

        # 7. ä¿å­˜ç»“æœ
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            topic_safe = topic.replace(' ', '_').replace('/', '_')
            output_file = f"output/deep_survey/{topic_safe}_{timestamp}.json"

        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, cls=DateTimeEncoder)

        logger.info(f"\nğŸ’¾ æ·±åº¦ç»¼è¿°æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")

        # 8. ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š
        logger.info("\nğŸ“„ ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š...")
        md_file = generate_markdown_from_result(result, output_path)
        logger.info(f"  âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜è‡³: {md_file}")

        logger.info("="*80)

        return result

    except Exception as e:
        logger.error(f"âŒ æ·±åº¦ç»¼è¿°ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    papers_file = "/home/lexy/ä¸‹è½½/CLwithRAG/KGdemo/226_papers_natural_language_processing_20251218_032842.json"
    graph_file = "/home/lexy/ä¸‹è½½/CLwithRAG/KGdemo/226_graph_data_natural_language_processing_20251218_032842.json"
    topic = "Natural Language Processing"

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(papers_file).exists():
        logger.error(f"âŒ è®ºæ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {papers_file}")
        return

    if not Path(graph_file).exists():
        logger.error(f"âŒ çŸ¥è¯†å›¾è°±æ–‡ä»¶ä¸å­˜åœ¨: {graph_file}")
        return

    # é‡æ–°ç”Ÿæˆç»¼è¿°
    try:
        result = regenerate_survey(papers_file, graph_file, topic)
        logger.info("\nğŸ‰ ç»¼è¿°é‡æ–°ç”Ÿæˆå®Œæˆ!")
    except Exception as e:
        logger.error(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
