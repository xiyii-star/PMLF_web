"""
å¢å¼ºç‰ˆæµ‹è¯•å¯è§†åŒ–ç”Ÿæˆè„šæœ¬
ä½¿ç”¨å®Œæ•´çš„ graph_data.json + deep_survey + research_ideas ç”Ÿæˆäº¤äº’å¼çŸ¥è¯†å›¾è°±
"""

import json
import logging
from pathlib import Path
from src.knowledge_graph import CitationGraph

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def load_json_file(file_path: str) -> dict:
    """åŠ è½½JSONæ–‡ä»¶"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.info(f"âœ“ æˆåŠŸåŠ è½½: {Path(file_path).name}")
        return data
    except Exception as e:
        logger.error(f"âœ— åŠ è½½å¤±è´¥ {file_path}: {e}")
        return {}


def merge_papers_data(papers_list: list, graph_data: dict) -> list:
    """
    åˆå¹¶ papers.json å’Œ graph_data.json ä¸­çš„è®ºæ–‡ä¿¡æ¯
    papers_list: ä» papers.json åŠ è½½çš„å®Œæ•´è®ºæ–‡åˆ—è¡¨
    graph_data: ä» graph_data.json åŠ è½½çš„å›¾è°±æ•°æ®ï¼ˆåŒ…å«èŠ‚ç‚¹çš„åˆ†æç»“æœï¼‰
    """
    # åˆ›å»º paper_id åˆ° paper æ•°æ®çš„æ˜ å°„
    papers_dict = {p['id']: p for p in papers_list}

    # ä» graph_data çš„ nodes ä¸­è·å–åˆ†æç»“æœ
    merged_papers = []

    if 'nodes' in graph_data:
        for node in graph_data['nodes']:
            paper_id = node.get('id', '')

            # è·å–åŸºç¡€è®ºæ–‡æ•°æ®
            if paper_id in papers_dict:
                paper = papers_dict[paper_id].copy()
            else:
                # å¦‚æœ papers_list ä¸­æ²¡æœ‰ï¼Œä½¿ç”¨ node ä¸­çš„åŸºç¡€ä¿¡æ¯
                paper = {
                    'id': paper_id,
                    'title': node.get('title', 'Unknown'),
                    'authors': node.get('authors', []),
                    'year': node.get('year', 2020),
                    'cited_by_count': node.get('cited_by_count', 0),
                    'venue': node.get('venue', ''),
                    'is_open_access': node.get('is_open_access', False),
                    'is_seed': node.get('is_seed', False)
                }

            # æ·»åŠ æ¥è‡ª graph_data nodes çš„åˆ†æç»“æœ
            if 'deep_analysis' in node:
                paper['deep_analysis'] = node['deep_analysis']

            if 'rag_analysis' in node:
                paper['rag_analysis'] = node['rag_analysis']

            if 'ai_analysis' in node:
                paper['ai_analysis'] = node['ai_analysis']

            # æ·»åŠ å…¶ä»–èŠ‚ç‚¹å±æ€§
            if 'analysis_method' in node:
                paper['analysis_method'] = node['analysis_method']

            if 'sections_extracted' in node:
                paper['sections_extracted'] = node['sections_extracted']

            merged_papers.append(paper)

    logger.info(f"âœ“ åˆå¹¶è®ºæ–‡æ•°æ®: {len(merged_papers)} ç¯‡è®ºæ–‡")
    return merged_papers


def extract_citations_from_graph_data(graph_data: dict) -> list:
    """ä» graph_data.json ä¸­æå–å¼•ç”¨å…³ç³»"""
    citations = []

    if 'edges' in graph_data:
        for edge in graph_data['edges']:
            from_id = edge.get('from', '')
            to_id = edge.get('to', '')
            edge_type = edge.get('edge_type', 'Baselines')

            if from_id and to_id:
                citations.append((from_id, to_id, edge_type))

    logger.info(f"âœ“ ä»å›¾è°±æ•°æ®ä¸­æå–äº† {len(citations)} ä¸ªå¼•ç”¨å…³ç³»")
    return citations


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 70)
    logger.info("å¢å¼ºç‰ˆå¯è§†åŒ–æµ‹è¯• - ä½¿ç”¨å®Œæ•´å›¾è°±æ•°æ®")
    logger.info("=" * 70)

    # æ–‡ä»¶è·¯å¾„
    papers_file = "/home/lexy/ä¸‹è½½/CLwithRAG/KGdemo/226_papers_natural_language_processing_20251218_032842.json"
    graph_data_path = "/home/lexy/ä¸‹è½½/CLwithRAG/KGdemo/226_graph_data_natural_language_processing_20251218_032842.json"
    deep_survey_path = "/home/lexy/ä¸‹è½½/CLwithRAG/KGdemo/output/deep_survey/Natural_Language_Processing_20251221_213646.json"
    research_ideas_path = "/home/lexy/ä¸‹è½½/CLwithRAG/KGdemo/test_idea_generation_result.json"
    output_viz_path = "/home/lexy/ä¸‹è½½/CLwithRAG/KGdemo/test_visualization_papers.html"

    # 1. åŠ è½½æ•°æ®
    logger.info("\nğŸ“‚ æ­¥éª¤ 1: åŠ è½½æ•°æ®æ–‡ä»¶")
    papers_list = load_json_file(papers_file)
    graph_data = load_json_file(graph_data_path)
    deep_survey_data = load_json_file(deep_survey_path)
    research_ideas_data = load_json_file(research_ideas_path)

    if not graph_data:
        logger.error("âŒ å›¾è°±æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡º")
        return

    # å¦‚æœ papers_list ä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•è½¬æ¢
    if isinstance(papers_list, dict):
        logger.warning("papers_file æ˜¯å­—å…¸æ ¼å¼ï¼Œå°è¯•æå–è®ºæ–‡åˆ—è¡¨")
        papers_list = papers_list.get('papers', [])

    # 2. åˆå¹¶è®ºæ–‡æ•°æ®å’Œæå–å¼•ç”¨å…³ç³»
    logger.info("\nğŸ” æ­¥éª¤ 2: åˆå¹¶è®ºæ–‡æ•°æ®å’Œæå–å¼•ç”¨å…³ç³»")
    papers = merge_papers_data(papers_list, graph_data)
    citations = extract_citations_from_graph_data(graph_data)

    if not papers:
        logger.error("âŒ æœªèƒ½æå–åˆ°ä»»ä½•è®ºæ–‡æ•°æ®ï¼Œé€€å‡º")
        return

    # ç»Ÿè®¡åˆ†ææ–¹æ³•ç±»å‹
    analysis_methods = {}
    for paper in papers:
        method = paper.get('analysis_method', 'unknown')
        analysis_methods[method] = analysis_methods.get(method, 0) + 1

    # 3. åˆ›å»ºçŸ¥è¯†å›¾è°±
    logger.info("\nğŸ—ï¸  æ­¥éª¤ 3: æ„å»ºçŸ¥è¯†å›¾è°±")
    topic = graph_data.get('metadata', {}).get('topic', 'Natural Language Processing')
    citation_graph = CitationGraph(topic=topic)

    # æ„å»ºå¼•ç”¨ç½‘ç»œ
    citation_graph.build_citation_network(papers, citations)

    # 4. ç”Ÿæˆå¯è§†åŒ–
    logger.info("\nğŸ¨ æ­¥éª¤ 4: ç”Ÿæˆäº¤äº’å¼å¯è§†åŒ–")
    max_nodes = 300  # æ˜¾ç¤ºæœ€å¤š300ä¸ªèŠ‚ç‚¹ï¼ˆè¶³å¤ŸåŒ…å«æ‰€æœ‰226ç¯‡è®ºæ–‡ï¼‰

    viz_file = citation_graph.visualize_graph(
        output_path=output_viz_path,
        max_nodes=max_nodes,
        deep_survey_report=deep_survey_data,
        research_ideas=research_ideas_data
    )

    # 5. ç»Ÿè®¡å¼•ç”¨å…³ç³»ç±»å‹
    edge_types = {}
    for citation in citations:
        if len(citation) >= 3:
            edge_type = citation[2]
            edge_types[edge_type] = edge_types.get(edge_type, 0) + 1

    # 6. è¾“å‡ºè¯¦ç»†ç»Ÿè®¡
    logger.info("\n" + "=" * 70)
    logger.info("âœ… å¯è§†åŒ–ç”Ÿæˆå®Œæˆ!")
    logger.info("=" * 70)

    logger.info("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
    logger.info(f"  â€¢ è®ºæ–‡æ€»æ•°: {len(papers)}")
    logger.info(f"  â€¢ ç§å­è®ºæ–‡: {sum(1 for p in papers if p.get('is_seed', False))}")
    logger.info(f"  â€¢ å¼•ç”¨å…³ç³»: {len(citations)}")
    logger.info(f"  â€¢ æ˜¾ç¤ºèŠ‚ç‚¹: {min(len(papers), max_nodes)}")

    logger.info("\nğŸ”¬ åˆ†ææ–¹æ³•åˆ†å¸ƒ:")
    for method, count in sorted(analysis_methods.items(), key=lambda x: x[1], reverse=True):
        logger.info(f"  â€¢ {method}: {count} ç¯‡")

    logger.info("\nğŸ”— å¼•ç”¨å…³ç³»ç±»å‹åˆ†å¸ƒ:")
    for edge_type, count in sorted(edge_types.items(), key=lambda x: x[1], reverse=True):
        logger.info(f"  â€¢ {edge_type}: {count} ä¸ª")

    logger.info("\nğŸ“ ç»¼è¿°ä¸åˆ›æ„:")
    logger.info(f"  â€¢ ç ”ç©¶æ¼”åŒ–è·¯å¾„: {len(deep_survey_data.get('evolutionary_paths', []))}")
    logger.info(f"  â€¢ å…³é”®è½¬æŠ˜è®ºæ–‡: {len(deep_survey_data.get('pivotal_papers', []))}")
    logger.info(f"  â€¢ ç§‘ç ”åˆ›æ„æ€»æ•°: {research_ideas_data.get('total_ideas', 0)}")
    logger.info(f"  â€¢ å¯è¡Œåˆ›æ„æ•°é‡: {research_ideas_data.get('successful_ideas', 0)}")

    logger.info("\nğŸ“„ è¾“å‡ºæ–‡ä»¶:")
    logger.info(f"  {viz_file}")

    logger.info("\nğŸ’¡ ä½¿ç”¨æç¤º:")
    logger.info("  1. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ HTML æ–‡ä»¶æŸ¥çœ‹äº¤äº’å¼å¯è§†åŒ–")
    logger.info("  2. ç‚¹å‡»èŠ‚ç‚¹æŸ¥çœ‹è®ºæ–‡è¯¦æƒ…å’Œ RAG åˆ†æç»“æœ")
    logger.info("  3. åˆ‡æ¢æ ‡ç­¾é¡µæŸ¥çœ‹æ·±åº¦ç»¼è¿°å’Œç§‘ç ”åˆ›æ„")
    logger.info("=" * 70)


if __name__ == "__main__":
    main()
