"""
æµ‹è¯•ç§‘ç ”åˆ›æ„ç”ŸæˆåŠŸèƒ½
ä½¿ç”¨å·²ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶æµ‹è¯• research_idea_generator with survey.py
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional
import networkx as nx
import yaml

# é…ç½®æ—¥å¿—ï¼ˆéœ€è¦åœ¨å…¶ä»–å‡½æ•°ä¹‹å‰ï¼‰
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥ research_idea_generator with survey.py
import importlib.util
_research_idea_spec = importlib.util.spec_from_file_location(
    "research_idea_generator_with_survey",
    str(Path(__file__).parent / "src" / "research_idea_generator with survey.py")
)
if _research_idea_spec and _research_idea_spec.loader:
    _research_idea_module = importlib.util.module_from_spec(_research_idea_spec)
    _research_idea_spec.loader.exec_module(_research_idea_module)
    ResearchIdeaGenerator = _research_idea_module.ResearchIdeaGenerator
else:
    raise ImportError("æ— æ³•å¯¼å…¥ research_idea_generator with survey.py")


def load_full_config(config_path: str = './config/config.yaml') -> Dict:
    """
    ä»YAMLæ–‡ä»¶åŠ è½½å®Œæ•´é…ç½®
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        å®Œæ•´é…ç½®å­—å…¸
    """
    config_file = Path(config_path)
    
    if not config_file.exists():
        logger.warning(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return {}
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        return config if config else {}
    except Exception as e:
        logger.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return {}


def load_papers_data(papers_file: str) -> List[Dict]:
    """
    åŠ è½½è®ºæ–‡æ•°æ®
    
    Args:
        papers_file: è®ºæ–‡æ•°æ®JSONæ–‡ä»¶è·¯å¾„
    
    Returns:
        è®ºæ–‡åˆ—è¡¨
    """
    logger.info(f"åŠ è½½è®ºæ–‡æ•°æ®: {papers_file}")
    with open(papers_file, 'r', encoding='utf-8') as f:
        papers = json.load(f)
    logger.info(f"  æˆåŠŸåŠ è½½ {len(papers)} ç¯‡è®ºæ–‡")
    return papers


def load_graph_data(graph_file: str) -> nx.DiGraph:
    """
    ä»JSONæ–‡ä»¶åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®å¹¶æ„å»ºNetworkXå›¾
    
    Args:
        graph_file: å›¾æ•°æ®JSONæ–‡ä»¶è·¯å¾„
    
    Returns:
        NetworkXæœ‰å‘å›¾
    """
    logger.info(f"åŠ è½½å›¾æ•°æ®: {graph_file}")
    
    with open(graph_file, 'r', encoding='utf-8') as f:
        graph_data = json.load(f)
    
    # åˆ›å»ºæœ‰å‘å›¾
    G = nx.DiGraph()
    
    # æ·»åŠ èŠ‚ç‚¹
    nodes = graph_data.get('nodes', [])
    logger.info(f"  æ·»åŠ  {len(nodes)} ä¸ªèŠ‚ç‚¹...")
    
    for node in nodes:
        node_id = node.get('id')
        if not node_id:
            continue
        
        # æå–èŠ‚ç‚¹å±æ€§
        node_attrs = {
            'title': node.get('title', ''),
            'year': node.get('year', 0),
            'cited_by_count': node.get('cited_by_count', 0),
            'venue': node.get('venue', ''),
            'is_open_access': node.get('is_open_access', False),
            'is_seed': node.get('is_seed', False),
        }
        
        # æå–RAGåˆ†æç»“æœï¼ˆå…¼å®¹å¤šç§æ ¼å¼ï¼‰
        deep_analysis = node.get('deep_analysis', {})
        rag_analysis = node.get('rag_analysis', {})
        
        # ä¼˜å…ˆä½¿ç”¨deep_analysisï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨rag_analysis
        if deep_analysis:
            node_attrs['rag_problem'] = deep_analysis.get('problem', '')
            node_attrs['rag_method'] = deep_analysis.get('method', '')
            node_attrs['rag_limitation'] = deep_analysis.get('limitation', '')
            node_attrs['rag_future_work'] = deep_analysis.get('future_work', '')
        elif rag_analysis:
            node_attrs['rag_problem'] = rag_analysis.get('problem', '')
            node_attrs['rag_method'] = rag_analysis.get('method', '')
            node_attrs['rag_limitation'] = rag_analysis.get('limitation', '')
            node_attrs['rag_future_work'] = rag_analysis.get('future_work', '')
        else:
            # å¦‚æœæ²¡æœ‰åˆ†æç»“æœï¼Œä½¿ç”¨ç©ºå­—ç¬¦ä¸²
            node_attrs['rag_problem'] = ''
            node_attrs['rag_method'] = ''
            node_attrs['rag_limitation'] = ''
            node_attrs['rag_future_work'] = ''
        
        G.add_node(node_id, **node_attrs)
    
    # æ·»åŠ è¾¹
    edges = graph_data.get('edges', [])
    logger.info(f"  æ·»åŠ  {len(edges)} æ¡è¾¹...")
    
    for edge in edges:
        source = edge.get('source') or edge.get('from')
        target = edge.get('target') or edge.get('to')
        edge_type = edge.get('edge_type') or edge.get('type', 'CITES')
        
        if source and target:
            G.add_edge(source, target, edge_type=edge_type, weight=1)
    
    logger.info(f"âœ… å›¾æ„å»ºå®Œæˆ: {G.number_of_nodes()} ä¸ªèŠ‚ç‚¹, {G.number_of_edges()} æ¡è¾¹")
    return G


def load_survey_data(survey_file: str) -> Dict:
    """
    åŠ è½½æ·±åº¦ç»¼è¿°æ•°æ®
    
    Args:
        survey_file: æ·±åº¦ç»¼è¿°JSONæ–‡ä»¶è·¯å¾„
    
    Returns:
        æ·±åº¦ç»¼è¿°æ•°æ®å­—å…¸
    """
    logger.info(f"åŠ è½½æ·±åº¦ç»¼è¿°æ•°æ®: {survey_file}")
    with open(survey_file, 'r', encoding='utf-8') as f:
        survey_data = json.load(f)
    
    # æå–æ¼”åŒ–è·¯å¾„
    evolutionary_paths = survey_data.get('evolutionary_paths', [])
    logger.info(f"  æå–åˆ° {len(evolutionary_paths)} æ¡æ¼”åŒ–è·¯å¾„")
    
    return survey_data


def test_idea_generation(
    papers_file: str,
    graph_file: str,
    survey_file: str,
    config: Optional[Dict] = None
) -> Dict:
    """
    æµ‹è¯•ç§‘ç ”åˆ›æ„ç”Ÿæˆ
    
    Args:
        papers_file: è®ºæ–‡æ•°æ®JSONæ–‡ä»¶è·¯å¾„
        graph_file: å›¾æ•°æ®JSONæ–‡ä»¶è·¯å¾„
        survey_file: æ·±åº¦ç»¼è¿°JSONæ–‡ä»¶è·¯å¾„
        config: é…ç½®å‚æ•°å­—å…¸ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        ç”Ÿæˆç»“æœå­—å…¸
    """
    logger.info("=" * 80)
    logger.info("å¼€å§‹æµ‹è¯•ç§‘ç ”åˆ›æ„ç”Ÿæˆ")
    logger.info("=" * 80)
    
    # 1. åŠ è½½æ•°æ®
    logger.info("\nğŸ“‚ æ­¥éª¤1: åŠ è½½æ•°æ®æ–‡ä»¶")
    papers = load_papers_data(papers_file)
    graph = load_graph_data(graph_file)
    survey_data = load_survey_data(survey_file)
    
    # 2. æå–æ¼”åŒ–è·¯å¾„
    logger.info("\nğŸ” æ­¥éª¤2: æå–æ¼”åŒ–è·¯å¾„")
    evolutionary_paths = survey_data.get('evolutionary_paths', [])
    if evolutionary_paths:
        logger.info(f"  æ‰¾åˆ° {len(evolutionary_paths)} æ¡æ¼”åŒ–è·¯å¾„")
        for i, path in enumerate(evolutionary_paths[:3], 1):
            pattern_type = path.get('pattern_type', 'Unknown')
            title = path.get('title', '')[:60]
            logger.info(f"    è·¯å¾„ {i}: {pattern_type} - {title}...")
    else:
        logger.warning("  æœªæ‰¾åˆ°æ¼”åŒ–è·¯å¾„")
    
    # 3. åˆå§‹åŒ–åˆ›æ„ç”Ÿæˆå™¨
    logger.info("\nğŸ”§ æ­¥éª¤3: åˆå§‹åŒ–ç§‘ç ”åˆ›æ„ç”Ÿæˆå™¨")
    
    # åŠ è½½é…ç½®æ–‡ä»¶ï¼ˆå¦‚æœconfigä¸ºNoneæˆ–éœ€è¦è¡¥å……é…ç½®ï¼‰
    config_path = './config/config.yaml'
    if config is None:
        # ä»é…ç½®æ–‡ä»¶åŠ è½½
        full_config = load_full_config(config_path)
        config = full_config
        logger.info(f"  ä»é…ç½®æ–‡ä»¶åŠ è½½: {config_path}")
    else:
        # åˆå¹¶ç”¨æˆ·æä¾›çš„é…ç½®å’Œé…ç½®æ–‡ä»¶
        full_config = load_full_config(config_path)
        # åˆå¹¶é…ç½®ï¼šç”¨æˆ·é…ç½®ä¼˜å…ˆï¼Œç„¶åä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼
        merged_config = full_config.copy()
        for key, value in config.items():
            if isinstance(value, dict) and key in merged_config and isinstance(merged_config[key], dict):
                merged_config[key].update(value)
            else:
                merged_config[key] = value
        config = merged_config
        logger.info(f"  åˆå¹¶ç”¨æˆ·é…ç½®å’Œé…ç½®æ–‡ä»¶: {config_path}")
    
    # æ£€æŸ¥APIå¯†é’¥
    api_key = None
    if config.get('llm', {}).get('api_key'):
        api_key = config['llm']['api_key']
    elif config.get('openai_api_key'):
        api_key = config['openai_api_key']
    else:
        import os
        api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        logger.warning("âš ï¸  æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·ç¡®ä¿:")
        logger.warning("  1. åœ¨é…ç½®æ–‡ä»¶ä¸­è®¾ç½® llm.api_key æˆ– openai_api_key")
        logger.warning("  2. æˆ–è®¾ç½®ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
        logger.warning("  3. æˆ–åœ¨configå‚æ•°ä¸­ä¼ å…¥ api_key")
    
    generator = ResearchIdeaGenerator(config=config)
    logger.info(f"  æ¨¡å‹: {config.get('llm', {}).get('model', 'gpt-4o')}")
    logger.info(f"  æœ€å¤§åˆ›æ„æ•°: {config.get('research_idea', {}).get('max_ideas', 10)}")
    if api_key:
        logger.info(f"  APIå¯†é’¥: {'*' * 20}...{api_key[-4:] if len(api_key) > 4 else '****'}")
    
    # 4. ç”Ÿæˆç§‘ç ”åˆ›æ„
    logger.info("\nğŸ’¡ æ­¥éª¤4: ç”Ÿæˆç§‘ç ”åˆ›æ„ï¼ˆä½¿ç”¨æ¼”åŒ–è·¯å¾„å­¦ä¹ ï¼‰")
    topic = survey_data.get('topic', 'Natural Language Processing')
    
    result = generator.generate_from_knowledge_graph(
        graph=graph,
        topic=topic,
        evolutionary_paths=evolutionary_paths,
        verbose=True
    )
    
    # 5. è¾“å‡ºç»“æœ
    logger.info("\n" + "=" * 80)
    logger.info("âœ… æµ‹è¯•å®Œæˆ")
    logger.info("=" * 80)
    logger.info(f"ç ”ç©¶ä¸»é¢˜: {result.get('topic', 'N/A')}")
    logger.info(f"Step 1 - æå–çš„å±€é™æ€§: {result.get('pools', {}).get('unsolved_limitations', 0)} æ¡")
    logger.info(f"Step 1 - æå–çš„æ–¹æ³•: {result.get('pools', {}).get('candidate_methods', 0)} æ¡")
    logger.info(f"Step 2 - ç”Ÿæˆçš„åˆ›æ„æ€»æ•°: {result.get('total_ideas', 0)} ä¸ª")
    logger.info(f"Step 2 - æˆåŠŸåˆ›æ„æ•°: {result.get('successful_ideas', 0)} ä¸ª")
    
    # æ˜¾ç¤ºç”Ÿæˆçš„åˆ›æ„
    ideas = result.get('ideas', [])
    if ideas:
        logger.info(f"\nğŸ“‹ ç”Ÿæˆçš„åˆ›æ„åˆ—è¡¨:")
        for i, idea in enumerate(ideas, 1):
            logger.info(f"\n  åˆ›æ„ {i}:")
            logger.info(f"    æ ‡é¢˜: {idea.get('title', 'N/A')}")
            logger.info(f"    çŠ¶æ€: {idea.get('status', 'N/A')}")
            if idea.get('modification'):
                logger.info(f"    å…³é”®ä¿®æ”¹: {idea.get('modification', '')[:100]}...")
    
    return result


def save_results(result: Dict, output_file: str) -> None:
    """
    ä¿å­˜ç”Ÿæˆç»“æœåˆ°JSONæ–‡ä»¶
    
    Args:
        result: ç”Ÿæˆç»“æœå­—å…¸
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    logger.info(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    logger.info("âœ… ç»“æœå·²ä¿å­˜")


if __name__ == "__main__":
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    base_dir = Path(__file__).parent
    papers_file = base_dir / "226_papers_natural_language_processing_20251218_032842.json"
    graph_file = base_dir / "226_graph_data_natural_language_processing_20251218_032842.json"
    survey_file = base_dir / "226_survey_Natural_Language_Processing_20251218_202205.json"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not papers_file.exists():
        logger.error(f"è®ºæ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {papers_file}")
        exit(1)
    if not graph_file.exists():
        logger.error(f"å›¾æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {graph_file}")
        exit(1)
    if not survey_file.exists():
        logger.error(f"æ·±åº¦ç»¼è¿°æ–‡ä»¶ä¸å­˜åœ¨: {survey_file}")
        exit(1)
    
    # é…ç½®å‚æ•°ï¼ˆä¼˜å…ˆä»config.yamlåŠ è½½ï¼Œè¿™é‡Œå¯ä»¥è¦†ç›–éƒ¨åˆ†é…ç½®ï¼‰
    # å¦‚æœä¸éœ€è¦è¦†ç›–ï¼Œå¯ä»¥è®¾ç½®ä¸º Noneï¼Œå®Œå…¨ä½¿ç”¨é…ç½®æ–‡ä»¶
    # config = None  # è®¾ç½®ä¸º None ä»¥å®Œå…¨ä½¿ç”¨é…ç½®æ–‡ä»¶
    # æˆ–è€…æä¾›éƒ¨åˆ†è¦†ç›–é…ç½®ï¼š
    config = {
        'llm': {
            'model': 'gpt-4o',  # è¦†ç›–é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹
            'temperature': 0.3
        },
        'research_idea': {
            'max_ideas': 20  # æœ€å¤§ç”Ÿæˆåˆ›æ„æ•°
        }
    }
    
    # è¿è¡Œæµ‹è¯•
    try:
        result = test_idea_generation(
            papers_file=str(papers_file),
            graph_file=str(graph_file),
            survey_file=str(survey_file),
            config=config
        )
        
        # ä¿å­˜ç»“æœ
        output_file = base_dir / "test_idea_generation_result.json"
        save_results(result, str(output_file))
        
        logger.info(f"\nğŸ‰ æµ‹è¯•å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}", exc_info=True)
        exit(1)


