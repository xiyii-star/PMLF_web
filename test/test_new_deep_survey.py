#!/usr/bin/env python3
"""
æµ‹è¯•æ–°ç‰ˆ DeepSurveyAnalyzer
åŸºäºå…³ç³»å‰ªæ + æ¼”åŒ–è·¯å¾„è¯†åˆ«çš„æ–¹æ³•
"""

import sys
import json
import logging
from pathlib import Path
from datetime import datetime

# æ·»åŠ srcç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent / "src"))

from deep_survey_analyzer import DeepSurveyAnalyzer
from knowledge_graph import CitationGraph

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DateTimeEncoder(json.JSONEncoder):
    """è‡ªå®šä¹‰JSONç¼–ç å™¨ï¼Œå¤„ç†datetimeå¯¹è±¡"""
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)


def generate_markdown_from_result(result: dict, json_path: Path) -> str:
    """
    ä»æ–°ç‰ˆåˆ†æç»“æœç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š

    Args:
        result: æ·±åº¦ç»¼è¿°åˆ†æç»“æœå­—å…¸ï¼ˆæ–°ç‰ˆæ ¼å¼ï¼šåŸºäºThreadï¼‰
        json_path: JSONæ–‡ä»¶è·¯å¾„

    Returns:
        ç”Ÿæˆçš„Markdownæ–‡ä»¶è·¯å¾„
    """
    topic = result['topic']
    survey_report = result['survey_report']
    threads = survey_report.get('threads', [])
    pruning_stats = result.get('pruning_stats', {})

    # ç”ŸæˆMarkdownå†…å®¹
    md_lines = []

    # æ ‡é¢˜
    md_lines.append(f"# {survey_report['title']}")
    md_lines.append("")
    md_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {result.get('timestamp', '')}")
    md_lines.append("")

    # æ‘˜è¦
    md_lines.append("## ğŸ“‹ æ‘˜è¦")
    md_lines.append("")
    md_lines.append(survey_report.get('abstract', ''))
    md_lines.append("")

    # ç»Ÿè®¡æ¦‚è§ˆ
    md_lines.append("## ğŸ“Š ç»Ÿè®¡æ¦‚è§ˆ")
    md_lines.append("")
    md_lines.append("### å›¾è°±å‰ªæç»Ÿè®¡")
    md_lines.append("")
    md_lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
    md_lines.append("|------|------|")
    md_lines.append(f"| åŸå§‹è®ºæ–‡æ•° | {pruning_stats.get('original_papers', 0)} |")
    md_lines.append(f"| Seed Papers | {pruning_stats.get('seed_papers', 0)} |")
    md_lines.append(f"| å‰ªæåè®ºæ–‡æ•° | {pruning_stats.get('pruned_papers', 0)} |")
    md_lines.append(f"| ä¿ç•™ç‡ | {pruning_stats.get('retention_rate', 0)*100:.1f}% |")
    md_lines.append(f"| å¼ºå…³ç³»è¾¹æ•° | {pruning_stats.get('strong_edges', 0)} |")
    md_lines.append(f"| å‰”é™¤å¼±å…³ç³»è¾¹ | {pruning_stats.get('weak_edges_removed', 0)} |")
    md_lines.append("")

    # å…³ç³»ç±»å‹åˆ†å¸ƒ
    relation_dist = pruning_stats.get('relation_type_distribution', {})
    if relation_dist:
        md_lines.append("### å…³ç³»ç±»å‹åˆ†å¸ƒ")
        md_lines.append("")
        md_lines.append("| å…³ç³»ç±»å‹ | æ•°é‡ | å æ¯” |")
        md_lines.append("|---------|------|------|")
        total_relations = sum(relation_dist.values())
        for rel_type, count in sorted(relation_dist.items(), key=lambda x: x[1], reverse=True):
            percentage = count / total_relations * 100 if total_relations > 0 else 0
            md_lines.append(f"| {rel_type} | {count} | {percentage:.1f}% |")
        md_lines.append("")

    md_lines.append("### æ¼”åŒ–è·¯å¾„")
    md_lines.append("")
    md_lines.append("| æŒ‡æ ‡ | æ•°å€¼ |")
    md_lines.append("|------|------|")
    md_lines.append(f"| æ¼”åŒ–æ•…äº‹æ•° (Threads) | {len(threads)} |")

    chain_count = sum(1 for t in threads if t.get('pattern_type', '').startswith('The Chain'))
    star_count = sum(1 for t in threads if t.get('pattern_type', '').startswith('The Star'))
    md_lines.append(f"| çº¿æ€§é“¾æ¡ (Chain) | {chain_count} |")
    md_lines.append(f"| æ˜Ÿå‹çˆ†å‘ (Star) | {star_count} |")
    md_lines.append("")

    # æ¼”åŒ–è·¯å¾„è¯¦ç»†åˆ†æ
    md_lines.append("## ğŸ”— å…³é”®æ¼”åŒ–è·¯å¾„ (Critical Evolutionary Paths)")
    md_lines.append("")
    md_lines.append("è¿™é‡Œå®Œå…¨ä¸ç”¨æ‹…å¿ƒå®ƒä»¬æ˜¯ä¸è¿é€šçš„ï¼Œæ¯ä¸ªThreadéƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å…³é”®æ•…äº‹ã€‚")
    md_lines.append("")

    for thread in threads:
        thread_id = thread['thread_id']
        pattern_type = thread['pattern_type']
        title = thread['title']
        narrative = thread['narrative']
        papers = thread['papers']
        total_citations = thread.get('total_citations', 0)
        visual_structure = thread.get('visual_structure', '')
        relation_stats = thread.get('relation_stats', {})

        md_lines.append(f"### Thread {thread_id}: {pattern_type}")
        md_lines.append("")
        md_lines.append(f"**{title}**")
        md_lines.append("")

        # å¯è§†åŒ–ç»“æ„
        if visual_structure:
            md_lines.append("**æ¼”åŒ–ç»“æ„**:")
            md_lines.append("")
            md_lines.append(f"```")
            md_lines.append(visual_structure)
            md_lines.append(f"```")
            md_lines.append("")

        # å…³ç³»ç»Ÿè®¡
        if relation_stats:
            md_lines.append("**å…³ç³»ç»Ÿè®¡**:")
            md_lines.append("")
            md_lines.append(f"- æ€»å…³ç³»æ•°: {relation_stats.get('total_relations', 0)}")
            md_lines.append(f"- ä¸»å¯¼å…³ç³»: {relation_stats.get('dominant_relation', 'Unknown')}")

            rel_dist = relation_stats.get('relation_distribution', {})
            if rel_dist:
                md_lines.append("- åˆ†å¸ƒ: " + ", ".join([f"{k}({v})" for k, v in rel_dist.items()]))
            md_lines.append("")

        # è¯¦ç»†å…³ç³»é“¾ï¼ˆæ–°å¢ï¼‰
        relation_chain = thread.get('relation_chain', [])
        if relation_chain:
            md_lines.append("**è¯¦ç»†å…³ç³»é“¾**:")
            md_lines.append("")

            if thread.get('thread_type') == 'chain':
                # çº¿æ€§é“¾æ¡ï¼šæ˜¾ç¤ºè®ºæ–‡æ¼”è¿›è·¯å¾„
                md_lines.append("| æºè®ºæ–‡ | å…³ç³»ç±»å‹ | ç›®æ ‡è®ºæ–‡ |")
                md_lines.append("|--------|----------|----------|")
                for rel in relation_chain:
                    from_title = rel['from_paper']['title'][:50] + "..." if len(rel['from_paper']['title']) > 50 else rel['from_paper']['title']
                    to_title = rel['to_paper']['title'][:50] + "..." if len(rel['to_paper']['title']) > 50 else rel['to_paper']['title']
                    from_year = rel['from_paper']['year']
                    to_year = rel['to_paper']['year']
                    relation = rel['relation_type']

                    md_lines.append(f"| {from_title} ({from_year}) | **{relation}** | {to_title} ({to_year}) |")

            elif thread.get('thread_type') == 'star':
                # æ˜Ÿå‹ç»“æ„ï¼šæ˜¾ç¤ºä¸­å¿ƒè®ºæ–‡åˆ°å„è·¯çº¿çš„å…³ç³»
                md_lines.append("| è·¯çº¿ | ä¸­å¿ƒè®ºæ–‡ | å…³ç³»ç±»å‹ | ç›®æ ‡è®ºæ–‡ |")
                md_lines.append("|------|----------|----------|----------|")
                for rel in relation_chain:
                    route_id = rel.get('route_id', '?')
                    from_title = rel['from_paper']['title'][:40] + "..." if len(rel['from_paper']['title']) > 40 else rel['from_paper']['title']
                    to_title = rel['to_paper']['title'][:40] + "..." if len(rel['to_paper']['title']) > 40 else rel['to_paper']['title']
                    from_year = rel['from_paper']['year']
                    to_year = rel['to_paper']['year']
                    relation = rel['relation_type']

                    md_lines.append(f"| è·¯çº¿{route_id} | {from_title} ({from_year}) | **{relation}** | {to_title} ({to_year}) |")

            md_lines.append("")

        # å™äº‹æ–‡æœ¬
        md_lines.append("**æ¼”åŒ–å™äº‹**:")
        md_lines.append("")
        md_lines.append(narrative)
        md_lines.append("")

        # æ¶‰åŠè®ºæ–‡
        md_lines.append("**æ¶‰åŠè®ºæ–‡**:")
        md_lines.append("")
        md_lines.append(f"- è®ºæ–‡æ•°é‡: {len(papers)}")
        md_lines.append(f"- æ€»å¼•ç”¨æ•°: {total_citations}")
        md_lines.append("")

        md_lines.append("**ä»£è¡¨æ€§è®ºæ–‡åˆ—è¡¨**:")
        md_lines.append("")
        md_lines.append("| æ ‡é¢˜ | å¹´ä»½ | å¼•ç”¨æ•° |")
        md_lines.append("|------|------|--------|")
        for paper in papers[:5]:  # åªæ˜¾ç¤ºå‰5ç¯‡
            title_short = paper['title'][:60] + "..." if len(paper['title']) > 60 else paper['title']
            md_lines.append(f"| {title_short} | {paper.get('year', 'N/A')} | {paper.get('cited_by_count', 0)} |")
        md_lines.append("")

        md_lines.append("---")
        md_lines.append("")

    # æ–¹æ³•è®ºè¯´æ˜
    md_lines.append("## ğŸ”¬ æ–¹æ³•è®ºè¯´æ˜")
    md_lines.append("")
    md_lines.append("### ç¬¬ä¸€æ­¥ï¼šåŸºäºå…³ç³»çš„å›¾è°±å‰ªæ (Relation-Based Pruning)")
    md_lines.append("")
    md_lines.append("- âœ… ä¿ç•™æ‰€æœ‰ Seed Papers")
    md_lines.append("- âœ… é€šè¿‡å¼ºé€»è¾‘å…³ç³»ï¼ˆOvercomes, Realizes, Extends, Alternative, Adapts_toï¼‰è¿›è¡Œè¿é€šæ€§åˆ†æ")
    md_lines.append("- âœ… å‰”é™¤ä»…ç”±å¼±å…³ç³»ï¼ˆBaselinesï¼‰è¿æ¥çš„è®ºæ–‡")
    md_lines.append("- âœ… æå¤§æå‡æ•°æ®çº¯åº¦")
    md_lines.append("")

    md_lines.append("### ç¬¬äºŒæ­¥ï¼šå…³é”®æ¼”åŒ–è·¯å¾„è¯†åˆ« (Critical Evolutionary Paths)")
    md_lines.append("")
    md_lines.append("**è¯†åˆ«ä¸¤ç§æ ¸å¿ƒæ¼”åŒ–æ¨¡å¼ï¼š**")
    md_lines.append("")
    md_lines.append("1. **çº¿æ€§é“¾æ¡ (The Chain)** - æŠ€æœ¯è¿­ä»£æ•…äº‹")
    md_lines.append("   - ç»“æ„ï¼šA -> (Overcomes) -> B -> (Extends) -> C")
    md_lines.append("   - å™äº‹æ¨¡æ¿ï¼šèµ·å›  â†’ è½¬æŠ˜ â†’ å‘å±•")
    md_lines.append("")
    md_lines.append("2. **æ˜Ÿå‹çˆ†å‘ (The Star)** - ç™¾å®¶äº‰é¸£æ•…äº‹")
    md_lines.append("   - ç»“æ„ï¼šSeed -> (Overcomes) -> A, Seed -> (Alternative) -> B, Seed -> (Extends) -> C")
    md_lines.append("   - å™äº‹æ¨¡æ¿ï¼šç„¦ç‚¹ â†’ åˆ†æ­§ â†’ å¯¹æ¯”")
    md_lines.append("")

    md_lines.append("### ç¬¬ä¸‰æ­¥ï¼šç»“æ„åŒ– Deep Survey æŠ¥å‘Š")
    md_lines.append("")
    md_lines.append("- ğŸ“Š Thread å½¢å¼å±•ç¤ºå„ä¸ªæ¼”åŒ–æ•…äº‹")
    md_lines.append("- ğŸ“ˆ é…åˆå¯è§†åŒ–å›¾å’Œæ–‡å­—è§£è¯»")
    md_lines.append("- ğŸ¯ æ¯ä¸ªThreadæ˜¯ç‹¬ç«‹çš„å…³é”®æ•…äº‹ï¼Œäº’ä¸è¿é€šä¹Ÿæ¸…æ™°")
    md_lines.append("")

    # ç»“è®º
    md_lines.append("## ğŸ¯ ç»“è®º")
    md_lines.append("")
    md_lines.append(f"æœ¬ç»¼è¿°åŸºäºçŸ¥è¯†å›¾è°±å‰ªææŠ€æœ¯ï¼Œä» {pruning_stats.get('original_papers', 0)} ç¯‡è®ºæ–‡ä¸­")
    md_lines.append(f"ç­›é€‰å‡º {pruning_stats.get('pruned_papers', 0)} ç¯‡é«˜è´¨é‡è®ºæ–‡ï¼Œ")
    md_lines.append(f"å¹¶è¯†åˆ«å‡º {len(threads)} æ¡å…³é”®æ¼”åŒ–è·¯å¾„ï¼ˆ{chain_count} æ¡çº¿æ€§é“¾æ¡ + {star_count} ä¸ªæ˜Ÿå‹çˆ†å‘ï¼‰ã€‚")
    md_lines.append("")
    md_lines.append("é€šè¿‡å…³ç³»ç±»å‹åˆ†æå’Œæ¼”åŒ–è·¯å¾„è¯†åˆ«ï¼Œå®Œæ•´å‘ˆç°äº†è¯¥é¢†åŸŸçš„æŠ€æœ¯æ¼”è¿›è„‰ç»œå’Œå¤šå…ƒåŒ–å‘å±•è¶‹åŠ¿ã€‚")
    md_lines.append("")

    # ç”ŸæˆMarkdownæ–‡æœ¬
    markdown_text = "\n".join(md_lines)

    # ä¿å­˜æ–‡ä»¶
    md_file = json_path.parent / json_path.name.replace('.json', '.md')
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write(markdown_text)

    return str(md_file)


def test_new_deep_survey(
    papers_file: str,
    graph_file: str,
    topic: str,
    output_file: str = None
):
    """
    æµ‹è¯•æ–°ç‰ˆæ·±åº¦ç»¼è¿°åˆ†æå™¨

    Args:
        papers_file: è®ºæ–‡æ•°æ®JSONæ–‡ä»¶è·¯å¾„
        graph_file: çŸ¥è¯†å›¾è°±JSONæ–‡ä»¶è·¯å¾„
        topic: ç ”ç©¶ä¸»é¢˜
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    logger.info("="*80)
    logger.info("ğŸ§ª å¼€å§‹æµ‹è¯•æ–°ç‰ˆ DeepSurveyAnalyzer")
    logger.info("="*80)

    # 1. åŠ è½½è®ºæ–‡æ•°æ®
    logger.info(f"\nğŸ“‚ åŠ è½½è®ºæ–‡æ•°æ®: {papers_file}")
    with open(papers_file, 'r', encoding='utf-8') as f:
        papers_data = json.load(f)
    logger.info(f"  âœ… åŠ è½½äº† {len(papers_data)} ç¯‡è®ºæ–‡")

    # 2. åŠ è½½çŸ¥è¯†å›¾è°±æ•°æ®
    logger.info(f"\nğŸ“‚ åŠ è½½çŸ¥è¯†å›¾è°±: {graph_file}")
    with open(graph_file, 'r', encoding='utf-8') as f:
        graph_data = json.load(f)

    # 3. é‡å»ºçŸ¥è¯†å›¾è°±
    logger.info("\nğŸ”¨ é‡å»ºçŸ¥è¯†å›¾è°±...")
    citation_graph = CitationGraph()

    # æ·»åŠ èŠ‚ç‚¹
    for paper in papers_data:
        citation_graph.add_paper_node(paper)

    # æ·»åŠ è¾¹
    edges_data = graph_data.get('edges', [])
    for edge in edges_data:
        source = edge.get('from') or edge.get('source')
        target = edge.get('to') or edge.get('target')
        edge_type = edge.get('edge_type') or edge.get('type', 'CITES')

        if source and target:
            citation_graph.add_citation_edge(source, target, edge_type=edge_type)

    logger.info(f"  âœ… çŸ¥è¯†å›¾è°±é‡å»ºå®Œæˆ")
    logger.info(f"     - èŠ‚ç‚¹æ•°: {citation_graph.graph.number_of_nodes()}")
    logger.info(f"     - è¾¹æ•°: {citation_graph.graph.number_of_edges()}")

    # 4. åˆå§‹åŒ–æ–°ç‰ˆæ·±åº¦ç»¼è¿°åˆ†æå™¨
    logger.info("\nğŸ”§ åˆå§‹åŒ–æ–°ç‰ˆæ·±åº¦ç»¼è¿°åˆ†æå™¨...")
    config = {
        'llm_config_path': './config/config.yaml',
        'min_chain_length': 3,  # æœ€å°é“¾æ¡é•¿åº¦
        'max_threads': 5,       # æœ€å¤šä¿ç•™5ä¸ªThread
    }

    analyzer = DeepSurveyAnalyzer(config=config)
    logger.info("  âœ… åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

    # 5. æ‰§è¡Œæ·±åº¦ç»¼è¿°åˆ†æ
    logger.info(f"\nğŸ“Š å¼€å§‹åˆ†æä¸»é¢˜: '{topic}'")
    logger.info("="*80)

    try:
        result = analyzer.analyze(citation_graph.graph, topic)

        logger.info("\nâœ… æ·±åº¦ç»¼è¿°åˆ†æå®Œæˆ!")
        logger.info("="*80)

        # 6. æ˜¾ç¤ºç»“æœæ‘˜è¦
        logger.info("\nğŸ“ˆ åˆ†æç»“æœæ‘˜è¦:")
        logger.info(f"  - åŸå§‹è®ºæ–‡æ•°: {result['summary']['original_papers']}")
        logger.info(f"  - å‰ªæåè®ºæ–‡æ•°: {result['summary']['pruned_papers']}")
        logger.info(f"  - æ¼”åŒ–è·¯å¾„æ•° (Threads): {result['summary']['total_threads']}")

        # æ˜¾ç¤ºæ¼”åŒ–è·¯å¾„ä¿¡æ¯
        threads = result['survey_report']['threads']
        logger.info("\nğŸ”— æ¼”åŒ–è·¯å¾„è¯¦æƒ…:")
        for thread in threads:
            logger.info(f"\n  Thread {thread['thread_id']}: {thread['pattern_type']}")
            logger.info(f"    ğŸ“ æ ‡é¢˜: {thread['title']}")
            logger.info(f"    ğŸ“„ è®ºæ–‡æ•°: {len(thread['papers'])}")
            logger.info(f"    ğŸ“Š æ€»å¼•ç”¨æ•°: {thread['total_citations']}")
            logger.info(f"    ğŸ”— ç»“æ„: {thread['visual_structure']}")

            # å…³ç³»ç»Ÿè®¡
            rel_stats = thread.get('relation_stats', {})
            if rel_stats:
                logger.info(f"    ğŸ”€ å…³ç³»ç»Ÿè®¡:")
                logger.info(f"       - æ€»å…³ç³»: {rel_stats.get('total_relations', 0)}")
                logger.info(f"       - ä¸»å¯¼å…³ç³»: {rel_stats.get('dominant_relation', 'Unknown')}")
                rel_dist = rel_stats.get('relation_distribution', {})
                if rel_dist:
                    logger.info(f"       - åˆ†å¸ƒ: {rel_dist}")

        # 7. ä¿å­˜ç»“æœ
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            topic_safe = topic.replace(' ', '_').replace('/', '_')
            output_file = f"output/deep_survey/{topic_safe}_{timestamp}.json"

        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2, cls=DateTimeEncoder)

        logger.info(f"\nğŸ’¾ æ·±åº¦ç»¼è¿°æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")

        # 8. ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š
        logger.info("\nğŸ“„ ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š...")
        md_file = generate_markdown_from_result(result, output_path)
        logger.info(f"  âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜è‡³: {md_file}")

        logger.info("="*80)

        return result

    except Exception as e:
        logger.error(f"âŒ æ·±åº¦ç»¼è¿°ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


def main():
    """ä¸»å‡½æ•°"""
    # é…ç½®å‚æ•°
    papers_file = "/home/lexy/ä¸‹è½½/CLwithRAG/KGdemo/226_papers_natural_language_processing_20251218_032842.json"
    graph_file = "/home/lexy/ä¸‹è½½/CLwithRAG/KGdemo/226_graph_data_natural_language_processing_20251218_032842.json"
    topic = "Natural Language Processing"

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(papers_file).exists():
        logger.error(f"âŒ è®ºæ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {papers_file}")
        return

    if not Path(graph_file).exists():
        logger.error(f"âŒ çŸ¥è¯†å›¾è°±æ–‡ä»¶ä¸å­˜åœ¨: {graph_file}")
        return

    # æµ‹è¯•æ–°ç‰ˆåˆ†æå™¨
    try:
        result = test_new_deep_survey(papers_file, graph_file, topic)
        logger.info("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    except Exception as e:
        logger.error(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
