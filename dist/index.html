
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Natural Language Processing - Interactive Citation Knowledge Graph</title>
            <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
            <style>
                body {
                    margin: 0;
                    padding: 0;
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                    background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
                    height: 100vh;
                    overflow: hidden;
                }
                .container {
                    display: flex;
                    width: 100%;
                    height: 100vh;
                    background: white;
                    overflow: hidden;
                }
                .graph-section {
                    width: 70%;
                    display: flex;
                    flex-direction: column;
                }
                .graph-container {
                    flex: 1;
                    padding: 10px;
                }
                .legend-container {
                    padding: 15px;
                    background: #f8f9fa;
                    border-top: 1px solid #dee2e6;
                    min-height: 180px;
                    max-height: 200px;
                }
                .details-section {
                    width: 30%;
                    background: #f8f9fa;
                    border-left: 1px solid #dee2e6;
                    display: flex;
                    flex-direction: column;
                }
                .details-header {
                    padding: 15px 20px;
                    background: #6c757d;
                    color: white;
                    font-weight: bold;
                    font-size: 18px;
                }
                .details-content {
                    padding: 20px;
                    flex: 1;
                    overflow-y: auto;
                }
                .paper-info {
                    margin-bottom: 15px;
                }
                .paper-info h3 {
                    color: #2c3e50;
                    margin: 0 0 10px 0;
                    font-size: 16px;
                    line-height: 1.4;
                }
                .paper-info p {
                    margin: 5px 0;
                    color: #5a5a5a;
                    font-size: 14px;
                }
                .legend-grid {
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
                    gap: 10px;
                }
                .legend-item {
                    display: flex;
                    align-items: center;
                    padding: 8px;
                    background: white;
                    border-radius: 5px;
                    font-size: 12px;
                }
                .legend-color {
                    width: 20px;
                    height: 3px;
                    margin-right: 8px;
                    border-radius: 2px;
                }
                .stats {
                    background: #e9ecef;
                    padding: 15px;
                    border-radius: 8px;
                    margin-bottom: 15px;
                }
                .stat-item {
                    display: flex;
                    justify-content: space-between;
                    margin: 5px 0;
                    font-size: 14px;
                }
                .placeholder {
                    text-align: center;
                    color: #6c757d;
                    font-style: italic;
                    margin-top: 50px;
                }
                .title {
                    text-align: center;
                    color: #2c3e50;
                    margin-bottom: 20px;
                    font-size: 24px;
                    font-weight: bold;
                }
                /* Tab styles */
                .tabs {
                    display: flex;
                    background: #dee2e6;
                    border-bottom: 2px solid #6c757d;
                }
                .tab {
                    flex: 1;
                    padding: 12px 10px;
                    text-align: center;
                    cursor: pointer;
                    border: none;
                    background: #dee2e6;
                    color: #495057;
                    font-size: 14px;
                    font-weight: 500;
                    transition: all 0.3s;
                }
                .tab:hover {
                    background: #c4c8cc;
                }
                .tab.active {
                    background: #6c757d;
                    color: white;
                }
                .tab-content {
                    display: none;
                    padding: 20px;
                    overflow-y: auto;
                    height: calc(100vh - 130px);
                }
                .tab-content.active {
                    display: block;
                }
                .epoch-card {
                    background: white;
                    border-left: 4px solid #3498DB;
                    padding: 15px;
                    margin-bottom: 15px;
                    border-radius: 5px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }
                .epoch-card h4 {
                    margin: 0 0 10px 0;
                    color: #2c3e50;
                    font-size: 15px;
                }
                .epoch-card p {
                    margin: 5px 0;
                    font-size: 13px;
                    color: #555;
                }
                .idea-card {
                    background: white;
                    border-left: 4px solid #2ECC71;
                    padding: 15px;
                    margin-bottom: 15px;
                    border-radius: 5px;
                    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                }
                .idea-card h4 {
                    margin: 0 0 10px 0;
                    color: #2c3e50;
                    font-size: 15px;
                }
                .idea-card .status-badge {
                    display: inline-block;
                    padding: 3px 8px;
                    border-radius: 3px;
                    font-size: 11px;
                    font-weight: bold;
                    margin-left: 8px;
                }
                .status-success {
                    background: #d4edda;
                    color: #155724;
                }
                .status-incompatible {
                    background: #f8d7da;
                    color: #721c24;
                }
                .pivot-paper {
                    background: #fff3cd;
                    padding: 10px;
                    margin: 8px 0;
                    border-radius: 4px;
                    font-size: 12px;
                }
            </style>
        </head>
        <body>
            <div class="container">
                <!-- Left Graph Section (70%) -->
                <div class="graph-section">
                    <div class="title">Natural Language Processing - Interactive Citation Knowledge Graph</div>
                    <div class="graph-container">
                        <div id="graph" style="width:100%; height:100%;"></div>
                    </div>
                    <div class="legend-container">
                        <h4 style="margin-top:0; color:#2c3e50;">üîå Socket Matching Citation Relationship Types (6 Core Types)</h4>
                        <div class="legend-grid">
                            <!-- Socket Matching Core Types (6 types) -->
                            <div class="legend-item" style="border-left: 3px solid #E74C3C;">
                                <div class="legend-color" style="background-color:#E74C3C; height:3px;"></div>
                                <span><strong>Overcomes</strong> - Overcomes/Optimizes (Vertical Deepening)</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #9B59B6;">
                                <div class="legend-color" style="background-color:#9B59B6; height:3px;"></div>
                                <span><strong>Realizes</strong> - Realizes Vision (Research Heritage)</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #2ECC71;">
                                <div class="legend-color" style="background-color:#2ECC71; height:2px;"></div>
                                <span><strong>Extends</strong> - Method Extension (Micro-innovation)</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #E67E22;">
                                <div class="legend-color" style="background-color:#E67E22; border: 2px dotted #E67E22; height:1px;"></div>
                                <span><strong>Alternative</strong> - Alternative Approach (Disruptive Innovation)</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #3498DB;">
                                <div class="legend-color" style="background-color:#3498DB; border: 2px dashed #3498DB; height:1px;"></div>
                                <span><strong>Adapts_to</strong> - Adapts/Applies (Horizontal Diffusion)</span>
                            </div>
                            <div class="legend-item" style="border-left: 3px solid #95A5A6;">
                                <div class="legend-color" style="background-color:#95A5A6; height:1px;"></div>
                                <span><strong>Baselines</strong> - Baseline Comparison (Background Noise)</span>
                            </div>
                        </div>
                        <p style="margin-top:10px; font-size:11px; color:#666;">
                            üí° <strong>üí° Logic Connection Matrix (4 Matches ‚Üí 6 Types)</strong>: Match1(Limitation‚ÜíProblem) ‚Üí Overcomes | Match2(FutureWork‚ÜíProblem) ‚Üí Realizes | Match3(Method‚ÜíMethod) ‚Üí Extends/Alternative | Match4(Problem Cross-domain) ‚Üí Adapts_to | No Match ‚Üí Baselines
                        </p>
                    </div>
                </div>

                <!-- Right Details Section (30%) -->
                <div class="details-section">
                    <!-- Tab navigation -->
                    <div class="tabs">
                        <div class="tab active" onclick="switchTab(event, 'paper-tab')">üìÑ Paper Details</div>
                        <div class="tab" onclick="switchTab(event, 'survey-tab')">üìù Deep Survey</div>
                        <div class="tab" onclick="switchTab(event, 'ideas-tab')">üí° Research Ideas</div>
                    </div>

                    <!-- Paper Details Tab -->
                    <div id="paper-tab" class="tab-content active">
                        <div class="stats">
                            <h4 style="margin-top:0;">Graph Statistics</h4>
                            <div class="stat-item">
                                <span>Total Papers:</span>
                                <span>226</span>
                            </div>
                            <div class="stat-item">
                                <span>Citation Relationships:</span>
                                <span>282</span>
                            </div>
                            <div class="stat-item">
                                <span>Time Span:</span>
                                <span>1980 - 2025</span>
                            </div>
                        </div>
                        <div class="placeholder">
                            üëÜ Click on nodes in the graph to view detailed paper information
                        </div>
                    </div>

                    <!-- Deep Survey Tab -->
                    <div id="survey-tab" class="tab-content"></div>

                    <!-- Research Ideas Tab -->
                    <div id="ideas-tab" class="tab-content"></div>
                </div>
            </div>

            <script>
                // ========== Data Initialization ==========
                const nodesData = [
  {
    "id": "W4382246105",
    "x": 2023.3020806074312,
    "y": -26.811047576622297,
    "title": "Recent Advances in Natural Language Processing via Large Pre-trained Language Models: A Survey",
    "authors": [
      "Bonan Min",
      "Hayley Ross",
      "Elior Sulem"
    ],
    "first_author": "Bonan Min",
    "first_author_surname": "Min",
    "year": 2023,
    "cited_by_count": 956,
    "venue": "",
    "size": 40.23883557315182,
    "color": "hsl(7, 70%, 60%)",
    "label": "Min ,2023",
    "rag_problem": "Traditional NLP approaches require hand-crafted features and task-specific model training, which limits the quality of latent feature representations due to varying training data sizes.",
    "rag_method": "The use of pre-trained language models (PLMs) which are pre-trained on a language modeling task using large unlabelled corpora, followed by fine-tuning for specific NLP tasks.\n\n**Explanation:** PLMs learn generic, latent representations of language from extensive unlabelled data, capturing common language nuances which can be shared across different NLP tasks, reducing the need for task-specific training while allowing efficient adaptation through fine-tuning.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- There is a preliminary theoretical understanding of the paradigms discussed, with little insight into what specifically makes these paradigms successful and whether their success can be generalized across different models and languages.\n- There is a lack of rigorous experiments to determine how many labeled examples are necessary for pre-trained language models (PLMs) to achieve various performance levels across NLP tasks, which is crucial for understanding the pros and cons of each paradigm in terms of cost-efficiency and labeled data requirement.\n- The integration of implicit semantic information through QA data as a supervision signal is not fully explored, and the survey suggests potential but does not provide a definitive method for leveraging this approach effectively across tasks.",
    "rag_future_work": "- Investigating the complementarity of different pre-trained language models (PLMs) could lead to further improvements in natural language processing tasks by combining models like ELMo, BERT, mBERT, and XLM-R rather than using a single model.\n- Further exploration is needed to understand whether the seemingly meaningful prompts used in zero- and few-shot learning are truly effective or if they simply exploit existing patterns in the training data; distinguishing these factors could help refine prompt-based learning methods.\n- Research could focus on determining the minimal amount of unlabeled data necessary for training PLMs without sacrificing performance, especially considering the potential to train models effectively with significantly less data as demonstrated by experiments with MiniBERTas and BabyBERTa.\n- Future work could involve developing methods to automatically and reliably assess the impact of various discrete and continuous prompts on PLM performance to understand the necessity of meaningful instructions.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 57
  },
  {
    "id": "W3046375318",
    "x": 2021.3651113998014,
    "y": -17.59417936391754,
    "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
    "authors": [
      "Ë£ï‰∫å Ê±†Ë∞∑",
      "Robert Tinn",
      "Hao Cheng"
    ],
    "first_author": "Ë£ï‰∫å Ê±†Ë∞∑",
    "first_author_surname": "Ê±†Ë∞∑",
    "year": 2021,
    "cited_by_count": 1737,
    "venue": "",
    "size": 43.04143498109181,
    "color": "hsl(21, 70%, 60%)",
    "label": "Ê±†Ë∞∑ ,2021",
    "rag_problem": "In-domain biomedical NLP tasks suffer from negative transfer due to pretraining on mixed-domain corpora, which includes a large amount of out-domain text.",
    "rag_method": "Pretraining language models solely on domain-specific in-domain biomedical text from scratch, using vocabulary specifically derived from this text.\n\n**Explanation:** By pretraining from scratch using only in-domain biomedical text, the model avoids the influence of out-domain text which can cause negative transfer in domain-specific applications. The in-domain vocabulary ensures that the language model is better suited to understand and process biomedical terms without fragmenting them into irrelevant subwords. This leads to improved representations and better attention mechanisms, resulting in enhanced performance on downstream biomedical NLP tasks. The results show that domain-specific pretraining yields significant gains over conventional mixed-domain approaches.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The approach still lacks extensive benchmarking in biomedical NLP as comprehensive benchmarks and leaderboards are rare compared to general domains.\n- The method might face challenges in evaluating new or emerging tasks in biomedical NLP due to the evolving focus and diversity of available datasets.",
    "rag_future_work": "- Further exploration of domain-specific pretraining strategies to optimize performance and adaptability in biomedical NLP applications.\n- Incorporating a wider range of tasks within biomedical NLP to enhance the model's utility and versatility across different applications and use cases.\n- Extending the BLURB benchmark to include clinical and other high-value domains to ensure comprehensive evaluation and improvement of language models in different biomedical contexts.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 42
  },
  {
    "id": "W4205802268",
    "x": 2021.9563255725507,
    "y": 6.488484575151303,
    "title": "The Routledge Handbook of Translation and Methodology",
    "authors": [
      "Federico Zanettin",
      "Christopher Rundle"
    ],
    "first_author": "Federico Zanettin",
    "first_author_surname": "Zanettin",
    "year": 2022,
    "cited_by_count": 40,
    "venue": "",
    "size": 20.353924076159966,
    "color": "hsl(14, 70%, 60%)",
    "label": "Zanettin ,2022",
    "rag_problem": "The need for efficient and effective methodologies in translation studies that can handle large volumes of linguistic data while providing insights into translated texts.",
    "rag_method": "Adoption of computational linguistics methods such as machine learning and statistical models to analyze and visualize large corpora, alongside tools like concordancing, frequency analysis, and text visualisation.\n\n**Explanation:** Computational linguistics methods facilitate the processing and analysis of large corpora through automation and advanced statistical techniques. By using machine learning for pattern detection and employing visualization tools for clear representation, researchers and practitioners can efficiently analyze translation data, uncover linguistic patterns, and gain insights into translation characteristics quickly. These tools handle the data-intensive nature of corpus-based translation studies, addressing both quantitative and qualitative aspects effectively.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method requires a level of proficiency in machine learning models that cannot be achieved solely through improved user interfaces, making it difficult for non-experts to properly utilize these tools.\n- The reliance on web-based tools for computational linguistics and corpus analysis limits flexibility and the ability for experienced users to utilize their own resources, due to software access and licensing constraints.\n- The absence of stand-alone, offline tools restricts users' autonomy, leaving them dependent on web-based platforms which might change underlying algorithms or withdraw tools entirely, thus impacting analysis continuity and validity.",
    "rag_future_work": "- Develop new techniques for visualization and model explanation in machine learning to ensure proper use and understanding of computational linguistics methods by translators, beyond merely improving user interfaces.\n- Address legal and accessibility issues surrounding the licensing and sharing of analytic tools and corpora to enhance flexibility and reproducibility in corpus-based translation studies through FLOSS platforms.\n- Foster interdisciplinary collaboration among translation scholars, linguists, statisticians, and computer scientists to advance methodology in emerging fields such as corpus-based translation studies.\n- Standardize and develop FLOSS platforms for corpus and translation researchers to document and share their analyses, ensuring that tools are stable and accessible despite changes or withdrawal of web-based services.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 13
  },
  {
    "id": "W4385292983",
    "x": 2022.979672035594,
    "y": 22.346766855112776,
    "title": "Evaluating Large Language Models for Radiology Natural Language Processing",
    "authors": [
      "Zhengliang Liu",
      "Tianyang Zhong",
      "Yiwei Li"
    ],
    "first_author": "Zhengliang Liu",
    "first_author_surname": "Liu",
    "year": 2023,
    "cited_by_count": 19,
    "venue": "",
    "size": 17.656606832130976,
    "color": "hsl(7, 70%, 60%)",
    "label": "Liu ,2023",
    "rag_problem": "The lack of comprehensive evaluation of large language models (LLMs) in the specific domain of radiology natural language processing hinders informed deployment and optimization.",
    "rag_method": "Conducting a systematic and rigorous evaluation of thirty-two LLMs using radiology report datasets, assessing their ability to derive impressions from radiologic findings.\n\n**Explanation:** By benchmarking the models against key performance metrics using standardized datasets, the study identifies the strengths and weaknesses of different LLMs in interpreting radiology reports. This nuanced understanding allows for informed model selection and optimization, enhancing radiology practice by automating image interpretation and assisting in preliminary diagnosis.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The study highlights the need for careful consideration regarding the effective application and ethical deployment of large language models (LLMs) in healthcare, indicating that these aspects remain areas of concern and limitation.\n- There is a suggestion for further exploration into expanding LLMs into different medical specialties and developing multimodal LLMs, which implies that the current model might struggle to handle complex and diverse medical data types comprehensively.\n- Although competitive performance is observed, the study affirms that the true potential of Chinese LLMs in healthcare, particularly radiology, is yet to be fully realized, suggesting their capabilities may still be limited compared to what is possible.",
    "rag_future_work": "- Expand Large Language Models into different medical specialties to enhance their applicability across various domains of healthcare, potentially improving diagnostic and treatment capabilities beyond radiology.\n- Develop multimodal Large Language Models that can integrate and analyze complex, diverse data types such as images, text, and numerical data to provide a more holistic understanding of patient health.\n- Investigate the ethical implications and effective deployment strategies of Large Language Models in healthcare to ensure responsible use and maximize positive impact on patient care.\n- Explore the potential of multilingual and diverse Large Language Models to contribute significantly to global healthcare systems, emphasizing their ability to aid in the interpretation of radiology reports and other medical specialties.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 28
  },
  {
    "id": "W4390602553",
    "x": 2023.8691901255388,
    "y": 53.49307716903277,
    "title": "Fairness Certification for Natural Language Processing and Large Language Models",
    "authors": [
      "Vincent Freiberger",
      "Erik Buchmann"
    ],
    "first_author": "Vincent Freiberger",
    "first_author_surname": "Freiberger",
    "year": 2024,
    "cited_by_count": 3,
    "venue": "",
    "size": 11.609067150120806,
    "color": "hsl(0, 70%, 60%)",
    "label": "Freiberger ,2024",
    "rag_problem": "Natural Language Processing (NLP) systems, especially those using Large Language Models (LLM), are prone to encode and perpetuate biases, leading to unfair outcomes that discriminate against marginalized groups.",
    "rag_method": "Development of a fairness certification framework with defined criteria for auditing NLP systems to ensure fairness.\n\n**Explanation:** The fairness certification framework establishes a set of criteria aimed at operationalizing and systematically evaluating the fairness of NLP systems. These criteria include governance measures, process definitions, data-related assessments, and project planning considerations. By auditing these aspects through expert interviews and literature review, the framework ensures that biases are detected and addressed, reducing the potential for discrimination and unfair treatment in the systems.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method does not cover fairness certification in sectors other than corporate environments, such as public or military sectors.\n- The study may not have reached theoretical saturation due to the limited number of 14 interviewees, potentially impacting the comprehensiveness of the findings.\n- Designing and testing a fairness certification process was beyond the scope of this paper, indicating a gap in the practical application of our research.\n- There is a mismatch between the significance of model architectures in reducing social biases in existing research and the perceived importance assigned by interviewees, suggesting a potential oversight in our approach's focus.",
    "rag_future_work": "- Explore how a certification process can handle the dynamic and subjective nature of fairness in various use cases, especially in non-binary contexts.\n- Investigate the feasibility and implications of making a fairness certification process mandatory for NLP and large language models.\n- Develop best practices and standards upon which the fairness certification for large language models can be established, ensuring comprehensiveness.\n- Design a certification process that specifically addresses the unique challenges posed by large language models, including their scale and complexity.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 33
  },
  {
    "id": "W2593831809",
    "x": 2021.849376419321,
    "y": -6.727753302149822,
    "title": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    "authors": [
      "Hongbin Ye",
      "Ningyu Zhang",
      "Hui Chen"
    ],
    "first_author": "Hongbin Ye",
    "first_author_surname": "Ye",
    "year": 2022,
    "cited_by_count": 450,
    "venue": "",
    "size": 36.705162346907954,
    "color": "hsl(14, 70%, 60%)",
    "label": "Ye ,2022",
    "rag_problem": "Existing natural language generation (NLG) systems do not effectively balance production and comprehension costs while maximizing communicative utility, leading to less efficient human-like communication.",
    "rag_method": "A conceptual framework for NLG systems that integrates considerations of communicative goals, production and comprehension costs, and utility optimization.\n\n**Explanation:** The proposed framework models speakers as decision makers who optimize communication efficiency and effectiveness by reasoning about goals, costs, and utility. By incorporating human-like pragmatic strategies in the decision-making process, the framework helps NLG systems mimic human communicative behavior more closely. This is achieved by estimating and balancing the effort involved in producing utterances and the expected comprehension effort of the audience, ultimately minimizing joint collaborative effort while achieving communicative goals.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Develop more sophisticated decision-making algorithms that can better mimic human-like production strategies by learning from experience.\n- Explore the application of the proposed natural language generation framework to additional communication scenarios to test its adaptability and effectiveness.\n- Investigate the integration of communicative goals, production and comprehension costs, and utility optimisation within other existing natural language processing systems to enhance their pragmatic capabilities.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 14
  },
  {
    "id": "W4399528455",
    "x": 2024.4549232278948,
    "y": -51.033042641625215,
    "title": "Bias and Fairness in Large Language Models: A Survey",
    "authors": [
      "Isabel O. Gallegos",
      "Ryan A. Rossi",
      "Joe Barrow"
    ],
    "first_author": "Isabel O. Gallegos",
    "first_author_surname": "Gallegos",
    "year": 2024,
    "cited_by_count": 288,
    "venue": "",
    "size": 27.691868413696596,
    "color": "hsl(0, 70%, 60%)",
    "label": "Gallegos ,2024",
    "rag_problem": "Large language models (LLMs) can learn, perpetuate, and amplify harmful social biases that disproportionately affect marginalized communities.",
    "rag_method": "The authors propose a comprehensive taxonomy categorizing bias evaluation metrics, datasets, and mitigation techniques.\n\n**Explanation:** The taxonomy organizes bias evaluation metrics by their underlying data structure and leverages it to assess bias at various model levels, such as embeddings, probabilities, and generated text. Datasets are categorized based on their structure as counterfactual inputs or prompts, allowing researchers to identify which datasets are compatible with various metrics. Bias mitigation techniques are classified by intervention stages including pre-processing, in-training, intra-processing, and post-processing, providing a structured approach to systematically reduce bias across different phases of LLM operation.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Intra-processing mitigations, such as decoding strategy modifications, face challenges in balancing bias mitigation with diverse output generation, risking the disproportionate filtering of minority voices.\n- Modeling choices, such as decoding parameters, heavily influence text bias metrics and can lead to contrasting results, reducing the reliability of generated text-based metrics across different datasets.\n- Classifier-based metrics may themselves have intrinsic biases, potentially flagging minority dialects or statements about stigmatized groups inaccurately, making these metrics unreliable for fair bias assessment.\n- Probability-based metrics, like masked token metrics, suffer from semantic and syntactic limitations, reducing their generalizability and reliability, and can misrepresent a model's tendency to produce stereotypical outputs.",
    "rag_future_work": "- Better characterize the performance-fairness trade-off in bias mitigation techniques by analyzing Pareto frontiers for different hyperparameter values and understanding performance declines across social groups.\n- Investigate how and in which components of LLMs bias is encoded, and how bias mitigations affect these components, to develop more targeted technical solutions.\n- Derive theoretical guarantees for bias mitigation techniques to ensure fairness, replacing reliance on empirical assessments with more robust theoretical frameworks.\n- Expand bottleneck resources like word lists and human feedback for bias mitigation techniques to enable scalability, while considering human-and community-in-the-loop frameworks.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 101
  },
  {
    "id": "W4394994587",
    "x": 2023.617391536157,
    "y": -45.701305993066654,
    "title": "Recommender Systems in the Era of Large Language Models (LLMs)",
    "authors": [
      "Zihuai Zhao",
      "Wenqi Fan",
      "Jiatong Li"
    ],
    "first_author": "Zihuai Zhao",
    "first_author_surname": "Zhao",
    "year": 2024,
    "cited_by_count": 218,
    "venue": "",
    "size": 26.649693935676403,
    "color": "hsl(0, 70%, 60%)",
    "label": "Zhao ,2024",
    "rag_problem": "Existing Deep Neural Network (DNN)-based recommender systems struggle to effectively understand users' interests and generalize across different recommendation scenarios.",
    "rag_method": "Integrate Large Language Models (LLMs) like ChatGPT and GPT-4 into recommender systems, leveraging their strong language understanding and reasoning capabilities.\n\n**Explanation:** LLMs have been trained on extensive text data, developing robust language understanding and generation abilities, as well as impressive generalization and reasoning skills. By using LLMs to model complex user-item interactions and to capture textual side information, the recommender systems can better understand and adapt to user's preferences and generalize across various recommendation tasks, including unseen scenarios, thus addressing the limitations of DNN-based methods.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The research on LLMs for recommender systems is still in its early stages, indicating that the findings and implementations may lack maturity and comprehensiveness.\n- There is a need for more systematic and comprehensive studies of LLMs within recommender systems, suggesting that current approaches may have gaps in coverage or application depth.",
    "rag_future_work": "- Investigate the integration challenges of LLMs with existing recommendation algorithms to improve recommendation accuracy and efficiency.\n- Explore personalized recommendation strategies that leverage the unique capabilities of LLMs, particularly in understanding user-generated content and sentiments.\n- Develop scalable and efficient training methods for LLMs within recommender systems, considering real-time data and high-dimensional user-item interactions.\n- Address the potential biases introduced by LLMs in recommendation processes and devise methods to ensure fair and unbiased recommendations.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 49
  },
  {
    "id": "W4384561707",
    "x": 2022.4837884556425,
    "y": -34.619408820186315,
    "title": "Large language models in medicine",
    "authors": [
      "Arun James Thirunavukarasu",
      "Darren Shu Jeng Ting",
      "Kabilan Elangovan"
    ],
    "first_author": "Arun James Thirunavukarasu",
    "first_author_surname": "Thirunavukarasu",
    "year": 2023,
    "cited_by_count": 2502,
    "venue": "",
    "size": 44.75466529012082,
    "color": "hsl(7, 70%, 60%)",
    "label": "Thirunavukarasu ,2023",
    "rag_problem": "The vast and dynamic nature of medical data, along with intricate domain-specific language and evolving medical knowledge, makes it challenging for traditional systems to deliver swift and accurate medical information retrieval and processing.",
    "rag_method": "Utilization of large language models (LLMs) in medicine to deeply understand and semantically reason with medical texts, comprehend medical terminology, and integrate diverse medical data sources.\n\n**Explanation:** LLMs leverage deep learning and NLP techniques to process complex medical semantics and terminology, ensuring accurate retrieval and processing of medical information. By integrating diverse data sources such as medical literature and clinical guidelines, LLMs provide comprehensive and accurate medical information support. Their ability to update with the latest research further enhances the precision and timeliness of information delivery.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Data privacy is a challenge in implementing LLMs in medical applications, as handling sensitive patient information requires significant precautions.\n- Model interpretability remains a limitation, which can lead to difficulties in understanding how decisions are made by LLMs, impacting trust and adoption in clinical settings.\n- Ethical concerns are associated with the use of LLMs in medicine, necessitating careful consideration to ensure responsible deployment and adherence to ethical standards.\n- Technical difficulties exist in the practical implementation of LLMs, which can hinder their effective use in real clinical environments.",
    "rag_future_work": "- Develop medical LLMs integrated into smart medical devices to enhance the functionality and decision-making capabilities of diagnostic and therapeutic equipment.\n- Implement intelligent robots and virtual assistants powered by medical LLMs to provide personalized and efficient patient care in medical settings.\n- Explore the use of medical LLMs within the Metaverse to create immersive healthcare environments for training and patient management.\n- Enhance the security of medical LLMs, possibly using blockchain technology, to ensure privacy and data integrity in medical information handling.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 55
  },
  {
    "id": "W4384071683",
    "x": 2022.7344957535956,
    "y": -32.03647145509324,
    "title": "Large language models encode clinical knowledge",
    "authors": [
      "Karan Singhal",
      "Shekoofeh Azizi",
      "Tao Tu"
    ],
    "first_author": "Karan Singhal",
    "first_author_surname": "Singhal",
    "year": 2023,
    "cited_by_count": 2248,
    "venue": "",
    "size": 44.25207290669649,
    "color": "hsl(7, 70%, 60%)",
    "label": "Singhal ,2023",
    "rag_problem": "Current assessments of large language models' clinical knowledge rely on limited benchmarks and automated metrics, which do not capture the detailed requirements needed for real-world clinical applications.",
    "rag_method": "Introduction of MultiMedQA benchmark, which combines multiple medical question-answering datasets and proposes a human evaluation framework.\n\n**Explanation:** MultiMedQA addresses the limitation by providing a diverse and comprehensive evaluation of LLMs across various medical domains. The human evaluation framework assesses key axes like factuality, reasoning, possible harm, and bias, offering a more nuanced understanding of how models perform in a clinical context compared to simple accuracy metrics.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method struggles with generating appropriate answers for safety-critical medical domains, as even strong LLMs can produce inappropriate responses.\n- Scale alone is insufficient for ensuring accuracy, factuality, consistency, safety, harm, and bias in the medical domain, indicating a need for additional techniques beyond model scaling.",
    "rag_future_work": "- Collaborate with interdisciplinary teams to responsibly integrate AI in healthcare by involving stakeholders such as patients, clinicians, ethicists, and policymakers to translate early research findings into practical applications.\n- Investigate the effects of scaling and instruction fine-tuning on out-of-domain biomedical datasets further, as these strategies have shown significant improvements in medical question answering.\n- Examine the potential memorization issues in large-scale models by exploring the impact of training corpus content on performance, ensuring robustness in real-world medical applications.\n- Continue improving model performance on medical datasets, as current models, despite scaling and fine-tuning, remain inferior to clinicians in terms of providing relevant and helpful responses.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 65
  },
  {
    "id": "W3156333129",
    "x": 2020.4773278788184,
    "y": -14.622751851998594,
    "title": "Deep Learning--based Text Classification",
    "authors": [
      "Shervin Minaee",
      "Nal Kalchbrenner",
      "Erik Cambria"
    ],
    "first_author": "Shervin Minaee",
    "first_author_surname": "Minaee",
    "year": 2021,
    "cited_by_count": 1298,
    "venue": "",
    "size": 41.673968086211445,
    "color": "hsl(21, 70%, 60%)",
    "label": "Minaee ,2021",
    "rag_problem": "Classical machine learning approaches struggle to effectively handle the complexity and variety of text data in classification tasks.",
    "rag_method": "Deep learning-based models that employ neural networks like CNNs, RNNs, and Transformers to improve the representation and classification of text.\n\n**Explanation:** Deep learning models utilize complex architectures that enable the extraction of more nuanced features from text data. CNNs can capture spatial hierarchies in text, RNNs can capture sequential dependencies, and Transformers utilize attention mechanisms to consider the context of words within a text. This allows for more effective learning and classification compared to classical methods which might treat text as a simple bag-of-words or fail to capture dependencies.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method still struggles with text classification tasks that require multi-step reasoning, reflecting a limitation in handling more complex reasoning challenges.\n- Although our approach has made progress, it remains limited in effectively classifying multi-lingual documents, indicating challenges in handling diverse language contexts.\n- The method faces difficulties with extremely long documents in text classification, suggesting limitations in processing very large input lengths efficiently.",
    "rag_future_work": "- Investigate novel neural architectures that integrate advanced mechanisms like attention and self-attention to further enhance text classification performance.\n- Develop techniques to improve model adaptability to new or unseen text classification datasets, addressing limitations in generalization.\n- Explore the application and optimization of cutting-edge models like BERT and XLNet for specific text classification tasks.\n- Address open challenges in interpreting and explaining the predictions of deep learning models in text classification to increase transparency and trustworthiness.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 58
  },
  {
    "id": "W4360845368",
    "x": 2022.6868393005132,
    "y": 19.095613620687885,
    "title": "Translation Technology and Ethical Competence: An Analysis and Proposal for Translators‚Äô Training",
    "authors": [
      "Laura Ram√≠rez-Polo",
      "Chelo Vargas-Sierra"
    ],
    "first_author": "Laura Ram√≠rez-Polo",
    "first_author_surname": "Ram√≠rez-Polo",
    "year": 2023,
    "cited_by_count": 22,
    "venue": "",
    "size": 18.181768993416398,
    "color": "hsl(7, 70%, 60%)",
    "label": "Ram√≠rez-Polo ,2023",
    "rag_problem": "Inadequate integration of ethical considerations into translator training programs affected by the use of technology and artificial intelligence.",
    "rag_method": "Proposal for revised pedagogical models that incorporate ethical competence alongside technological training in translator curricula.\n\n**Explanation:** By revising pedagogical models to include ethical competence, the training programs can directly address concerns regarding technology use, ensuring translators are equipped to make ethically sound decisions while utilizing translation technology. This integration helps future translators understand and navigate ethical dilemmas posed by AI and technology, thus making them more competent in avoiding unethical practices.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate the integration of ethical competence in existing translator training curricula, focusing on updating pedagogical models to address ethical issues posed by technology and artificial intelligence.\n- Develop practical teaching approaches and materials that effectively incorporate ethical considerations in the use of translation technologies during translator training.\n- Conduct longitudinal studies to assess the impact of revised ethical training on translators' professional practices and decision-making when using technology in translation tasks.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4317823603",
    "x": 2021.6427044678612,
    "y": 17.248100727073325,
    "title": "Natural Language Processing for Policymaking",
    "authors": [
      "Zhijing Jin",
      "Rada Mihalcea"
    ],
    "first_author": "Zhijing Jin",
    "first_author_surname": "Jin",
    "year": 2022,
    "cited_by_count": 12,
    "venue": "",
    "size": 16.03791948838553,
    "color": "hsl(14, 70%, 60%)",
    "label": "Jin ,2022",
    "rag_problem": "There is a significant amount of textual data in the political domain, making it challenging to manually code and interpret for policymaking decisions.",
    "rag_method": "Utilizing Natural Language Processing (NLP) methods such as text classification, topic modelling, event extraction, and text scaling to automate the extraction of relevant information from large volumes of text.\n\n**Explanation:** NLP methods provide computational tools to automatically analyze text and extract key information, such as sentiment, stance, and topics, which can inform policymakers. By automating the data processing, NLP reduces the reliance on manual coding and allows for efficient analysis of vast amounts of textual data. This enables policymakers to use evidence-based insights extracted from text data to make informed decisions.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method struggles with potential ethical misuse, such as optimizing policy communication excessively, which could lead to propaganda, data privacy intrusion, or human rights violations, necessitating robust policies to regulate it.\n- The data-driven approach of NLP can lead to biased results, especially when using social media data, which may not represent the entire population accurately compared to traditional polls and surveys.\n- Due to the black-box nature of modern NLP models, the method is less interpretable and transparent, potentially affecting trustworthiness and making it vulnerable to adversarial attacks, highlighting the need for preference towards more explainable models and detailed performance analysis.",
    "rag_future_work": "- Investigate methods to improve policy communication in order to achieve more beneficial societal outcomes. This involves shifting the focus from merely analyzing political texts to finding ways to enhance the constructive impact of policy language.\n- Conduct empirical research on effective communication strategies within policymaking to ensure that policy intentions align more closely with societal benefits and welfare.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 22
  },
  {
    "id": "W4389155560",
    "x": 2022.7481101633803,
    "y": 27.448181044775442,
    "title": "Queering Sexual Health Translation Pedagogy",
    "authors": [
      "Piero Toto"
    ],
    "first_author": "Piero Toto",
    "first_author_surname": "Toto",
    "year": 2023,
    "cited_by_count": 9,
    "venue": "",
    "size": 15.052073257070575,
    "color": "hsl(7, 70%, 60%)",
    "label": "Toto ,2023",
    "rag_problem": "The lack of inclusive translations in sexual health campaigns, which can hinder effective communication and understanding across diverse, multicultural populations, potentially limiting the reach and effectiveness of these campaigns in reducing sexually transmitted infections in England.",
    "rag_method": "Collaborating with translation students to produce inclusive translations of sexual health content, incorporating local cultural nuances and addressing diverse needs.\n\n**Explanation:** By involving translation students in the process, the initiative harnesses fresh perspectives and academic rigor in crafting translations that are sensitive to and inclusive of multicultural aspects. This approach not only ensures that sexual health messages are accurately and appropriately adapted to target audiences' cultural contexts but also acts as a form of social activism, enhancing community engagement and comprehension. This inclusive translation thus helps bridge communication gaps, improving the effectiveness of such campaigns in curbing sexually transmitted infections.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4387425757",
    "x": 2022.6769451922457,
    "y": -1.6292697076241371,
    "title": "A comprehensive survey of ChatGPT: Advancements, applications, prospects, and challenges",
    "authors": [
      "Anam Nazir",
      "Ze Wang"
    ],
    "first_author": "Anam Nazir",
    "first_author_surname": "Nazir",
    "year": 2023,
    "cited_by_count": 198,
    "venue": "",
    "size": 26.289845224338844,
    "color": "hsl(7, 70%, 60%)",
    "label": "Nazir ,2023",
    "rag_problem": "The need to generate human-like responses in conversational AI across diverse scenarios.",
    "rag_method": "Use of Large Language Models (LLMs) combined with Generative Pre-trained Transformers (GPT) to develop ChatGPT.\n\n**Explanation:** LLMs, when integrated with GPT architecture, can understand and process natural language at a high level, enabling the generation of responses that closely mimic human conversation. This architecture is designed to learn from vast amounts of text data, capturing contextual nuances and patterns of human dialogue, which allows ChatGPT to produce coherent and contextually appropriate responses across various situations.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Although the survey provides a comprehensive overview, it may not cover every single advancement and application of ChatGPT due to the rapid pace of developments in the field, leaving some areas potentially unexplored or underexplored.\n- The method of categorizing challenges might not fully capture the nuanced and multifaceted nature of issues surrounding ChatGPT's implementation and ethical considerations, which could lead to oversimplification in some aspects.",
    "rag_future_work": "- Investigate the ethical implications of ChatGPT in various applications to ensure responsible AI deployment and address concerns related to bias, privacy, and misinformation.\n- Explore the integration of ChatGPT with other AI models to enhance its capabilities, allowing it to perform more complex tasks and provide more accurate responses.\n- Develop techniques to improve the scalability and efficiency of ChatGPT, aiming to reduce computational costs and increase applicability in real-time scenarios.\n- Study user interaction patterns with ChatGPT to refine user experience, focusing on areas like customization, adaptability, and accessibility for diverse groups.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4405910844",
    "x": 2023.45199869795,
    "y": -17.89291160363662,
    "title": "Large language models for robotics: Opportunities, challenges, and perspectives",
    "authors": [
      "Jiaqi Wang",
      "Enze Shi",
      "Huawen Hu"
    ],
    "first_author": "Jiaqi Wang",
    "first_author_surname": "Wang",
    "year": 2024,
    "cited_by_count": 61,
    "venue": "",
    "size": 21.90790420290339,
    "color": "hsl(0, 70%, 60%)",
    "label": "Wang ,2024",
    "rag_problem": "Text-only Large Language Models (LLMs) struggle with performing embodied tasks because they lack the ability to integrate robotic visual perception, which is essential for tasks requiring interaction with complex environments.",
    "rag_method": "The integration of multimodal capabilities in Large Language Models, specifically using GPT-4V, enhances embodied task planning by combining natural language instructions with robot visual perceptions.\n\n**Explanation:** By utilizing GPT-4V, the framework incorporates visual information that complements the text-based instructions, enabling the creation of precise and efficient task sequences. This integration helps robots understand and interpret their physical environment better, improving their ability to generate coherent action plans and conduct tasks like manipulation and navigation effectively.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The generated plans are homogenous and lack detailed embodiment, which makes them insufficiently robust for managing complex environments and tasks.\n- Current multimodal LLMs require carefully crafted, lengthy prompts to produce reliable outputs, demanding domain expertise and the use of extensive tricks.\n- The robot is constrained by predefined actions, which limits its executional freedom and robustness in diverse scenarios.\n- The closed-source nature of the GPT-4V API and the associated time delays can impede the development of embedded systems and real-time commercial applications.",
    "rag_future_work": "- Future research should focus on developing more robust AGI robotic systems by overcoming current limitations, such as the homogeneity in generated plans, the need for complex prompts, limited executional freedom, and API constraints.\n- There is potential for applying multimodal-LLM-centric AGI robots in precision agriculture for tasks such as fruit picking and crop phenotyping, where advanced reasoning and precise action are required.\n- In the healthcare domain, enhancing the perceptual and reasoning abilities of multimodal LLMs is crucial for robot-assisted screening and surgeries, where individualized tasks are essential.\n- Developing Brain-Computer Interfaces (BCIs) in LLM-centric AGI robotic systems through models like CLIP could enable reading and interpreting human brain signals for self-planning and control in complex task completion.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 23
  },
  {
    "id": "W4405660551",
    "x": 2023.9312325227363,
    "y": -5.356740493011262,
    "title": "Understanding LLMs: A comprehensive overview from training to inference",
    "authors": [
      "Yiheng Liu",
      "Hao He",
      "Tianle Han"
    ],
    "first_author": "Yiheng Liu",
    "first_author_surname": "Liu",
    "year": 2024,
    "cited_by_count": 31,
    "venue": "",
    "size": 19.422667875302015,
    "color": "hsl(0, 70%, 60%)",
    "label": "Liu ,2024",
    "rag_problem": "Training and deploying Large Language Models (LLMs) require handling large-scale data and substantial engineering capabilities, which can be cost-prohibitive and impractical for most researchers.",
    "rag_method": "The paper provides a comprehensive overview of efficient parallel training, model compression methods such as knowledge distillation and model pruning, and parameter-efficient tuning techniques like LoRA and Prompt Learning to reduce computational and memory overhead.\n\n**Explanation:** Parallel training techniques help distribute the computational load across multiple GPUs, making it feasible to handle large-scale data efficiently. Techniques like model compression reduce the size of models without significant loss in performance, making them easier to deploy on limited hardware. Parameter-efficient tuning allows fine-tuning of LLMs by adjusting only a subset of parameters, significantly reducing the cost associated with full model tuning. These solutions collectively facilitate the practical training and deployment of LLMs by lowering resource requirements.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The reliance on OpenAI's infrastructure is a significant limitation of the current approach, indicating the need for alternative LLMs and emphasizing the development of domain-specific models.\n- The process of training and deploying LLMs requires significant expertise and collaboration between researchers and engineers, underlining the complexity and resource intensity involved in handling large-scale data and distributed parallel training.",
    "rag_future_work": "- Explore developmental trends in LLM technology: Investigate the future advancements in the architecture and capabilities of LLMs to enhance their efficiency and effectiveness.\n- Examine developmental directions for AI researchers: Focus on new methodologies and techniques that AI researchers can develop to improve the training and deployment of LLMs.\n- Analyze societal impact of LLM development: Study the broader societal implications of LLM advancements, including ethical considerations and potential societal changes.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 58
  },
  {
    "id": "W4393318216",
    "x": 2024.2286702980186,
    "y": 5.4451963403113135,
    "title": "A Comparative Analysis to Evaluate Bias and Fairness Across Large Language Models with Benchmarks",
    "authors": [
      "Èô≥ÊñáÊÑè",
      "ÈªÉÂÖÜÊòé"
    ],
    "first_author": "Èô≥ÊñáÊÑè",
    "first_author_surname": "Èô≥ÊñáÊÑè",
    "year": 2024,
    "cited_by_count": 20,
    "venue": "",
    "size": 17.83993819995006,
    "color": "hsl(0, 70%, 60%)",
    "label": "Èô≥ÊñáÊÑè ,2024",
    "rag_problem": "Large Language Models exhibit bias in dimensions such as gender, race, and ethnicity, impacting their fairness and applicability.",
    "rag_method": "Utilize the Google BIG-Bench benchmark to evaluate bias and fairness across various models, including ChatGPT-4, Google Gemini, and Llama 2.\n\n**Explanation:** The Google BIG-Bench benchmark provides a comprehensive suite of tests designed to expose biases by systematically probing language models across diverse demographic categories. By using this benchmark, researchers can concretely measure and compare bias levels quantitatively among different models, facilitating the identification of disparities. It allows for a structured and scalable approach to detect bias, informing model adjustments and improvements to enhance fairness.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The Google BIG-Bench benchmark, although instrumental in identifying biases, faces challenges in effectively capturing the sophistication of certain bias aspects.\n- The analysis reveals varied levels of biases across models, indicating that the method may struggle with consistently evaluating and comparing bias metrics across different dimensions such as gender, race, and ethnicity.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4399496842",
    "x": 2023.8611662551998,
    "y": 33.51483225073479,
    "title": "A Comparative Analysis of Cultural Alignment in Large Language Models in Bilingual Contexts",
    "authors": [
      "Ximen Yuan",
      "Jinshan Hu",
      "Qian Zhang"
    ],
    "first_author": "Ximen Yuan",
    "first_author_surname": "Yuan",
    "year": 2024,
    "cited_by_count": 8,
    "venue": "",
    "size": 14.6561760966799,
    "color": "hsl(0, 70%, 60%)",
    "label": "Yuan ,2024",
    "rag_problem": "AI systems often exhibit cultural biases that may misalign with specific cultural values, leading to ineffective and inequitable deployment in bilingual contexts.",
    "rag_method": "The research conducts a comparative analysis of AI-generated responses to examine their alignment with mainstream Chinese cultural values.\n\n**Explanation:** By analyzing the responses generated by AI systems for their alignment with cultural values such as Confucian harmony and Daoist balance, the research identifies cultural biases and assesses how well these systems can adapt to different cultural contexts. This enables the development of culturally-aligned AI systems that can more effectively and equitably interact with users in bilingual environments.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4409894900",
    "x": 2025.447644069345,
    "y": 14.245470020774635,
    "title": "Foundations of AI in Educational Assessment",
    "authors": [
      "Goran Trajkovski",
      "Heather Hayes"
    ],
    "first_author": "Goran Trajkovski",
    "first_author_surname": "Trajkovski",
    "year": 2025,
    "cited_by_count": 0,
    "venue": "",
    "size": 8,
    "color": "hsl(0, 70%, 60%)",
    "label": "Trajkovski ,2025",
    "rag_problem": "No clear research problem description found",
    "rag_method": "No clear method description found",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4385574029",
    "x": 2021.5067488007364,
    "y": 20.052640209093056,
    "title": "On the Role of Bidirectionality in Language Model Pre-Training",
    "authors": [
      "Mikel Artetxe",
      "Jingfei Du",
      "Naman Goyal"
    ],
    "first_author": "Mikel Artetxe",
    "first_author_surname": "Artetxe",
    "year": 2022,
    "cited_by_count": 10,
    "venue": "",
    "size": 15.410205801366397,
    "color": "hsl(14, 70%, 60%)",
    "label": "Artetxe ,2022",
    "rag_problem": "Existing language model pre-training approaches use different architectures and learning objectives, making it difficult to compare their effectiveness across tasks and model scales.",
    "rag_method": "The authors propose a new framework that separates and controls two notions of bidirectionality: bidirectional context and bidirectional attention, allowing systematic evaluation across various language modeling tasks.\n\n**Explanation:** By distinguishing and providing separate control over bidirectional context and attention, the framework enables a clear understanding of how each component affects different tasks like next token prediction, text infilling, zero-shot priming, and fine-tuning. This separation allows for principled comparisons that were previously obscured by the mixing of methodologies, thus addressing the difficulty of understanding the benefits of bidirectional models versus unidirectional models on divergent tasks.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method does not have a single optimal configuration that works best across all use cases, which suggests that different setups are required for different applications, and this may limit its practicality in diverse scenarios.\n- The approach may involve prohibitive computational costs when considering the full vocabulary for tasks like full sequence scoring, which necessitates constraints on the candidate set, potentially limiting the model's performance in certain scenarios.\n- Scaling up the method does not significantly mitigate the fundamental interference between different capabilities such as unidirectional and bidirectional attention, indicating a persistent challenge in balancing these forces effectively within the model.",
    "rag_future_work": "- Explore modular and adaptation approaches for language model pre-training to efficiently utilize a single model with specialized components or adapt existing models, reducing the need for training multiple models from scratch.\n- Investigate the benefits of bidirectional attention in both fine-tuning and infilling tasks, particularly in scenarios where unidirectional context may still compete effectively, especially with a small set of candidates.\n- Further analyze the effectiveness of models trained jointly on unidirectional and bidirectional contexts to improve next token prediction and infilling tasks, particularly focusing on optimizing the use of bidirectional attention in prefixes.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 15
  },
  {
    "id": "W3106340866",
    "x": 2020.4073191241193,
    "y": 14.191824534570111,
    "title": "Augmented Natural Language for Generative Sequence Labeling",
    "authors": [
      "Ben Athiwaratkun",
      "C√≠cero Nogueira dos Santos",
      "Jason Krone"
    ],
    "first_author": "Ben Athiwaratkun",
    "first_author_surname": "Athiwaratkun",
    "year": 2020,
    "cited_by_count": 53,
    "venue": "",
    "size": 21.388797720080248,
    "color": "hsl(28, 70%, 60%)",
    "label": "Athiwaratkun ,2020",
    "rag_problem": "Current token-level classification frameworks for sequence labeling struggle to share knowledge across multiple tasks and do not incorporate label semantics effectively, limiting their efficiency particularly in low-resource or few-shot settings.",
    "rag_method": "The generative framework proposed uses a single, shared natural language output space to perform joint sequence labeling and sentence-level classification, allowing the incorporation of label semantics and knowledge sharing across tasks.\n\n**Explanation:** By framing sequence labeling as a conditional sequence generation problem, the model uses natural language expressions as labels, enriching the semantic understanding of labels. This approach allows the model to learn more efficiently from limited examples because it leverages shared semantic context inherent in natural language, thus reducing the gap between different tasks and enabling effective knowledge transfer.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Explore the use of alternative source domains for meta-training, such as the Ontonotes NER task, to establish a versatile, single meta-trained model applicable across diverse evaluation domains.\n- Investigate more challenging scenarios without any meta-training to evaluate the robustness and generalizability of the proposed generative sequence labeling framework.\n- Incorporate label semantics more deeply by experimenting with embedding example slot values along with slot descriptions, which may yield improvements in zero-shot slot labeling.\n- Analyze the potential benefits of using locally tagged words with labels in word sequences for improving attention mechanisms in transformer models, potentially enhancing performance in limited resource scenarios.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 16
  },
  {
    "id": "W2970925270",
    "x": 2018.6439793855552,
    "y": 3.7126677467638007,
    "title": "Simple, Scalable Adaptation for Neural Machine Translation",
    "authors": [
      "Ankur Bapna",
      "Orhan Fƒ±rat"
    ],
    "first_author": "Ankur Bapna",
    "first_author_surname": "Bapna",
    "year": 2019,
    "cited_by_count": 300,
    "venue": "",
    "size": 27.844738892398624,
    "color": "hsl(35, 70%, 60%)",
    "label": "Bapna ,2019",
    "rag_problem": "Fine-tuning requires maintaining a separate model for each language and domain, which is inefficient and challenging as the number of languages and domains increases.",
    "rag_method": "Inject lightweight, task-specific adapter layers into a pre-trained NMT model.\n\n**Explanation:** By only fine-tuning small adapter layers rather than the entire NMT model, the approach reduces parameter inefficiency and simplifies model maintenance. Adapters allow a single model to be adapted to multiple languages and domains simultaneously without requiring separate models for each task.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method shows a minor regression for high resource languages when translating into English compared to bilingual baselines, and it might require increasing adapter size or exploring more efficient solutions to bridge this gap.\n- The need to vary adapter capacity for different datasets reveals that the method's adaptability may be limited by fixed capacity configurations, particularly when dealing with large quantities of adaptation data.\n- There is a significant performance deterioration for high resource languages due to limited model capacity and the model converging before sufficiently training on these datasets, suggesting a need for more balanced resource allocation.",
    "rag_future_work": "- Explore the development of massively multitask translation models that can handle a vast number of languages and domains simultaneously.\n- Investigate the potential of universal translation models which leverage shared parameters for efficient cross-lingual and cross-domain adaptations.\n- Extend the adapter framework to improve its scalability and generalization ability across more complex and diverse translation challenges.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 18
  },
  {
    "id": "W2963716420",
    "x": 2019.0240457776897,
    "y": -9.29548476401982,
    "title": "Publicly Available Clinical",
    "authors": [
      "Emily Alsentzer",
      "John R. Murphy",
      "William Boag"
    ],
    "first_author": "Emily Alsentzer",
    "first_author_surname": "Alsentzer",
    "year": 2019,
    "cited_by_count": 1422,
    "venue": "",
    "size": 42.10219973760391,
    "color": "hsl(35, 70%, 60%)",
    "label": "Alsentzer ,2019",
    "rag_problem": "There are no publicly available pre-trained BERT models specifically for clinical text, which limits the performance of NLP tasks in clinical domains.",
    "rag_method": "Creation and public release of domain-specific BERT models trained on clinical text, specifically Clinical BERT for generic clinical notes and Discharge Summary BERT for discharge summaries.\n\n**Explanation:** By training BERT models specifically on clinical text, these models capture linguistic characteristics unique to clinical narratives, allowing for improved performance in clinical NLP tasks compared to general-domain BERT or BioBERT. This is especially beneficial for tasks such as named entity recognition and medical inference where context matters. The public release of these models allows others in the community to leverage these embeddings without incurring the computational cost of training from scratch.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method, clinical BERT, struggles on the de-ID tasks, i2b2 2006 and i2b2 2014, where it offers no improvements over BioBERT or general BERT, likely due to data distribution differences between MIMIC and the de-ID task, which affect the sentence structure used in training.\n- The assumption that contextual embedding models like ours, trained on task-like corpora, will offer dramatic improvements does not hold true for de-ID tasks where the sentence structure differs significantly from the training data.",
    "rag_future_work": "- Explore the impact of synthetic PHI masks on contextual embedding models: Investigate methods to mitigate the influence of different sentence structures caused by synthetic PHI masks in de-ID tasks on models like BERT.\n- Enhance specificity of corpus for domain-specific NLP tasks: Examine the benefits of customizing and fine-tuning underlying corpora for specific clinical NLP tasks to boost model performance.\n- Investigate performance across various clinical NLP tasks: Conduct further studies comparing BERT models with a focus on identifying strengths and weaknesses across different clinical tasks to refine model selection.\n- Improve models for de-ID tasks: Research approaches to improve contextual embedding models specifically for de-identification tasks where clinical BERT is currently less effective.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 15
  },
  {
    "id": "W2145383760",
    "x": 2010.8050014530406,
    "y": -0.4832739645450167,
    "title": "BioCreative III interactive task: an overview",
    "authors": [
      "Cecilia Arighi",
      "Phoebe M. Roberts",
      "Shashank Agarwal"
    ],
    "first_author": "Cecilia Arighi",
    "first_author_surname": "Arighi",
    "year": 2011,
    "cited_by_count": 82,
    "venue": "",
    "size": 23.004004241277045,
    "color": "hsl(92, 70%, 60%)",
    "label": "Arighi ,2011",
    "rag_problem": "Existing text mining tools for biological curation lack usability and integration within the workflow of biocurators, leading to suboptimal adoption and performance.",
    "rag_method": "BioCreative III introduced the InterActive Task (IAT) to assess the utility and usability of text mining tools by incorporating biocurator feedback in system development and evaluation.\n\n**Explanation:** The IAT involved biocurators actively in every phase of tool development, allowing direct feedback to developers about real curation challenges and usability issues. This approach encouraged the design of systems with user-friendly interfaces and interactive features tailored to biocurator needs. By making curators a part of the development process, the tools are more likely to match workflow requirements and gain adoption.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The testing process was limited by time constraints, as the assessment had to be completed within only two weeks, which restricted the number of articles processed and the availability of UAG members for the testing.\n- Comparison of time efficiency between assisted and unassisted curation was unreliable due to inconsistent timing methods used by different UAG members, which affects the ability to derive meaningful conclusions about the system's efficiency.\n- The demonstration task struggled to differentiate between performance and usability of the systems due to varying proposed gene identifiers, which distracted curators from focusing solely on the usability features.",
    "rag_future_work": "- Future developments could focus on systems to alleviate the backlog of uncurated articles, aid in creating structured digital abstracts, and enhance biocuration from novices by refining basic tasks such as gene normalization.\n- Enhancements to tools like MyMiner could include flexible editing options, species-specific selections, user-task management systems to record previous choices, and the addition of user-provided customized bio-entity dictionaries.\n- Future tasks should actively involve stakeholders in all phases of software development, focusing on the establishment of metrics and functional requirements for evaluating interactive curation systems, particularly in the context of the upcoming BioCreative IV challenge.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 36
  },
  {
    "id": "W2736047977",
    "x": 2016.876967868218,
    "y": 4.786904197469785,
    "title": "Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer",
    "authors": [
      "Simon Baker",
      "Imran Ali",
      "Ilona Silins"
    ],
    "first_author": "Simon Baker",
    "first_author_surname": "Baker",
    "year": 2017,
    "cited_by_count": 114,
    "venue": "",
    "size": 24.229308675426577,
    "color": "hsl(49, 70%, 60%)",
    "label": "Baker ,2017",
    "rag_problem": "The overwhelming amount of scientific literature on cancer makes it difficult for researchers to efficiently organize and retrieve relevant information on molecular mechanisms involved in cancer development.",
    "rag_method": "The Cancer Hallmarks Analytics Tool (CHAT) utilizes text mining algorithms to systematically organize and evaluate scientific literature related to cancer.\n\n**Explanation:** CHAT employs advanced text mining techniques to filter and categorize information from millions of scientific articles. By intelligently organizing this data according to specific cancer hallmarks, researchers can more efficiently access and review relevant literature, thus accelerating the process of understanding molecular mechanisms and driving new cancer research.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2070205520",
    "x": 2013.5884239389593,
    "y": -5.133702546835302,
    "title": "The Sketch Engine",
    "authors": [
      "Adam Kilgarriff",
      "V√≠t Baisa",
      "Jan Bu≈°ta"
    ],
    "first_author": "Adam Kilgarriff",
    "first_author_surname": "Kilgarriff",
    "year": 2014,
    "cited_by_count": 1879,
    "venue": "",
    "size": 43.41031695880249,
    "color": "hsl(71, 70%, 60%)",
    "label": "Kilgarriff ,2014",
    "rag_problem": "Lexicographers need efficient tools to generate and edit comprehensive dictionary entries from large corpora, especially as traditional paper dictionaries have declined and electronic dictionaries rise in prominence.",
    "rag_method": "The Sketch Engine's word sketches provide a one-page summary of a word's grammatical and collocational behavior, automatically organized and ready for lexicographers to edit and publish.\n\n**Explanation:** Word sketches automate the process of analyzing large corpora to find recurring patterns for each word. By presenting these patterns in a structured format, lexicographers can efficiently create dictionary entries without manually sifting through data. This supports the transition from paper to electronic lexicography, reducing workload and increasing accuracy.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The Sketch Engine may not be entirely suitable for handling the sequence of words in text because it relies on SQL databases, which may not be the best fit for a text where the sequence of words is more important than the relations.\n- Cleaning the data of potentially offensive topics, colloquially referred to as \"parsnips,\" remains a current challenge, necessitating further development to ensure the data is culturally and socially appropriate for global use.",
    "rag_future_work": "- Future work could explore the development of more extensive distributional thesauruses, which show promise for addressing various challenges in computational linguistics.\n- Investigating historical corpora and their applications in understanding language development and change remains a promising area for further research.\n- Enhancing the Sketch Engine to better support professional translators in identifying domain-specific terminology and phraseology will benefit its user base.\n- There is potential to expand learner corpora to cover additional languages, supporting research into language learning processes and curriculum development.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 30
  },
  {
    "id": "W2614620964",
    "x": 2016.891286344782,
    "y": -7.012639078845607,
    "title": "CORPUS LINGUISTICS: METHOD, THEORY, AND PRACTICE",
    "authors": [
      "Dana Waskita"
    ],
    "first_author": "Dana Waskita",
    "first_author_surname": "Waskita",
    "year": 2017,
    "cited_by_count": 963,
    "venue": "",
    "size": 40.27306637758645,
    "color": "hsl(49, 70%, 60%)",
    "label": "Waskita ,2017",
    "rag_problem": "The challenge in corpus linguistics is developing effective methodologies for studying language using large volumes of text data.",
    "rag_method": "McEnery and Hardie proposed a structured approach to corpus linguistics, involving both corpus-based and corpus-driven methodologies.\n\n**Explanation:** The corpus-based approach uses deductive reasoning where corpus data supports pre-existing theories, while the corpus-driven approach uses inductive reasoning where corpus data itself drives the formation of new linguistic theories. This dual methodology provides a framework that systematically handles large text data, allowing researchers to either validate existing theories or develop new insights directly from data, thus enhancing the robustness of linguistic research.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method may still struggle with effectively integrating the use of electronic text analysis in a way that balances traditional language and literary studies approaches with modern computational techniques.\n- It appears there might be limitations in the method's ability to comprehensively address all aspects of sociolinguistic variation solely through corpus linguistic approaches, suggesting a need for complementary approaches such as questionnaires or qualitative data.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 3
  },
  {
    "id": "W1562955078",
    "x": 2012.9830769043656,
    "y": 2.5306174208420913,
    "title": "Multilingual Distributed Representations without Word Alignment",
    "authors": [
      "Karl Moritz Hermann",
      "Phil Blunsom"
    ],
    "first_author": "Karl Moritz Hermann",
    "first_author_surname": "Hermann",
    "year": 2013,
    "cited_by_count": 67,
    "venue": "",
    "size": 22.255001356969103,
    "color": "hsl(78, 70%, 60%)",
    "label": "Hermann ,2013",
    "rag_problem": "Traditional multilingual embeddings require word alignment, which is a limiting factor especially for languages with sparse or no parallel resources.",
    "rag_method": "The authors propose a method for learning multilingual distributed representations at the sentence level using parallel sentence data without requiring word alignment.\n\n**Explanation:** By utilizing parallel sentences as a training signal, the model learns to assign similar embeddings to semantically equivalent sentences across languages, effectively capturing the shared semantic content without needing word-level alignment. This method allows for semantic transfer across languages even when word-level parallel data is unavailable.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method employs a simplified bag-of-words approach that does not account for word ordering and other linguistic effects, which may limit its ability to accurately capture the complexity of language semantics.\n- The method assumes perfectly trained cross-lingual vector models, which may not accurately reflect real-world conditions where models are imperfectly trained and misalignment can occur, potentially impacting performance.\n- The scalability of the method across different languages and large datasets may be challenged by the absence of secondary or tertiary representations, which limits its applicability to more resource-rich language pairs.",
    "rag_future_work": "- Explore the integration of the objective function with complex compositional vector models like MV-RNN or tensor-based approaches to improve both mono- and multilingual tasks.\n- Apply the cross-lingual model to a broader set of tasks, including machine translation and multilingual information extraction, to evaluate its effectiveness.\n- Extend the pivot language approach to incorporate multiple pivots, preserving different linguistic aspects such as case and gender, to address limitations of single pivot language models.\n- Investigate training semantic representations using document-aligned data or comparable corpora, aiming to enhance capabilities for low-resource languages.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 14
  },
  {
    "id": "W4383374753",
    "x": 2023.0665321698466,
    "y": 11.914148599402168,
    "title": "When brain-inspired AI meets AGI",
    "authors": [
      "Lin Zhao",
      "Lu Zhang",
      "Zihao Wu"
    ],
    "first_author": "Lin Zhao",
    "first_author_surname": "Zhao",
    "year": 2023,
    "cited_by_count": 76,
    "venue": "",
    "size": 22.722055952976504,
    "color": "hsl(7, 70%, 60%)",
    "label": "Zhao ,2023",
    "rag_problem": "Current AI systems lack the ability to achieve true artificial general intelligence (AGI), which requires understanding, learning, and adapting across multiple domains and complex tasks similar to human intelligence.",
    "rag_method": "Brain-inspired AI leveraging principles of the human brain, such as neuroplasticity, multimodality processing, and efficient architecture designs, like neuromorphic computing and attention mechanisms.\n\n**Explanation:** By mimicking the human brain's ability to integrate multimodal information, adapt through neuroplasticity, and efficiently process data, brain-inspired AI can develop systems that are more flexible and capable of generalizing across tasks. This replication of brain principles allows AI systems to handle complex, real-world problems with adaptability similar to human cognition, essential for achieving AGI.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method faces a limited understanding of the human brain, which poses a challenge in creating machines that can fully replicate human intelligence.\n- Current AGI and brain-inspired AI systems, including ours, require vast amounts of training data to achieve performance comparable to humans, which contrasts with the human ability to learn from few examples.",
    "rag_future_work": "- Development of more powerful and sophisticated AGI foundation models by leveraging breakthroughs in natural language processing, computer vision, knowledge graphs, and reinforcement learning is crucial to accelerate the progress towards AGI.\n- Integration of different AI systems and technologies, including involving humans in the loop through reinforcement learning from expert feedback, could create more versatile and adaptable intelligent systems, helping to overcome current AI system limitations.\n- Addressing ethical and societal implications such as bias, privacy, and security in AGI development is essential to ensure that AGI advances in a way that is responsible, ethical, and aligns with human values.\n- Advancements in technology, algorithms, and hardware will be necessary, along with continued multidisciplinary collaboration, to overcome the challenges faced in achieving AGI.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 20
  },
  {
    "id": "W4392353733",
    "x": 2024.0782637005832,
    "y": -66.51111108985131,
    "title": "A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly",
    "authors": [
      "Yifan Yao",
      "Jinhao Duan",
      "Kaidi Xu"
    ],
    "first_author": "Yifan Yao",
    "first_author_surname": "Yao",
    "year": 2024,
    "cited_by_count": 596,
    "venue": "",
    "size": 38.02241659084849,
    "color": "hsl(0, 70%, 60%)",
    "label": "Yao ,2024",
    "rag_problem": "LLMs are susceptible to adversarial attacks like data poisoning, which can compromise their security and ethical behavior.",
    "rag_method": "Implement optimization methods such as adversarial training to increase resilience against these perturbations.\n\n**Explanation:** Adversarial training enhances the robustness of LLMs by simulating potential adversarial scenarios during training, thus preparing the models to better resist data poisoning attacks. This method adjusts the learning algorithms to detect and mitigate these malicious inputs effectively, thereby preserving the integrity and security of the model.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The paper suggests using traditional Privacy-Enhancing Technologies to address privacy challenges posed by LLMs, yet it acknowledges that exploring new techniques is necessary, indicating that their current approach might not fully resolve these issues.\n- The authors acknowledge challenges associated with LLM-specific attacks, particularly related to the vast scale and private ownership of powerful LLMs, suggesting that their approach may struggle with implementing effective security measures given these novel characteristics.\n- While LLMs can potentially be applied to security applications that traditionally use machine learning methods, the paper implies that their approach needs further comparison with state-of-the-art methods to ensure effectiveness, indicating a possible limitation in the current implementation approach.",
    "rag_future_work": "- Explore additional Privacy-Enhancing Technologies (PETs) for LLMs: Future research could focus on utilizing both established and innovative PET techniques, such as zero-knowledge proofs and differential privacy, to address privacy challenges associated with large language models.\n- Identify security tasks for LLMs to replace human efforts: Security researchers can investigate traditional security tasks, especially those requiring human intervention like social engineering, to leverage LLMs' capabilities and potentially automate these processes.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 48
  },
  {
    "id": "W4391855109",
    "x": 2023.6109165821524,
    "y": -63.60108180938059,
    "title": "A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges",
    "authors": [
      "Mohaimenul Azam Khan Raiaan",
      "Md. Saddam Hossain Mukta",
      "Kaniz Fatema"
    ],
    "first_author": "Mohaimenul Azam Khan Raiaan",
    "first_author_surname": "Raiaan",
    "year": 2024,
    "cited_by_count": 444,
    "venue": "",
    "size": 36.642256032629064,
    "color": "hsl(0, 70%, 60%)",
    "label": "Raiaan ,2024",
    "rag_problem": "The rapid and overwhelming growth of research contributions in Large Language Models has made it difficult to obtain a comprehensive and concise understanding of the field.",
    "rag_method": "Conducting a thorough review of existing literature, covering aspects such as architectures, applications, taxonomies, and open issues to provide a clear synthesis of the current state of Large Language Models.\n\n**Explanation:** By systematically reviewing the literature on Large Language Models, the authors aim to consolidate and clarify the complex and rapidly changing information in the field. This includes summarizing architectures, applications, and challenges, which helps researchers and practitioners access a comprehensive overview in one consolidated resource, thus reducing complexity and aiding navigation.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The study is limited by the unavailability of review papers directly related to the topic of LLMs, restricting the ability to perform broad comparisons and evaluations.\n- The analysis predominantly focuses on the ground-level concepts of LLM configurations and applications, leaving detailed analysis of specific architectures and advanced topics inadequately covered.\n- Limited resources, time, and page constraints prevent extensive exploration of individual LLM architectures, impacting the depth of examination provided.\n- While the impact of LLMs across various domains is highlighted, assessing their practical impacts, especially on social aspects, remains complex and subjective.",
    "rag_future_work": "- Enhance Bias Mitigation: Future work involves refining training data and implementing effective debiasing techniques, along with integrating continuous monitoring and auditing mechanisms to ensure fairness and impartiality in language models.\n- Ethical and Responsible AI Deployment: Research must focus on establishing guidelines and practices for the ethical development and deployment of LLMs, ensuring they are used responsibly in various domains.\n- Improve Robustness and Controllability: There's a need for further research into enhancing the robustness and controllability of LLMs to increase their dependability and efficacy across applications.\n- Explore Societal Impacts: Future studies should investigate the societal effects of LLMs, focusing on how they can be utilized to solve complex problems and contribute positively to various fields such as healthcare and education.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 72
  },
  {
    "id": "W4392193048",
    "x": 2024.251240607185,
    "y": -58.13231416107073,
    "title": "Adapted large language models can outperform medical experts in clinical text summarization",
    "authors": [
      "Dave Van Veen",
      "Cara Van Uden",
      "Louis Blankemeier"
    ],
    "first_author": "Dave Van Veen",
    "first_author_surname": "Veen",
    "year": 2024,
    "cited_by_count": 443,
    "venue": "",
    "size": 36.631689246190966,
    "color": "hsl(0, 70%, 60%)",
    "label": "Veen ,2024",
    "rag_problem": "Clinicians spend significant time analyzing and summarizing vast amounts of textual information from electronic health records, which contributes to clinician burnout and detracts from direct patient care.",
    "rag_method": "Adapting large language models (LLMs) using techniques like in-context learning and quantized low-rank adaptation to perform clinical text summarization tasks such as summarizing radiology reports, patient questions, and progress notes.\n\n**Explanation:** By applying adaptation methods to LLMs for specific clinical summarization tasks, the models can generate summaries that are more complete, correct, and concise compared to those produced by medical experts. This reduces the documentation burden, potentially freeing up time for clinicians to focus more on patient care.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method does not address the context-specific preferences in summarization, as different specialists may require different styles or details tailored to their expertise, which is not fully explored in this study.\n- The system does not incorporate longitudinal context, such as references to prior studies, limiting the scope when medical experts provide summaries needing historical data.\n- The model's prompt engineering is limited, as only a minimal set of temperature values and instruction options were explored, leaving room for potential improvement in optimizing these elements.\n- The potential bias in large language models is not fully addressed, suggesting that summary qualities might depend on demographics or group membership, which requires further investigation.",
    "rag_future_work": "- Explore adaptation of summarization to specific clinical contexts and preferences by using curated examples tailored to different specialties or individual clinicians.\n- Investigate the integration of additional context and longitudinal information into the LLM to address the limitation of summarizing radiology reports that reference prior studies.\n- Examine potential biases in LLM outputs by considering summary qualities in relation to demographic group membership, despite the lack of demographic data in current datasets.\n- Conduct more extensive research on model temperature variations and prompt engineering to optimize LLM performance, beyond the limited scope currently explored.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 41
  },
  {
    "id": "W4400324908",
    "x": 2023.705973218329,
    "y": -56.28540046530098,
    "title": "Evaluation and mitigation of the limitations of large language models in clinical decision-making",
    "authors": [
      "Paul Hager",
      "Friederike Jungmann",
      "Robbie Holland"
    ],
    "first_author": "Paul Hager",
    "first_author_surname": "Hager",
    "year": 2024,
    "cited_by_count": 329,
    "venue": "",
    "size": 28.19036710677692,
    "color": "hsl(0, 70%, 60%)",
    "label": "Hager ,2024",
    "rag_problem": "Large language models (LLMs) excel on medical licensing exams but struggle to assess skills necessary for real-world clinical decision-making, such as gathering information, adhering to guidelines, and integrating into clinical workflows.",
    "rag_method": "Develop a framework to evaluate and improve the capabilities of LLMs specifically for clinical decision-making by identifying their limitations and designing interventions that enhance LLMs ability to handle comprehensive clinical tasks.\n\n**Explanation:** The framework targets the specific limitations of LLMs in clinical settings by focusing on their ability to gather relevant clinical information, adhere to established medical guidelines, and smoothly integrate into existing clinical workflows. By assessing these areas, bespoke interventions can be applied to modify LLM behavior and enhance their suitability for real-world clinical environments. This approach ensures that LLMs are not only proficient in exam scenarios but also capable of practical clinical tasks.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method may struggle with adequately gathering information needed for effective clinical decision-making, beyond what is tested in medical exams.\n- The approach has limitations in adhering to clinical guidelines and protocols within a realistic deployment environment.\n- Integrating our method into existing clinical workflows and decision-making processes remains a challenge that needs further attention.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4390919701",
    "x": 2024.5982933308844,
    "y": -48.863330520561796,
    "title": "Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications",
    "authors": [
      "Rajesh Bhayana"
    ],
    "first_author": "Rajesh Bhayana",
    "first_author_surname": "Bhayana",
    "year": 2024,
    "cited_by_count": 227,
    "venue": "",
    "size": 26.801025031647427,
    "color": "hsl(0, 70%, 60%)",
    "label": "Bhayana ,2024",
    "rag_problem": "Radiologists face challenges in efficiently interpreting and managing large volumes of unstructured textual data from various sources, such as medical records and research papers.",
    "rag_method": "Utilization of transformer-based large language models (LLMs) like ChatGPT to process and understand this unstructured data.\n\n**Explanation:** Transformer-based LLMs have the capability to comprehend language contextually and are trained on vast amounts of data, enabling them to extract relevant information and potentially automate the summarization, classification, and retrieval of pertinent data for radiologists. This reduces the time and cognitive load involved in manual processing of unstructured textual information.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Further exploration of large language models (LLMs) in radiology to improve diagnostic accuracy and workflow automation would be beneficial, given their recent advancements in contextual understanding and performance.\n- Research into integrating transformer-based chatbots with existing radiology information systems could optimize data querying and information retrieval processes.\n- Investigating the potential ethical implications and safety concerns of LLMs in radiologic practices may be necessary as their usage becomes more widespread in clinical settings.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2905810301",
    "x": 2018.4304224761213,
    "y": -9.175248443236445,
    "title": "A guide to deep learning in healthcare",
    "authors": [
      "Andre Esteva",
      "Alexandre Robicquet",
      "Bharath Ramsundar"
    ],
    "first_author": "Andre Esteva",
    "first_author_surname": "Esteva",
    "year": 2018,
    "cited_by_count": 3832,
    "venue": "",
    "size": 51.43193052764917,
    "color": "hsl(42, 70%, 60%)",
    "label": "Esteva ,2018",
    "rag_problem": "Difficulty in accurately diagnosing diseases due to complex and high-dimensional medical data.",
    "rag_method": "Utilization of deep learning models to process large volumes of complex data and identify patterns that indicate specific diseases.\n\n**Explanation:** Deep learning models, such as convolutional neural networks, excel at processing large datasets and can identify intricate patterns within medical imaging or patient data that correlate with particular diseases. This capability enhances diagnostic accuracy compared to traditional statistical methods.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3150212014",
    "x": 2021.1982382386825,
    "y": -9.128871156686285,
    "title": "Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis",
    "authors": [
      "Ravi Aggarwal",
      "Viknesh Sounderajah",
      "Guy Martin"
    ],
    "first_author": "Ravi Aggarwal",
    "first_author_surname": "Aggarwal",
    "year": 2021,
    "cited_by_count": 741,
    "venue": "",
    "size": 39.04368108231951,
    "color": "hsl(21, 70%, 60%)",
    "label": "Aggarwal ,2021",
    "rag_problem": "There is high heterogeneity and variance in the design, methodology, and reporting standards of deep learning studies for diagnostic accuracy in medical imaging, leading to overestimation of diagnostic accuracy and uncertainty in clinical applicability.",
    "rag_method": "Development of standardized guidelines such as AI-specific STARD and TRIPOD statements to enhance the consistency and reporting standards of DL diagnostic studies.\n\n**Explanation:** Standardized guidelines like AI-specific STARD and TRIPOD provide a framework for consistent reporting and study design, which would reduce heterogeneity, improve transparency, and allow for better evaluation of diagnostic accuracy. By aligning studies under these guidelines, the results become more comparable and reliable, facilitating the assessment of clinical applicability of DL technologies in medical imaging.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method's estimates of diagnostic performance may represent an overestimation of true accuracy due to the inclusion of studies with potential methodological deficiencies or poor reporting.\n- We did not conduct a quality assessment of transparency in reporting because existing guidelines are not fully applicable to the specifics of deep learning research.\n- Our study could not perform classical statistical comparisons of diagnostic accuracy measures between different imaging modalities due to the inherent nature of deep learning studies.\n- We were unable to separate and compare different subsets of imaging modalities, limiting our ability to break down heterogeneity and variance in the data.",
    "rag_future_work": "- Establish large, open-source, diverse anonymized datasets with annotations through government support to enhance the reproducibility of deep learning models.\n- Collaborate with academic centers to leverage their expertise in pragmatic trial design and methodology, implementing novel experimental and quasi-experimental methods to evaluate deep learning in clinical settings.\n- Conduct ongoing evaluation of algorithms in clinical practice to ensure they continue learning and adapting to the populations in which they are deployed.\n- Adopt recommended practices for the quality enhancement of deep learning research to facilitate its advancement in the future.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 37
  },
  {
    "id": "W2747680751",
    "x": 2022.1580696490994,
    "y": -15.04479603909808,
    "title": "Natural language processing: state of the art, current trends and challenges",
    "authors": [
      "Diksha Khurana",
      "Aditya Koli",
      "Kiran Khatter"
    ],
    "first_author": "Diksha Khurana",
    "first_author_surname": "Khurana",
    "year": 2022,
    "cited_by_count": 1514,
    "venue": "",
    "size": 42.396453858787346,
    "color": "hsl(14, 70%, 60%)",
    "label": "Khurana ,2022",
    "rag_problem": "Ambiguity in natural language processing, which makes understanding and generation of text by computers challenging.",
    "rag_method": "Various disambiguation techniques such as Minimising Ambiguity, Preserving Ambiguity, Interactive Disambiguity, and Weighting Ambiguity.\n\n**Explanation:** Disambiguation techniques help in resolving ambiguities by providing context or statistical measures, ensuring that the intended meaning is captured by computational models. These methods enable systems to correctly interpret sentences even when words may have multiple meanings or syntactic roles.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method currently relies heavily on transforming natural language into machine language, which may limit its ability to fully understand and deliver answers to complex queries.\n- There is still a long way to go before our approach can make data more user-friendly and easily accessible without the need for a graphical user interface, impacting user experience.",
    "rag_future_work": "- Develop NLP systems capable of automatically filtering and formulating answers through understanding semantic relationships, instead of merely presenting raw data to users.\n- Enhance AI components within NLP to improve their ability to genuinely understand and respond to user queries, transitioning from machine translation to delivering meaningful answers.\n- Improve accessibility and user-friendliness of data through voice and text commands, enabling users to interact seamlessly with personal data chatbots without reliance on graphical interfaces.\n- Advance NLP technology to enable real-time interaction with data, such as assessing customer sentiment or predicting brand perception, while encouraging mobile, on-the-go accessibility.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 31
  },
  {
    "id": "W4367310920",
    "x": 2023.5462955249284,
    "y": -29.834932504194864,
    "title": "Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum",
    "authors": [
      "John W. Ayers",
      "Adam Poliak",
      "Mark Dredze"
    ],
    "first_author": "John W. Ayers",
    "first_author_surname": "Ayers",
    "year": 2023,
    "cited_by_count": 1844,
    "venue": "",
    "size": 43.32204981856254,
    "color": "hsl(7, 70%, 60%)",
    "label": "Ayers ,2023",
    "rag_problem": "The surge in patient messages in virtual healthcare is causing increased workload and burnout among healthcare professionals, who must provide quality and empathetic responses.",
    "rag_method": "Utilization of an AI chatbot assistant, specifically ChatGPT, to draft initial responses to patient questions which can then be reviewed by clinicians.\n\n**Explanation:** By employing an AI chatbot, the initial burden of generating responses is shifted from healthcare professionals to the AI, which can efficiently draft potentially high-quality and empathetic answers. This allows healthcare professionals to focus on reviewing and editing these drafts rather than creating them from scratch, reducing workload and mitigating burnout.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate the ability of AI chatbots to generate specialized knowledge across a broader range of medical specialties. This could expand the tool's applicability and utility in diverse clinical scenarios.\n- Explore the integration of AI chatbot responses with clinician workflows to determine how such technology can best collaborate with human professionals in reducing burnout.\n- Assess the accuracy and empathy of AI responses in comparison to human clinicians across different types of patient inquiries to refine AI algorithms for better interaction quality.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4391136507",
    "x": 2024.4396392762992,
    "y": -69.10283085350481,
    "title": "A Survey on Evaluation of Large Language Models",
    "authors": [
      "Yupeng Chang",
      "Xu Wang",
      "Jindong Wang"
    ],
    "first_author": "Yupeng Chang",
    "first_author_surname": "Chang",
    "year": 2024,
    "cited_by_count": 1741,
    "venue": "",
    "size": 43.05223253663022,
    "color": "hsl(0, 70%, 60%)",
    "label": "Chang ,2024",
    "rag_problem": "Current evaluation protocols are not sufficient to comprehensively assess the true capabilities and potential risks of Large Language Models (LLMs). Existing evaluations often lack depth in assessing societal impact, robustness, and evolutionary dynamics of LLMs.",
    "rag_method": "The paper proposes a shift towards dynamic and evolving evaluation systems that go beyond static benchmark tests, integrating human-in-the-loop and principled evaluation strategies. Additionally, the paper outlines the necessity to design AGI benchmarks and complete behavioral evaluations.\n\n**Explanation:** Dynamic and evolving evaluation systems would allow assessments to adapt to the rapid development of LLMs, thus avoiding memorization issues with static data. Human-in-the-loop evaluations provide a more nuanced understanding of model performance in real-world contexts. AGI benchmarks and behavioral evaluations ensure a comprehensive understanding of LLMs' capabilities, including societal impact and interaction in open environments.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The survey highlights the inability of current evaluation systems to fully adapt and evolve, which may hinder the precise assessment of LLMs' capabilities and limitations.\n- The paper indicates the struggle to accurately evaluate reasoning and robustness tasks, suggesting limitations in the current methodologies employed for LLM evaluation.\n- Although the survey provides a comprehensive overview, it implies gaps in fully understanding the inherent challenges and opportunities within the evaluation tasks, protocols, and benchmarks of LLMs.",
    "rag_future_work": "- Redesign Evaluation Protocols: Redesigning current evaluation protocols to accurately assess the true capabilities of large language models (LLMs) and other AI models, considering the paradigm shift these models introduce.\n- Treat Evaluation as a Discipline: Advocate for the establishment of evaluation as a standalone discipline to systematically drive progress and address the complexities intrinsic to LLM assessments.\n- Explore New Evaluation Metrics: Develop new metrics and methods to unveil the varied dimensions of LLM capabilities, ensuring comprehensive evaluation beyond existing protocols.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 63
  },
  {
    "id": "W4387500346",
    "x": 2022.7446977441857,
    "y": -24.48284126011724,
    "title": "The future landscape of large language models in medicine",
    "authors": [
      "Jan Clusmann",
      "Fiona R. Kolbinger",
      "Hannah Sophie Muti"
    ],
    "first_author": "Jan Clusmann",
    "first_author_surname": "Clusmann",
    "year": 2023,
    "cited_by_count": 732,
    "venue": "",
    "size": 38.986361847148885,
    "color": "hsl(7, 70%, 60%)",
    "label": "Clusmann ,2023",
    "rag_problem": "Misinformation and biases in large language models can lead to inaccuracies and potentially harmful recommendations in clinical settings.",
    "rag_method": "Enhance LLMs through reinforcement learning from human feedback and continuous real-time updates for factual accuracy.\n\n**Explanation:** By incorporating human feedback into the learning process, models like GPT-4 improve in producing more credible outputs compared to previous versions, which helps in reducing misinformation. Additionally, implementing real-time updates allows LLMs to access the latest scientific data, thereby preventing static knowledge and reducing the risk of outdated information being spread.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to ensure the factual accuracy of LLM outputs, which can lead to misinformation with potentially harmful consequences in clinical settings.\n- The current lack of accountability in our approach with LLMs limits their clinical applicability, as there are no mechanisms to verify the correctness of outputs, posing risks for patient care.\n- Safety guardrails implemented in our method may inadvertently obscure important symptom variations between different demographics, such as between men and women.\n- Our method faces challenges due to privacy concerns and data leakage risks, as sensitive medical data is routinely exchanged in clinical environments, necessitating stricter security measures.",
    "rag_future_work": "- Develop mechanisms to ensure the accuracy of LLM outputs, addressing concerns about misinformation and hallucination, especially in clinical settings where errors could have severe consequences.\n- Implement and refine safety guardrails for LLMs that prevent bias without overlooking symptoms for different demographics, ensuring ethical and balanced medical application.\n- Explore new functionalities and applications of LLMs, such as integrating visual input and plugins, to expand their utility in medicine and other fields.\n- Investigate tailored versions of LLMs specifically designed for medical applications, focusing on training with medical data to enhance relevance and reliability.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 22
  },
  {
    "id": "W4386697749",
    "x": 2023.1304550449677,
    "y": -18.963919227664636,
    "title": "A foundation model for generalizable disease detection from retinal images",
    "authors": [
      "Yukun Zhou",
      "Mark A. Chia",
      "Siegfried K. Wagner"
    ],
    "first_author": "Yukun Zhou",
    "first_author_surname": "Zhou",
    "year": 2023,
    "cited_by_count": 603,
    "venue": "",
    "size": 38.07716915446164,
    "color": "hsl(7, 70%, 60%)",
    "label": "Zhou ,2023",
    "rag_problem": "Traditional AI models for disease detection from retinal images require substantial amounts of labeled data, which is both time-consuming and resource-intensive to gather, limiting their generalizability and applicability in diverse clinical settings.",
    "rag_method": "The RETFound foundation model utilizes self-supervised learning to pre-train on 1.6 million unlabeled retinal images, learning a robust and generalizable feature representation that can be adapted with less labeled data to various disease detection tasks.\n\n**Explanation:** By using self-supervised learning (SSL), RETFound leverages a vast amount of unlabeled data to learn general-purpose features that can be easily fine-tuned for specific disease detection tasks, reducing the reliance on large labeled datasets. This approach not only alleviates the expert annotation workload but also improves the model's generalization capability across different tasks, including both ocular and systemic diseases.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- RETFound's development data primarily comes from UK cohorts, which may limit its generalizability across diverse populations and necessitates exploring the use of more global datasets for comprehensive evaluation.\n- The multimodal information fusion between CFP and OCT has not been investigated, which could potentially enhance RETFound's performance in disease prediction.\n- Clinically relevant covariates such as demographics and visual acuity have not been integrated into the SSL models used in RETFound, possibly limiting its applicability in ocular and oculomic research.",
    "rag_future_work": "- Explore the impact of incorporating a larger and more diverse dataset of retinal images from worldwide cohorts to enhance the generalizability of RETFound.\n- Investigate the fusion of multimodal information between CFP and OCT modalities to potentially improve disease prediction performance.\n- Integrate clinically relevant information, such as demographics and visual acuity, into SSL models to strengthen RETFound's capabilities in ocular and systemic research.\n- Examine the differential strengths of CFP and OCT in predicting systemic diseases and understand their unique contributions to the early detection of disorders like stroke and Parkinson's disease.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 44
  },
  {
    "id": "W4366989525",
    "x": 2022.7433779898138,
    "y": -17.20714771342874,
    "title": "ChatGPT and the rise of large language models: the new AI-driven infodemic threat in public health",
    "authors": [
      "Luigi De Angelis",
      "Francesco Baglivo",
      "Guglielmo Arzilli"
    ],
    "first_author": "Luigi De Angelis",
    "first_author_surname": "Angelis",
    "year": 2023,
    "cited_by_count": 574,
    "venue": "",
    "size": 37.84606044679593,
    "color": "hsl(7, 70%, 60%)",
    "label": "Angelis ,2023",
    "rag_problem": "The rise of large language models like ChatGPT poses a new AI-driven 'infodemic' threat in public health by potentially spreading misinformation through their outputs.",
    "rag_method": "Implement mechanisms to evaluate and control the quality of information generated by large language models to prevent the dissemination of misinformation.\n\n**Explanation:** By developing evaluation and filtering techniques for the outputs of large language models, it becomes possible to identify and rectify any misinformation before it can be disseminated to the public. These measures can include integrating fact-checking algorithms and increasing model transparency, thus reducing the risk of an infodemic.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4224308101",
    "x": 2021.587863281088,
    "y": -17.106186591710546,
    "title": "PaLM: Scaling Language Modeling with Pathways",
    "authors": [
      "Aakanksha Chowdhery",
      "Sharan Narang",
      "Jacob Devlin"
    ],
    "first_author": "Aakanksha Chowdhery",
    "first_author_surname": "Chowdhery",
    "year": 2022,
    "cited_by_count": 2113,
    "venue": "",
    "size": 43.96131487514878,
    "color": "hsl(14, 70%, 60%)",
    "label": "Chowdhery ,2022",
    "rag_problem": "Existing large language models struggle with few-shot learning, requiring significant task-specific training data and facing limitations in performance scalability.",
    "rag_method": "The Pathways Language Model (PaLM) utilizes a 540-billion parameter dense Transformer framework with efficient scaling capabilities through Pathways infrastructure.\n\n**Explanation:** PaLM leverages the Pathways system to facilitate training across multiple TPU v4 Pods, achieving high efficiency in model FLOPs utilization, which enables the model to handle diverse NLP, code, and reasoning tasks with fewer task-specific examples while dramatically improving few-shot learning capabilities and scalability.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Despite pushing the boundaries of scale for few-shot language modeling, there remain open questions regarding the ideal network architecture and training scheme, indicating that PaLM is just an initial step and not the final solution.\n- The model's ability to improve prediction quality via explicit inference chains presents critical implications, yet it suggests that significant language generation capabilities are beneficial even when such generation is not typically required, implying a need for further exploration in this area.\n- Improvement patterns suggest that certain capabilities only emerge at significant scales, revealing discontinuous gains and indicating more capabilities might be unlocked with future scaling efforts.\n- Fairness evaluations are limited due to a lack of standardized benchmarks, an understanding of bias harm, and comprehensive coverage of identities, revealing potential measurement risks and biases in popular tasks.",
    "rag_future_work": "- Investigate Trade-offs: Future work will explore the trade-offs between various factors affecting language model capabilities, such as model architecture, pre-training tasks, and optimizer configuration, to enhance generalization.\n- Novel Architectural Choices: The authors aim to explore diverse architectural choices and training schemes beyond the dense, decoder-only Transformer model to leverage Pathways for broad generalization capabilities across multiple modalities.\n- Continued Scaling Improvements: As the scaling curve has not plateaued, future research could focus on further scaling models to potentially unlock new capabilities that emerge at larger scales.\n- Data Utilization Strategies: There is a need to reevaluate the impact of repeated versus unseen data in large-scale language model training, especially once high-quality web data begins to repeat with extended token counts.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 84
  },
  {
    "id": "W4307079201",
    "x": 2021.5392624124288,
    "y": -12.368637835641026,
    "title": "Scaling Instruction-Finetuned Language Models",
    "authors": [
      "Hyung Won Chung",
      "Le Hou",
      "Shayne Longpre"
    ],
    "first_author": "Hyung Won Chung",
    "first_author_surname": "Chung",
    "year": 2022,
    "cited_by_count": 1172,
    "venue": "",
    "size": 41.19473906075581,
    "color": "hsl(14, 70%, 60%)",
    "label": "Chung ,2022",
    "rag_problem": "Pre-trained language models often struggle to generalize effectively to unseen tasks when using direct, non-instruction-based finetuning.",
    "rag_method": "Instruction finetuning, especially with an increased number of tasks and chain-of-thought data, improves generalization capabilities.\n\n**Explanation:** By finetuning language models with instruction-based datasets and integrating chain-of-thought (CoT) data, models like Flan-PaLM are able to better understand and solve tasks by reasoning through step-by-step instructions. This enhances their ability to tackle unseen tasks, as they learn more flexible inferencing strategies via instructions and reasoning patterns.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The improvement from instruction finetuning does not scale proportionately with model size. While instruction finetuning improves the performance of both smaller and larger models, the effect is less pronounced in larger models due to the higher baseline performance, leading to a smaller percentage improvement.\n- Flan-PaLM, similar to its foundation model PaLM, shares the same inherent limitations of the PaLM architecture and frameworks, which may still face challenges such as those related to domain-specific knowledge or reasoning tasks below human-level performance.",
    "rag_future_work": "- Investigate the retention of multi-task abilities in large-scale models like PaLM and U-PaLM 540B, ensuring these models do not lose efficiency by performing only single tasks.\n- Evaluate the effectiveness of instruction-finetuned models specifically for single-task finetuning, as the current study primarily focuses on unseen tasks in few-shot settings.\n- Further explore the impact of using Chain-of-Thought (CoT) prompting for evaluation, to understand if it consistently enhances performance for various models.\n- Expand on the compatibility and performance improvements of instruction finetuning across different model sizes, architectures, and pre-training objectives, possibly by experimenting with other large-scale models.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 91
  },
  {
    "id": "W3162922479",
    "x": 2021.4776597636921,
    "y": 1.8234863604969502,
    "title": "What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams",
    "authors": [
      "Di Jin",
      "Eileen Pan",
      "Nassim Oufattole"
    ],
    "first_author": "Di Jin",
    "first_author_surname": "Jin",
    "year": 2021,
    "cited_by_count": 322,
    "venue": "",
    "size": 28.109804040034966,
    "color": "hsl(21, 70%, 60%)",
    "label": "Jin ,2021",
    "rag_problem": "Existing OpenQA systems struggle to answer complex medical questions that require extensive domain-specific knowledge and multi-hop logical reasoning.",
    "rag_method": "Development of MedQA, a large-scale open-domain multiple-choice dataset sourced from medical board exams and accompanied by a vast collection of medical textbook data.\n\n**Explanation:** MedQA provides a comprehensive platform where models can be trained to perform document retrieval and comprehension of advanced medical concepts from textbooks. This setup encourages testing on real-world medical scenarios rather than simplified datasets, promoting the development of models capable of multi-hop reasoning and domain-specific knowledge integration.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method currently struggles to achieve good performance on the MEDQA dataset, indicating that the complexity and domain-specific nature of the questions are challenging for state-of-the-art models.\n- Despite the large-scale and diverse nature of the dataset across multiple languages, current implementations with document retrieval and reading comprehension are insufficient for reliably solving the real-world medical problems presented.",
    "rag_future_work": "- Explore the development of more advanced OpenQA models capable of handling the complex real-world medical problems presented in the MEDQA dataset.\n- Investigate the integration of domain-specific knowledge systems to improve the capabilities of OpenQA datasets in understanding and answering medical examination questions.\n- Enhance document retrieval and reading comprehension components to improve performance on multilingual questions within the MEDQA dataset.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 31
  },
  {
    "id": "W2972522091",
    "x": 2018.8857009356032,
    "y": 20.24480012576838,
    "title": "PubMedQA: A Dataset for Biomedical Research Question Answering",
    "authors": [
      "Qiao Jin",
      "Bhuwan Dhingra",
      "Zheng¬≠ping Liu"
    ],
    "first_author": "Qiao Jin",
    "first_author_surname": "Jin",
    "year": 2019,
    "cited_by_count": 21,
    "venue": "",
    "size": 18.0147393764268,
    "color": "hsl(35, 70%, 60%)",
    "label": "Jin ,2019",
    "rag_problem": "Existing biomedical QA datasets either lack sufficient scale or require primarily factual answers, limiting their ability to assess reasoning over complex scientific texts.",
    "rag_method": "PubMedQA dataset was developed to include a substantial number of instances that require reasoning over biomedical research abstracts, tackling questions with yes/no/maybe answers.\n\n**Explanation:** PubMedQA incorporates questions derived from PubMed articles with abstracts serving as contexts, requiring models to interpret and reason over significant biomedical research, rather than merely extracting facts. This setup uniquely challenges models to engage in scientific reasoning, a step forward from datasets focusing on factual extraction, filling the gap for benchmarking reasoning abilities in biomedical QA.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Despite multi-phase fine-tuning of BioBERT with additional supervision, the method's results are much worse compared to single-human performance, indicating limitations in handling complex biomedical question answering tasks.\n- The lack of annotated data means training only on the final phase (PQA-L) yields results similar to the majority baseline, highlighting a dependency on large annotated datasets for improved performance.\n- Improvements observed with pre-training on large automatically collected datasets are significant but still limited, suggesting that more annotated data may be necessary for achieving human-level performance.",
    "rag_future_work": "- Develop methods to account for context-dependent nuances in answer annotation, enhancing the dataset's ability to reflect complex scenarios beyond binary responses.\n- Explore strategies to incorporate a wider range of experimental conditions and outcomes in the dataset, providing a more comprehensive framework for question answering in complex biomedical studies.\n- Investigate the effectiveness of employing advanced natural language processing techniques to predict answers that are not strictly binary, improving the ability to handle 'maybe' responses in varied biomedical contexts.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 19
  },
  {
    "id": "W3083410900",
    "x": 2020.26789870028,
    "y": -5.258374148199716,
    "title": "Measuring Massive Multitask Language Understanding",
    "authors": [
      "Dan Hendrycks",
      "Collin Burns",
      "Steven Basart"
    ],
    "first_author": "Dan Hendrycks",
    "first_author_surname": "Hendrycks",
    "year": 2020,
    "cited_by_count": 252,
    "venue": "",
    "size": 27.1919747947828,
    "color": "hsl(28, 70%, 60%)",
    "label": "Hendrycks ,2020",
    "rag_problem": "Existing NLP benchmarks do not adequately assess the breadth and depth of models' language understanding across diverse academic and professional subjects, leading to an incomplete evaluation of their capabilities.",
    "rag_method": "The paper proposes a new benchmark that measures multitask accuracy across 57 diverse subjects, ranging from STEM and humanities to social sciences, in zero-shot and few-shot settings.\n\n**Explanation:** By evaluating models across a wide range of subjects without fine-tuning, the benchmark reveals the actual knowledge that models have acquired during pretraining. This identifies blind spots and assesses world knowledge and problem solving abilities, providing a comprehensive picture of a model's capabilities beyond traditional linguistic benchmarks.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to accurately model human approval and moral scenarios, as indicated by poor performance on Professional Law and Moral Scenarios tasks.\n- There are significant challenges in achieving expert-level performance across all subjects, as our approach does not match expert standards (90%) in any domain, resulting in subhuman accuracy overall.\n- Attempts to enhance performance in Professional Law tasks through additional specialized pretraining data have shown limited success, suggesting that more high-quality text might not be sufficient to drastically improve model performance.\n- Our benchmark only supports text-based evaluation, lacking incorporation of multimodal inputs such as images and audio, which are important for conveying certain concepts that text alone cannot capture.",
    "rag_future_work": "- Investigate the scalability of language models while addressing data limitations: Future research should focus on the balance between increasing model size and the availability of diverse and esoteric data sources to prevent bottlenecks in scaling up language models.\n- Develop training methodologies that mimic human learning: New approaches should be explored which train models similar to human learning experiences, emphasizing reading and understanding comprehensive texts over rote memorization found in traditional question banks.\n- Improve model performance on tasks aligned with human values: Efforts should be directed at enhancing the accuracy of models in domains like Professional Law and Moral Scenarios, which are crucial for the alignment of AI systems with human values.\n- Enhance the ability of models to perform calculations and achieve expert-level proficiency: Future work should focus on improving model capabilities in arithmetic reasoning and specialized subjects to reach and exceed expert-level performance (90% accuracy) across various disciplines.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 25
  },
  {
    "id": "W2012976256",
    "x": 2015.2958994828607,
    "y": -4.199324254821985,
    "title": "Expanding horizons in the study of World Englishes with the 1.9 billion word Global Web-based English Corpus (GloWbE)",
    "authors": [
      "Mark Davies",
      "Robert Fuchs"
    ],
    "first_author": "Mark Davies",
    "first_author_surname": "Davies",
    "year": 2015,
    "cited_by_count": 379,
    "venue": "",
    "size": 28.720476665317648,
    "color": "hsl(64, 70%, 60%)",
    "label": "Davies ,2015",
    "rag_problem": "Lack of comprehensive and diverse data to study variations in World Englishes across different countries.",
    "rag_method": "Development of the Global Web-based English Corpus (GloWbE) with 1.9 billion words from 1.8 million web pages across 20 English-speaking countries.\n\n**Explanation:** GloWbE contains data from a wide variety of web-based sources, including informal blogs and other genres, allowing researchers to analyze linguistic variations and dialectal differences that were previously difficult to examine due to insufficient or homogeneous data sources. Its large size and diverse content enable the study of English use in various locales, providing insights into regional language characteristics and trends in World Englishes.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2791507392",
    "x": 2017.4309845519952,
    "y": 0.04774608844876027,
    "title": "The Effects of Corpus Use on Second Language Vocabulary Learning: A Multilevel Meta-analysis",
    "authors": [
      "Hansol Lee",
      "Mark Warschauer",
      "Jang Ho Lee"
    ],
    "first_author": "Hansol Lee",
    "first_author_surname": "Lee",
    "year": 2018,
    "cited_by_count": 218,
    "venue": "",
    "size": 26.649693935676403,
    "color": "hsl(42, 70%, 60%)",
    "label": "Lee ,2018",
    "rag_problem": "The difficulty of effectively learning vocabulary in a second language over both short-term and long-term periods.",
    "rag_method": "Utilizing corpus use as a method to enhance vocabulary acquisition in second language learning.\n\n**Explanation:** Corpus use provides contextualized examples and real-life usage patterns that help learners understand the meaning and usage of new vocabulary. The meta-analysis indicates that exposure to these rich linguistic environments leads to improved vocabulary retention and understanding, both shortly after learning and in the long-term. This fosters a deeper cognitive engagement with the vocabulary where learners can see practical applications and variations, enhancing memorization through contextual association.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate the differential effects of corpus use on learners with varying proficiency levels to understand how it might be tailored for diverse learner needs.\n- Conduct longitudinal studies that examine the sustained impact of corpus use beyond the immediate post-learning phase, exploring its effectiveness over extended periods.\n- Explore the integration of corpus use with other technology-enhanced language learning tools to assess combined effects on vocabulary acquisition.\n- Analyze the role of different types of corpora, such as specialized vs. general corpora, in enhancing vocabulary learning outcomes.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2067130497",
    "x": 2014.2975701875957,
    "y": 2.3020850356241835,
    "title": "ANNIS3: A new architecture for generic corpus query and visualization",
    "authors": [
      "Thomas Krause",
      "Amir Zeldes"
    ],
    "first_author": "Thomas Krause",
    "first_author_surname": "Krause",
    "year": 2014,
    "cited_by_count": 204,
    "venue": "",
    "size": 26.401463758170138,
    "color": "hsl(71, 70%, 60%)",
    "label": "Krause ,2014",
    "rag_problem": "Richly annotated, heterogeneous linguistic corpora require generic representation and querying capabilities that can handle multiple, potentially conflicting segmentation layers.",
    "rag_method": "ANNIS3 architecture introduces a well-defined concept of multiple segmentation layers within a general graph-based data model.\n\n**Explanation:** By integrating multiple segmentation layers, ANNIS3 allows for the representation and querying of linguistic corpora that have annotations from different sources and applications. This addresses the conflicts and complexities arising from heterogeneous data by providing a framework that can accommodate varying annotation schemes, thus enabling more flexible and comprehensive corpus analysis.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3089323922",
    "x": 2020.175827713308,
    "y": 8.149553166822384,
    "title": "The Oxford English Dictionary",
    "authors": [
      "Sarah Ogilvie"
    ],
    "first_author": "Sarah Ogilvie",
    "first_author_surname": "Ogilvie",
    "year": 2020,
    "cited_by_count": 172,
    "venue": "",
    "size": 25.763738836971665,
    "color": "hsl(28, 70%, 60%)",
    "label": "Ogilvie ,2020",
    "rag_problem": "Princeton WordNet lacks a distinction between word meanings that are systematically related (polysemy) and those that are coincidental (homonymy), which limits its utility as an ideal repository for research into these phenomena.",
    "rag_method": "Align WordNet with the Oxford English Dictionary (OED) using Transformer model embeddings to create a high-quality homonymy annotation layer.\n\n**Explanation:** By aligning WordNet with the Oxford English Dictionary, which explicitly distinguishes between different meanings of the same word into separate lemmas based on etymology, the proposed method uses sentence embeddings from Transformer models to identify the closest definitions in both resources. This allows for automatic splitting of homonymous senses into separate lemmas in the PWN. The method effectively resolves the ambiguity inherent in semantic clustering methods, ensuring that even figurative polysemous senses are correctly identified and labeled.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method currently lacks the integration of phonetic information from the OED, which limits its ability to identify and utilize homophony in the linguistic analysis.\n- The approach does not yet include an advanced sense-to-sense mapping to the OED, which restricts its capacity to exploit the detailed historical data on sense emergence for studying language change.",
    "rag_future_work": "- Enhance WordNet with phonetic information to better infer homophony, which could improve understanding of words with different meanings but similar sounds.\n- Develop complex models to establish a high-quality sense-to-sense mapping with the OED, enabling diachronic studies of language change by leveraging information about the dates of sense emergence.\n- Further improve methods for distinguishing between polysemy and homonymy, possibly applying more advanced language modelling techniques to refine homonymy identification.\n- Explore and validate the synthetic annotation layer created for WordNet with additional evaluation on larger datasets to ensure its reliability and enhance its practical application.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 20
  },
  {
    "id": "W2131243403",
    "x": 2004.4263535216153,
    "y": -4.385775712226704,
    "title": "AntConc: A Learner and Classroom Friendly, Multi-Platform Corpus Analysis Toolkit",
    "authors": [
      "Laurence Anthony"
    ],
    "first_author": "Laurence Anthony",
    "first_author_surname": "Anthony",
    "year": 2004,
    "cited_by_count": 151,
    "venue": "",
    "size": 25.277470558367877,
    "color": "hsl(141, 70%, 60%)",
    "label": "Anthony ,2004",
    "rag_problem": "Learners and educators often face difficulty in accessing and utilizing comprehensive corpus analysis tools in a classroom setting due to either software complexity or platform restrictions.",
    "rag_method": "AntConc provides a user-friendly, freeware, and multi-platform corpus analysis toolkit specifically designed for classroom use.\n\n**Explanation:** By offering a suite of tools such as a concordancer, word frequency generators, and cluster analysis in a unified and accessible interface, AntConc allows learners and educators to effectively engage with corpus analysis tasks without the typical barriers of software complexity or platform constraints. Its design focuses on ease of use, making it adaptable to various classroom technologies and learning environments.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- AntConc, while comprehensive, may not fully address the needs of advanced research applications due to its primary design focus on classroom use.\n- The toolkit might lack certain advanced features found in more specialized commercial software packages, potentially limiting its utility for complex or large-scale linguistic analyses.\n- Being freeware, ongoing updates and support for AntConc might be constrained by resource limitations, affecting the software's ability to keep pace with rapidly evolving technological advancements in corpus analysis.",
    "rag_future_work": "- Improvement of user interface to enhance accessibility and ease of use for learners and educators in the classroom.\n- Expansion of language support to cater to non-English corpora, providing more universal applications for the toolkit.\n- Development of advanced analytical tools to address current limitations in corpus analysis, offering deeper insights and functionalities.\n- Integration of collaborative features to facilitate group projects and shared learning experiences within academic settings.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2155870214",
    "x": 2009.4488666986706,
    "y": -6.488353093437798,
    "title": "The WaCky wide web: a collection of very large linguistically processed web-crawled corpora",
    "authors": [
      "Marco Baroni",
      "Silvia Bernardini",
      "Adriano Ferraresi"
    ],
    "first_author": "Marco Baroni",
    "first_author_surname": "Baroni",
    "year": 2009,
    "cited_by_count": 1164,
    "venue": "",
    "size": 41.16259567035884,
    "color": "hsl(106, 70%, 60%)",
    "label": "Baroni ,2009",
    "rag_problem": "There is a problem that requires creating and processing large-scale language datasets to support natural language processing research, but web-scraped data is noisy and has not undergone linguistic processing.",
    "rag_method": "Developed a method to scrape large amounts of web data and perform linguistic processing, making this data more suitable for language research and natural language processing tasks.\n\n**Explanation:** By performing linguistic processing on web-scraped data, such as lemmatization, tokenization, and syntactic analysis, the quality and utility of the data are enhanced. This enables researchers to conduct experiments and analysis more effectively on large-scale datasets, thereby supporting deeper language research and application development.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2345398358",
    "x": 2008.5309133765857,
    "y": 6.244746091369292,
    "title": "DeepDict ‚Äî A Graphical Corpus-based Dictionary of Word Relations",
    "authors": [
      "Eckhard Bick"
    ],
    "first_author": "Eckhard Bick",
    "first_author_surname": "Bick",
    "year": 2009,
    "cited_by_count": 13,
    "venue": "",
    "size": 16.316383726670512,
    "color": "hsl(106, 70%, 60%)",
    "label": "Bick ,2009",
    "rag_problem": "Learners and linguists lack an efficient, automated resource for discovering typical complementation patterns and collocations from corpus data.",
    "rag_method": "The creation of a lexical resource, DeepDict, which uses co-occurrence strength between mother-daughter dependency pairs from grammatically analyzed corpus data to generate dictionary entries.\n\n**Explanation:** By utilizing co-occurrence strength between dependency pairs, DeepDict offers automated extraction of word relations, allowing for quick access to typical complementation patterns and collocations. This approach effectively transforms corpus data into a practical resource similar to an advanced learner's dictionary, facilitating instant understanding of word usage patterns enriched by graphical user interface options for frequency thresholds.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Expand the lexical resource to include a wider range of grammatical structures and language variations, enhancing the depth and applicability of the dictionary entries.\n- Develop more sophisticated graphical interfaces that can dynamically adjust thresholds and frequencies to better cater to diverse user needs and preferences.\n- Integrate advanced machine learning algorithms to improve the accuracy and relevancy of predicted word relations, especially in complex dependency pairs.\n- Implement user feedback mechanisms to continuously refine the model and interface according to real-world usage patterns and linguistic evolution.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W1604834089",
    "x": 2004.4913522645911,
    "y": 4.1970622936387345,
    "title": "Language-Independent Methods for Compiling Monolingual Lexical Data",
    "authors": [
      "Christian Biemann",
      "Stefan Bordag",
      "Gerhard Heyer"
    ],
    "first_author": "Christian Biemann",
    "first_author_surname": "Biemann",
    "year": 2004,
    "cited_by_count": 62,
    "venue": "",
    "size": 21.96802624829001,
    "color": "hsl(141, 70%, 60%)",
    "label": "Biemann ,2004",
    "rag_problem": "The challenge of efficiently compiling lexical data for numerous languages without relying on language-specific methodologies.",
    "rag_method": "Develop language-independent methods that can compile monolingual lexical data regardless of the language being processed.\n\n**Explanation:** By using approaches that are not tailored to specific languages, the method allows for the compilation of lexical data across various languages uniformly. This circumvents the need for language-specific tools and resources which are often costly and difficult to produce for less-resourced languages. The language-independent paradigms ensure scalability and applicability to wide linguistic diversity without the need for individual tuning.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2044688197",
    "x": 2009.429930070371,
    "y": -2.785262428040033,
    "title": "The 385+ million word <i>Corpus of Contemporary American English</i> (1990‚Äì2008+)",
    "authors": [
      "Mark Davies"
    ],
    "first_author": "Mark Davies",
    "first_author_surname": "Davies",
    "year": 2009,
    "cited_by_count": 578,
    "venue": "",
    "size": 37.87862164888537,
    "color": "hsl(106, 70%, 60%)",
    "label": "Davies ,2009",
    "rag_problem": "The lack of a large, comprehensive, and genre-balanced corpus of contemporary American English, making linguistic research on contemporary language usage and variation difficult.",
    "rag_method": "The creation of the Corpus of Contemporary American English (COCA), which includes over 385 million words from 1990 to 2008, balanced across spoken language, fiction, popular magazines, newspapers, and academic journals.\n\n**Explanation:** COCA addresses this problem by providing a diverse, extensive collection of texts that represent varied genres, allowing researchers to perform detailed analyses across different forms of language expression. This comprehensive dataset facilitates the study of language trends, lexical frequency, and syntactic variation over an extended period, offering insights that were not possible with smaller or less balanced corpuses.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2911489562",
    "x": 2018.6423517944138,
    "y": -20.0615514841252,
    "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
    "authors": [
      "Jinhyuk Lee",
      "Wonjin Yoon",
      "Sungdong Kim"
    ],
    "first_author": "Jinhyuk Lee",
    "first_author_surname": "Lee",
    "year": 2019,
    "cited_by_count": 6148,
    "venue": "",
    "size": 58.771521045810644,
    "color": "hsl(35, 70%, 60%)",
    "label": "Lee ,2019",
    "rag_problem": "The performance of general NLP models, specifically BERT, is unsatisfactory in biomedical text mining tasks due to the domain-specific language and unique word distribution shifts present in biomedical corpora compared to general domain corpora.",
    "rag_method": "BioBERT, a domain-specific pre-trained language representation model, is developed by initializing with weights from general BERT and further pre-training it on large-scale biomedical corpora such as PubMed abstracts and PMC full-text articles.\n\n**Explanation:** By pre-training BERT on biomedical corpora, BioBERT is able to capture the unique characteristics and context-specific information of biomedical language, thus improving its ability to understand and process complex biomedical texts. This adaptation results in significant performance improvements across various biomedical text mining tasks such as named entity recognition, relation extraction, and question answering, as it can recognize specialized terminologies and intricate relationships inherent in the biomedical domain.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- BioBERT requires extensive computational resources for pre-training, which can be a limiting factor for institutions with constrained resources.\n- Our method's effectiveness is highly dependent on the quality and quantity of the biomedical text data used during pre-training, posing challenges when the available data is limited or biased.\n- Although BioBERT improves recognition of biomedical entities compared to general models, it may still struggle with certain complex entity recognition tasks where domain-specific subtleties are involved.",
    "rag_future_work": "- Develop updated versions of BioBERT, including BioBERT BASE and BioBERT LARGE, trained on only PubMed abstracts without any initialization from the existing BERT model to enhance its performance specifically for biomedical tasks.\n- Train BioBERT models utilizing domain-specific vocabulary based on WordPiece to potentially improve understanding and processing of specialized biomedical terminology.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 15
  },
  {
    "id": "W2970771982",
    "x": 2019.5541463701913,
    "y": -14.714600010315989,
    "title": "SciBERT: A Pretrained Language Model for Scientific Text",
    "authors": [
      "Iz Beltagy",
      "Kyle Lo",
      "Arman Cohan"
    ],
    "first_author": "Iz Beltagy",
    "first_author_surname": "Beltagy",
    "year": 2019,
    "cited_by_count": 2777,
    "venue": "",
    "size": 49.76870749172089,
    "color": "hsl(35, 70%, 60%)",
    "label": "Beltagy ,2019",
    "rag_problem": "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive.",
    "rag_method": "SCIBERT, a pretrained language model based on BERT trained on a large corpus of scientific text.\n\n**Explanation:** SCIBERT leverages unsupervised pretraining on scientific publications, allowing it to capture domain-specific linguistic patterns without needing extensive annotated data. By refining contextualized embeddings from scientific text, SCIBERT improves upon BERT's performance in scientific domain tasks, providing a solution that circumvents the need for large annotated datasets.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method currently lacks a version analogous to BERT-Large, limiting its performance compared to models with larger architectures.\n- SciBERT's effectiveness across multiple domains may be constrained due to the current proportion of papers used from each domain, indicating room for optimization in domain diversity.",
    "rag_future_work": "- Release a SCIBERT version analogous to BERT-Large, expanding the model's capacity and effectiveness in processing scientific texts.\n- Experiment with varying proportions of papers from different scientific domains to optimize SCIBERT's performance across multiple fields.\n- Develop a single language model resource that is cost-effective and useful across multiple scientific domains, addressing the limitation of high training costs.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 18
  },
  {
    "id": "W2971258845",
    "x": 2018.4053344576507,
    "y": -4.175326037395347,
    "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
    "authors": [
      "Yifan Peng",
      "Shankai Yan",
      "Zhiyong Lu"
    ],
    "first_author": "Yifan Peng",
    "first_author_surname": "Peng",
    "year": 2019,
    "cited_by_count": 797,
    "venue": "",
    "size": 39.38542701024642,
    "color": "hsl(35, 70%, 60%)",
    "label": "Peng ,2019",
    "rag_problem": "There is a lack of a standardized benchmarking system for evaluating language representations specifically in the biomedicine domain, which impedes fair comparison and development of NLP models specialized for this field.",
    "rag_method": "The Biomedical Language Understanding Evaluation (BLUE) benchmark is introduced to standardize the evaluation process by providing five distinct biomedicine text-mining tasks across ten datasets covering biomedical literature and clinical notes.\n\n**Explanation:** BLUE benchmark allows researchers to evaluate their language models on a consistent set of tasks and datasets, promoting fair comparisons and encouraging focused advancements in domain-specific language representations. This addresses the issue of disparate evaluation practices and varied dataset usage, providing a unified standard similar to the GLUE benchmark in the general domain.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- BERT-Large pre-trained on PubMed and MIMIC data performs worse overall compared to other models, indicating that these datasets may not be sufficient for effective pretraining of larger models.\n- The MIMIC-III dataset's relatively smaller size fails to adequately pretrain the BERT-Large model, limiting its performance on benchmark tasks.",
    "rag_future_work": "- Explore the integration of domain-specific knowledge into pre-trained language models to enhance their understanding and representation of complex biomedical texts.\n- Investigate the impact of additional training data from varied biomedical subfields on the model's performance across different benchmark datasets.\n- Develop and evaluate new algorithms or training techniques tailored specifically for biomedical applications to improve upon the existing frameworks like BERT and ELMo.\n- Perform comprehensive error analysis on current models to identify specific areas of improvement and potential solutions in biomedical natural language processing tasks.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 19
  },
  {
    "id": "W3160137267",
    "x": 2020.6635774289723,
    "y": -6.273021653662979,
    "title": "Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction",
    "authors": [
      "Laila Rasmy",
      "Yang Xiang",
      "Ziqian Xie"
    ],
    "first_author": "Laila Rasmy",
    "first_author_surname": "Rasmy",
    "year": 2021,
    "cited_by_count": 722,
    "venue": "",
    "size": 38.921842500360384,
    "color": "hsl(21, 70%, 60%)",
    "label": "Rasmy ,2021",
    "rag_problem": "Deep learning models require large datasets to train effectively, which can be a barrier in scenarios where large electronic health records (EHR) datasets are unavailable or difficult to annotate.",
    "rag_method": "Med-BERT, a pretrained contextualized embedding model on large-scale structured EHR data, adapted from the BERT framework to the EHR domain.\n\n**Explanation:** Med-BERT leverages the idea of transfer learning, similar to BERT in NLP, where the model is pretrained on large EHR datasets to develop contextual embeddings. These embeddings capture deep semantic relationships within the data, allowing for effective fine-tuning on smaller datasets. The model's design incorporates sequential dependencies and contextual semantics specific to structured EHR data, enabling it to boost prediction performance significantly, even with limited training samples.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Med-BERT currently uses only ICD-format diagnosis information, which may limit its ability to leverage other potentially valuable data types like time intervals, medications, procedures, and laboratory tests.\n- The temporal information between visits is not included in the model, potentially causing some loss of temporal context needed for accurate predictions.\n- The model does not fully explore the order of concepts within each visit and instead relies on code priorities, which might not adequately capture the necessary sequence of medical events.\n- Med-BERT does not outperform simpler models like logistic regression when the training sample size is very small (n < 500), indicating it might not be suitable for datasets with limited samples.",
    "rag_future_work": "- Conduct research on alternative contextualized embedding methodologies like ULMFiT, ELMo, and GPTs for pretraining and fine-tuning on electronic health records to evaluate their effectiveness compared to Med-BERT.\n- Investigate the inclusion of additional EHR data sources such as time intervals, medications, procedures, and laboratory tests to enhance model input and potentially improve predictive performance.\n- Design and test different pretraining and fine-tuning tasks beyond disease prediction to expand the applicability and utility of Med-BERT.\n- Explore task-specific visualizations and interpretations to provide deeper insights into the data semantics and model functioning, aiding in clinical validation and application.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 25
  },
  {
    "id": "W2955483668",
    "x": 2019.1552179389732,
    "y": 1.588483513815659,
    "title": "Enhancing clinical concept extraction with contextual embeddings",
    "authors": [
      "Yuqi Si",
      "Jingqi Wang",
      "Hua Xu"
    ],
    "first_author": "Yuqi Si",
    "first_author_surname": "Si",
    "year": 2019,
    "cited_by_count": 314,
    "venue": "",
    "size": 28.015565930300184,
    "color": "hsl(35, 70%, 60%)",
    "label": "Si ,2019",
    "rag_problem": "Clinical concept extraction tasks lack best practices for integrating advanced contextual embeddings such as ELMo and BERT, and traditional word embeddings like word2vec and GloVe fail to dynamically incorporate context-dependent semantic information into word representations.",
    "rag_method": "Utilizing contextual embeddings pre-trained on large clinical corpora like MIMIC-III significantly improves clinical concept extraction performance.\n\n**Explanation:** Contextual embeddings, specifically ELMo and BERT, dynamically adjust word representations based on surrounding context, capturing more nuanced semantic information. Pre-training these models on domain-specific data like clinical notes ensures they learn the specific linguistic patterns prevalent in medical texts, resulting in superior performance over traditional embeddings and achieving state-of-the-art results across several clinical concept extraction benchmarks.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method still faces the challenge of overfitting on the pre-training corpus if the pre-training process is not carefully limited, which could affect generalization to other data sets.\n- The performance of the BERT model on downstream tasks tends to decrease after reaching its optimal point due to the loss of information from the open-domain corpus over many iterations, suggesting limitations in long-term retention of diverse information during training.\n- The alignment of pre-trained models with clinical corpus data needs careful balancing of iterations to optimize performance, indicating a reliance on precise adjustment that may not be straightforward in different applications or with different corpora.",
    "rag_future_work": "- Explore the potential of task-specific fine-tuning for pre-trained deep language models using larger clinical corpora, which could lead to further improvements in clinical concept extraction tasks.\n- Investigate the semantic information provided by contextual embeddings to enhance understanding and extraction of complex clinical concepts not captured by traditional embeddings.\n- Develop new domain-specific embedding models that leverage unsupervised pretraining on clinical text corpora to achieve superior performance compared to existing off-the-shelf models.\n- Examine the application of contextual embeddings in other medical fields beyond clinical concept extraction to determine their effectiveness and adaptability in diverse healthcare contexts.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 19
  },
  {
    "id": "W2250539671",
    "x": 2013.6352038573996,
    "y": -11.39483952684072,
    "title": "Glove: Global Vectors for Word Representation",
    "authors": [
      "Jeffrey Pennington",
      "Richard Socher",
      "Christopher D. Manning"
    ],
    "first_author": "Jeffrey Pennington",
    "first_author_surname": "Pennington",
    "year": 2014,
    "cited_by_count": 32840,
    "venue": "",
    "size": 60,
    "color": "hsl(71, 70%, 60%)",
    "label": "Pennington ,2014",
    "rag_problem": "Existing word vector models like word2vec and traditional matrix factorization methods capture semantic regularities but do not explicitly address the origin and optimization of these regularities within their training methodologies.",
    "rag_method": "GloVe introduces a global log-bilinear regression model that explicitly factorizes the word-context co-occurrence matrix to optimize vector embeddings.\n\n**Explanation:** By factorizing the co-occurrence matrix, GloVe captures the statistical patterns directly from the corpus, ensuring the regularities are embedded in the vectors. This approach combines advantages from both matrix factorization (allowing global corpus-level optimization) and word2vec's local neighborhood prediction, offering a more interpretable and globally optimized word representation.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Explore the effects of different weighting functions on the GloVe and SGNS models to enhance their performance and understanding.\n- Investigate the bias terms convergence between the GloVe and SGNS models to determine if this represents a globally optimized value in empirical experiments.\n- Examine the differences in cost functions and weighting strategies between GloVe and SGNS, potentially refining their training objectives for improved similarity in results.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 8
  },
  {
    "id": "W2190333735",
    "x": 2015.3201706359678,
    "y": 1.4913841197817432,
    "title": "Annotating longitudinal clinical narratives for de-identification: The 2014 i2b2/UTHealth corpus",
    "authors": [
      "Amber Stubbs",
      "√ñzlem Uzuner"
    ],
    "first_author": "Amber Stubbs",
    "first_author_surname": "Stubbs",
    "year": 2015,
    "cited_by_count": 207,
    "venue": "",
    "size": 26.45605378862714,
    "color": "hsl(64, 70%, 60%)",
    "label": "Stubbs ,2015",
    "rag_problem": "Clinical narratives contain sensitive patient information that must be de-identified for use in research and other applications.",
    "rag_method": "Development of a comprehensive annotation scheme to identify and label identifiable information within clinical narratives for de-identification purposes.\n\n**Explanation:** By systematically annotating longitudinal clinical narratives, the scheme provides a structured approach to recognizing identifiable information. This enables automated systems to accurately remove or alter such information, ensuring patient privacy is maintained while allowing the data to be used for research.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W3122890974",
    "x": 2021.3726273888012,
    "y": -12.271122196058291,
    "title": "DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION",
    "authors": [
      "Pengcheng He",
      "Xiaodong Liu",
      "Jianfeng Gao"
    ],
    "first_author": "Pengcheng He",
    "first_author_surname": "He",
    "year": 2021,
    "cited_by_count": 922,
    "venue": "",
    "size": 40.068927924083084,
    "color": "hsl(21, 70%, 60%)",
    "label": "He ,2021",
    "rag_problem": "Traditional attention mechanisms in language models do not sufficiently account for both content and relative positional relationships, potentially limiting the model's ability to capture syntactical and contextual nuances.",
    "rag_method": "Disentangled attention mechanism with two separate vectors for content and position embeddings, calculating attention scores using both content-to-content and position-to-content matrices.\n\n**Explanation:** By disentangling attention into content and position components, the mechanism allows for a more nuanced capture of syntactical and contextual relationships between words, improving the model's understanding of language structures. This is because it accounts not only for word content but also for how the relative position of words influences their meaning and relevance in context.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Although DeBERTa aims to reduce additional parameters by sharing projection matrices, the involvement of large models still results in a significant increase in parameters, which may lead to higher computational costs.\n- The performance of the DeBERTa base model shows minimal improvement when its parameters are matched with those of RoBERTa, suggesting that the disentangled attention might not substantially enhance the model's effectiveness in certain scenarios.",
    "rag_future_work": "- A future direction involves optimizing the computational cost by fusing the attention computation kernel, which could significantly reduce the additional complexity introduced by the position-to-content and content-to-position attention scores.\n- Another area for exploration is enhancing DeBERTa's ability to incorporate compositional structures more explicitly, potentially allowing for a combination of neural and symbolic computation akin to human cognition.\n- Further research could examine integrating knowledge from various tasks to improve compositional generalization, enabling DeBERTa to better tackle new tasks with minimal task-specific demonstration.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 25
  },
  {
    "id": "W3180181113",
    "x": 2021.9213125583576,
    "y": -0.9226741655323636,
    "title": "A Survey on Data Augmentation for Text Classification",
    "authors": [
      "Markus Bayer",
      "Marc‚ÄìAndr√© Kaufhold",
      "Christian Reuter"
    ],
    "first_author": "Markus Bayer",
    "first_author_surname": "Bayer",
    "year": 2022,
    "cited_by_count": 357,
    "venue": "",
    "size": 28.496383036511926,
    "color": "hsl(14, 70%, 60%)",
    "label": "Bayer ,2022",
    "rag_problem": "The challenge of effectively using data augmentation in text classification with large pre-trained language models, as many traditional methods become obsolete.",
    "rag_method": "The use of sophisticated methods such as adversarial training, interpolation, and certain generative methods that are capable of introducing new linguistic patterns.\n\n**Explanation:** Large pre-trained language models are invariant to some simple transformations like synonym replacement. Sophisticated methods like adversarial training introduce variability and robustness that these models can leverage to improve performance. These methods can introduce new linguistic patterns which the pre-trained models have not seen, thus enhancing their learning capacity.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method still struggles with generating high-quality augmented data when the original dataset is not large enough.\n- There is a risk of introducing new biases through data augmentation, particularly when using pre-trained language models like GPT.\n- The complexity of some sophisticated data augmentation methods adds an additional layer of difficulty that needs to be understood.\n- Data augmentation can be resource-intensive and time-consuming, which limits its feasibility in time-sensitive machine learning applications.",
    "rag_future_work": "- Develop data augmentation methods capable of generating diverse linguistic patterns not seen during pre-training to improve the model's adaptability and reduce its dependency on large datasets.\n- Investigate techniques to minimize biases introduced by data augmentation processes, particularly those biases inherited from generative language models like GPT.\n- Design efficient data augmentation techniques suitable for time-sensitive applications, specifically focusing on reducing the computational resources and time required for training generative models in crisis informatics and other urgent domains.\n- Explore holistic approaches that integrate transfer learning and data augmentation to assess potential overlaps and develop methods that enhance data augmentation's effectiveness without redundancy.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 31
  },
  {
    "id": "W3195038684",
    "x": 2021.512282552956,
    "y": -1.2968917820439927,
    "title": "Artificial intelligence for proteomics and biomarker discovery",
    "authors": [
      "Matthias Mann",
      "Chanchal Kumar",
      "Wenfeng Zeng"
    ],
    "first_author": "Matthias Mann",
    "first_author_surname": "Mann",
    "year": 2021,
    "cited_by_count": 329,
    "venue": "",
    "size": 28.19036710677692,
    "color": "hsl(21, 70%, 60%)",
    "label": "Mann ,2021",
    "rag_problem": "High dimensionality and complexity in proteomics data make it difficult to identify useful biomarkers.",
    "rag_method": "Application of artificial intelligence algorithms to process and analyze large-scale proteomics datasets.\n\n**Explanation:** AI algorithms such as machine learning can handle high-dimensional and complex datasets by extracting relevant patterns and features, thus facilitating the identification of potential biomarkers. These techniques can efficiently process large volumes of proteomics data, which traditional statistical methods may struggle with, providing a way to discern significant biological information from complex datasets.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4382516982",
    "x": 2023.463240437187,
    "y": -11.47451658444785,
    "title": "Machine Learning Methods for Small Data Challenges in Molecular Science",
    "authors": [
      "Bozheng Dou",
      "Zailiang Zhu",
      "Ekaterina Merkurjev"
    ],
    "first_author": "Bozheng Dou",
    "first_author_surname": "Dou",
    "year": 2023,
    "cited_by_count": 326,
    "venue": "",
    "size": 28.15605136441898,
    "color": "hsl(7, 70%, 60%)",
    "label": "Dou ,2023",
    "rag_problem": "Small data sets in molecular science research pose challenges for machine learning models due to constraints like time, cost, ethics, privacy, and technical limitations.",
    "rag_method": "The authors propose specialized machine learning methods designed to work effectively with small data sets.\n\n**Explanation:** The proposed methods focus on maximizing the efficiency and accuracy of machine learning models in scenarios with limited data. This is achieved by tailoring algorithmic strategies that are less dependent on large volumes of data, potentially through techniques such as data augmentation, transfer learning, or leveraging domain knowledge more effectively. By implementing these approaches, the models can extract more meaningful patterns from small datasets, thus effectively overcoming the constraints posed by traditional methods designed for big data.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method still struggles with achieving high predictive accuracy when the data size is extremely small, which limits its practical applicability in certain molecular science scenarios.\n- The approach is limited by the inherent constraints of small data, such as biases introduced by insufficient sampling and challenges in generalizing the model findings.\n- Despite addressing small data challenges, our method may not fully capture the complex relationships present in molecular data due to the reduced availability of diverse data points.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3133966466",
    "x": 2020.8377289029836,
    "y": 3.724338049804455,
    "title": "Systematic reviews in sentiment analysis: a tertiary study",
    "authors": [
      "Alexander Ligthart",
      "Cagatay Catal",
      "Bedir TekiÃánerdoƒüan"
    ],
    "first_author": "Alexander Ligthart",
    "first_author_surname": "Ligthart",
    "year": 2021,
    "cited_by_count": 277,
    "venue": "",
    "size": 27.5460546178453,
    "color": "hsl(21, 70%, 60%)",
    "label": "Ligthart ,2021",
    "rag_problem": "The existing sentiment analysis research lacks a consolidated view of systematic review studies, making it difficult to understand the limitations and challenges of sentiment analysis comprehensively.",
    "rag_method": "Conducting a tertiary study that synthesizes data from secondary studies (i.e., systematic reviews and systematic mapping studies) in sentiment analysis.\n\n**Explanation:** By synthesizing and analyzing the data from multiple secondary studies, the tertiary study provides a comprehensive overview of key topics, methodologies, features, algorithms, and datasets in sentiment analysis. This consolidated view helps identify open problems and challenges, thus guiding future research directions effectively.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- This tertiary study has limitations common to all secondary studies, which might include bias from selected papers and inability to cover all literature comprehensively.\n- The variation in the number of primary studies included in different secondary papers might lead to inconsistent coverage, as papers with more primary studies may offer more diverse insights compared to those with fewer.\n- The study notes the need for further exploration of domain adaptation techniques and multi-lingual sentiment analysis applications, indicating potential gaps in comprehensively addressing these areas in the current work.",
    "rag_future_work": "- Develop more interpretable and memory-efficient deep learning models to address challenges in sentiment analysis and reduce computational costs.\n- Conduct further research on cross-domain and multi-lingual sentiment analysis models, as current attempts require improvement to tackle language and domain dependencies effectively.\n- Investigate linguistic phenomena such as irony and sarcasm, and integrate them into existing models to enhance the accuracy of sentiment classification.\n- Explore the use of hybrid approaches combining traditional and deep learning techniques to improve performance and application in specialized domains like medical and security screening.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 39
  },
  {
    "id": "W2147152072",
    "x": 1990.1367922779802,
    "y": -0.012156865853871657,
    "title": "Indexing by latent semantic analysis",
    "authors": [
      "Scott Deerwester",
      "Susan Dumais",
      "George W. Furnas"
    ],
    "first_author": "Scott Deerwester",
    "first_author_surname": "Deerwester",
    "year": 1990,
    "cited_by_count": 12614,
    "venue": "",
    "size": 60,
    "color": "hsl(240, 70%, 60%)",
    "label": "Deerwester ,1990",
    "rag_problem": "Traditional indexing methods struggle to detect relevant documents when queries contain terms that are not directly present in the documents.",
    "rag_method": "The use of latent semantic analysis (LSA) and singular-value decomposition (SVD) to uncover the higher-order semantic structure from a large term-document matrix.\n\n**Explanation:** LSA uses SVD to decompose the term-document matrix into a set of orthogonal factors. These factors capture implicit relationships between terms and documents that are not immediately visible. By approximating the original matrix with these factors, LSA can identify relevant documents based on the latent semantic relationships, even when the exact query terms are not present in the documents.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Explore the optimization of singular-value decomposition for larger datasets to improve computational efficiency and scalability in indexing and retrieval processes.\n- Investigate the application of latent semantic analysis on more diverse and multilingual corpora to enhance the method's adaptability and accuracy across different languages and disciplines.\n- Develop algorithms to better address the limitations in capturing polysemy and synonymy within the semantic structure, potentially improving the accuracy of document retrieval systems.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3034850762",
    "x": 2020.5970066276627,
    "y": -13.975567290985104,
    "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
    "authors": [
      "Yixin Nie",
      "Adina Williams",
      "Emily Dinan"
    ],
    "first_author": "Yixin Nie",
    "first_author_surname": "Nie",
    "year": 2020,
    "cited_by_count": 571,
    "venue": "",
    "size": 37.82149054984091,
    "color": "hsl(28, 70%, 60%)",
    "label": "Nie ,2020",
    "rag_problem": "Natural Language Understanding benchmarks become quickly obsolete as models rapidly improve, leading to concerns that models may be overfitting specific dataset biases rather than genuinely understanding language.",
    "rag_method": "The paper proposes an iterative adversarial human-and-model-in-the-loop process for creating a benchmark dataset called Adversarial NLI (ANLI).\n\n**Explanation:** This process involves human annotators intentionally creating examples that current model iterations fail to classify correctly, thus identifying weaknesses and biases in the models. These failures are verified for accuracy and included in further training, while new test sets are created to pose ongoing challenges to updated models. This continual adversarial loop allows the dataset to remain relevant and challenging, supporting perpetual improvement without saturation.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method did not verify the correctness of each training example, potentially leaving room for inaccuracies or biases in the dataset.\n- There is a concern that the dynamic adversarial data collection process is more costly compared to a static approach, due to the requirement of a verification step to ensure example correctness.\n- The benchmark may eventually saturate and become less challenging, although the authors mention that new rounds can be collected if this occurs.",
    "rag_future_work": "- Future work could explore a detailed cost and time trade-off between adversarial and static data collection approaches, determining the efficiency and practicality of each method.\n- Researchers could investigate the opportunities provided by the annotator-provided explanations for misclassified examples, allowing for more nuanced analysis of NLI model performance.\n- The method proposed for classification tasks could be extended and adapted to ranking tasks involving hard negatives, either generated by adversarial models or retrieved and verified by humans.\n- There is potential for improvement by verifying the correctness of each training example in the ANLI dataset, potentially enhancing the dataset's overall reliability and usefulness.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 25
  },
  {
    "id": "W2963143606",
    "x": 2015.9279973749285,
    "y": -3.8058387340794324,
    "title": "Learning to Compose Neural Networks for Question Answering",
    "authors": [
      "Jacob Andreas",
      "Marcus Rohrbach",
      "Trevor Darrell"
    ],
    "first_author": "Jacob Andreas",
    "first_author_surname": "Andreas",
    "year": 2016,
    "cited_by_count": 469,
    "venue": "",
    "size": 36.89898302115149,
    "color": "hsl(56, 70%, 60%)",
    "label": "Andreas ,2016",
    "rag_problem": "The challenge of answering questions using both structured data (like knowledge bases) and unstructured data (like images) where traditional methods struggle to dynamically tailor the computational structure to the specific data source and question complexity.",
    "rag_method": "Introduction of dynamic neural module networks that automatically assemble question-specific neural networks using composable modules.\n\n**Explanation:** By using an inventory of neural modules that can be dynamically composed, the model can tailor the network structure to the complexity of the question and the type of data source (image or structured data). This approach allows the model to leverage continuous representations, improving the expressiveness and learnability for both structured and unstructured data, bypassing the need for fixed computation structures or manual module configurations.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Explore network architectures that further enhance the adaptability and efficiency of semantic structure prediction, potentially by integrating more complex dynamic topologies that tailor computation specifically to each question type.\n- Extend the compositional question-answering approach to handle more diverse and continuous real-world representations, including visual data such as images, to improve the versatility and application range of the model.\n- Investigate the potential of neural predicate representations to define reusable attributes and relations in structured world schemas, aiming to improve the model's semantic understanding and expressiveness in handling varied information sources.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 14
  },
  {
    "id": "W2612228435",
    "x": 2016.514588223039,
    "y": 1.5266013817603694,
    "title": "Search-based Neural Structured Learning for Sequential Question Answering",
    "authors": [
      "Mohit Iyyer",
      "Wen-tau Yih",
      "Ming‚ÄêWei Chang"
    ],
    "first_author": "Mohit Iyyer",
    "first_author_surname": "Iyyer",
    "year": 2017,
    "cited_by_count": 211,
    "venue": "",
    "size": 26.527628289305905,
    "color": "hsl(49, 70%, 60%)",
    "label": "Iyyer ,2017",
    "rag_problem": "Sequential question answering requires handling context and dependencies from previous questions and answers, which increases the complexity of the task compared to answering isolated complex questions.",
    "rag_method": "The dynamic neural semantic parsing framework (DynSP) formulates the semantic parsing problem as a state-action search problem, using a weakly supervised reward-guided learning approach to handle dependencies between sequential questions.\n\n**Explanation:** DynSP leverages the context from previous questions and answers by dynamically constructing neural network structures as the state-action space evolves. This allows the model to incorporate dependencies into the parsing process without having a predefined network structure, thereby maintaining flexibility to adapt to sequential contexts. The reward-guided search enhances learning efficiency by guiding the search process to optimize parsing accuracy based on expected outcomes.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The current formal language design may not cover all question types in SQA, indicating the need to further extend the semantic parser for robustness in handling complex queries.\n- The approach faces serious computational challenges both in model learning and inference when dealing with more complicated semantic parse structures, such as increased primitive statements or parse length.\n- The dynamic framework of the method makes it difficult to leverage GPU capabilities effectively using minibatched training, limiting computational efficiency.\n- Resolving semantic matching errors remains a challenge, highlighting the potential need for unsupervised learning from large external corpora to improve accuracy.",
    "rag_future_work": "- Extend the current formal language design to enhance the robustness of the semantic parser, particularly by including features like UNION operations and comparisons of multiple previous answers.\n- Investigate methods to leverage modern computing machinery, such as GPUs, more effectively, especially addressing the computational challenges posed by complex semantic parse structures in model learning and inference.\n- Improve the resolution of semantic matching errors, potentially through unsupervised learning techniques applied to large external corpora.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 19
  },
  {
    "id": "W2979462702",
    "x": 2019.8940645519735,
    "y": 3.0552547436574686,
    "title": "The Cambridge Handbook of Discourse Studies",
    "authors": [
      "Anna De Fina",
      "Anna De Fina",
      "R. G. Moore"
    ],
    "first_author": "Anna De Fina",
    "first_author_surname": "Fina",
    "year": 2020,
    "cited_by_count": 192,
    "venue": "",
    "size": 26.174809270768346,
    "color": "hsl(28, 70%, 60%)",
    "label": "Fina ,2020",
    "rag_problem": "Corporate internal dialogue is often considered dry and purely transactional, which overlooks the interpersonal interaction features present in corporate dialogue.",
    "rag_method": "Reveal the significance of interpersonal discourse features in corporate dialogue through discourse analysis and sociolinguistic research, including hedges, humor, politeness, and impoliteness.\n\n**Explanation:** These studies show that corporate dialogue involves not only transactional content, but many interactive discourse features indicate that relational goals are equally important as transactional goals in corporate environments. By demonstrating the existence and function of these discourse features, this challenges traditional stereotypes of corporate dialogue and provides a more comprehensive and authentic understanding of corporate communication.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate the dynamics of interpersonal discourse features in different corporate settings to understand their impact on relational goals. This could include further studies on humor, politeness, and hedges in varying cultural and organizational contexts.\n- Explore the misconception of corporate talk as purely transactional by analyzing the balance between transactional and relational discourse in corporate communication. Future work could focus on identifying specific industries or corporate roles where this balance may vary significantly.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2725817383",
    "x": 2012.613376252428,
    "y": -6.060057051386797,
    "title": "Language, Culture, and Society",
    "authors": [
      "Ana Deumert"
    ],
    "first_author": "Ana Deumert",
    "first_author_surname": "Deumert",
    "year": 2013,
    "cited_by_count": 183,
    "venue": "",
    "size": 25.99536971859761,
    "color": "hsl(78, 70%, 60%)",
    "label": "Deumert ,2013",
    "rag_problem": "The lack of a comprehensive understanding of the historical development and theoretical foundation of sociolinguistics and linguistic anthropology.",
    "rag_method": "The chapter provides an overview and analysis of key figures and movements throughout the history of sociolinguistics and linguistic anthropology, from the works of Humboldt, Whitney, and Schuchardt in the nineteenth century, to Boas and Sapir in the early twentieth century, and the consolidation of socio-cultural linguistics in the 1960s.\n\n**Explanation:** By systematically outlining the contributions of significant scholars and historical trends within the realm of sociolinguistics and linguistic anthropology, the paper offers a structured understanding of how these fields have developed over time. This comprehensive narrative and analysis offer insights into the theoretical bases and cultural contexts that shaped these disciplines, thus bridging gaps in historical knowledge for contemporary scholars.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W1977132984",
    "x": 2013.639113128921,
    "y": 5.906296090701223,
    "title": "Significant or random?",
    "authors": [
      "V√°clav B≈ôezina",
      "Miriam Meyerhoff"
    ],
    "first_author": "V√°clav B≈ôezina",
    "first_author_surname": "B≈ôezina",
    "year": 2014,
    "cited_by_count": 173,
    "venue": "",
    "size": 25.78539623221546,
    "color": "hsl(71, 70%, 60%)",
    "label": "B≈ôezina ,2014",
    "rag_problem": "The challenge is to find conditions under which operators with certain types of kernels, specifically Tracy-Widom type operators, can be expressed as squares of Hankel operators. This understanding is crucial for simplifying spectral analysis in random matrix theory.",
    "rag_method": "The solution involves establishing sufficient conditions for expressing an operator with a specific kernel as the square of a Hankel operator by leveraging properties of matrices and theorems related to Hankel operators and spectral multiplicity.\n\n**Explanation:** By identifying the kernel of the operator as a Tracy-Widom type and expressing it in terms of the square of a Hankel operator, the problem of calculating the operator's spectrum is reduced to calculating the spectrum of the simpler Hankel operator. This is achieved by utilizing spectral properties such as eigenvalues and unit eigenvectors, as well as mathematical identities and theorems concerning self-adjoint and trace-class operators.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate the implications of having finite rank R + W R + operators on rational functions. Future work could focus on identifying conditions under which these operators influence the properties of rational functions and exploring potential applications.\n- Explore the generalization of findings related to finite rank operators to broader classes of functions or operators. This could involve extending the current results to non-rational function spaces or non-Hilbert spaces.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 8
  },
  {
    "id": "W4214933144",
    "x": 2012.8931503097058,
    "y": -2.582184483449918,
    "title": "The Oxford Handbook of the History of Linguistics",
    "authors": [
      "Allan, Keith 1943-"
    ],
    "first_author": "Allan, Keith 1943-",
    "first_author_surname": "1943-",
    "year": 2013,
    "cited_by_count": 168,
    "venue": "",
    "size": 25.67583897677106,
    "color": "hsl(78, 70%, 60%)",
    "label": "1943- ,2013",
    "rag_problem": "Understanding the vast and varied historical progression of linguistic theories and methodologies over two-and-a-half millennia.",
    "rag_method": "Compilation and critical examination of interconnected approaches, skills, and tasks by leading scholars in the field, as presented in the Oxford Handbook of the History of Linguistics.\n\n**Explanation:** By gathering insights from a diverse group of leading scholars, the handbook offers a comprehensive view that integrates various historical perspectives and methodologies. This compilation helps to illustrate and clarify how linguistic theories have evolved, the factors influencing these changes, and the interconnected nature of different linguistic approaches. Thus, it provides a structured and detailed understanding that can be referenced by both scholars and students.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate the historical development and transformation of language origins and changes. This entails deeper exploration into linguistic evolution over millennia to better understand past and present language dynamics.\n- Examine the progression and historical contexts of signing and writing systems. This could include studying the interplay between different systems and their influence on communication and documentation practices.\n- Conduct comparative studies of human speech sounds across different cultures and historical periods to identify commonalities and divergences. This could enhance our understanding of phonetic variation and its implications for linguistic theory.\n- Develop comprehensive descriptions and recordings of grammars and lexicons from historically understudied languages to preserve linguistic diversity and provide resources for further research.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2045236476",
    "x": 2015.3854445132692,
    "y": 4.642017077092142,
    "title": "‚ÄúShe does have an accent but‚Ä¶‚Äù: Race and language ideology in students' evaluations of mathematics instructors on RateMyProfessors.com",
    "authors": [
      "Nicholas Close Subtirelu"
    ],
    "first_author": "Nicholas Close Subtirelu",
    "first_author_surname": "Subtirelu",
    "year": 2015,
    "cited_by_count": 156,
    "venue": "",
    "size": 25.399084657820502,
    "color": "hsl(64, 70%, 60%)",
    "label": "Subtirelu ,2015",
    "rag_problem": "Nonnative English speaking instructors at English-medium institutions in the US are often subject to negative student evaluations due to language ideologies that frame them as 'incomprehensible Others', reinforcing social exclusion.",
    "rag_method": "The study investigates these language ideologies by examining student evaluations of nonnative English speaking mathematics instructors on RateMyProfessors.com, exploring the role of race and language in these perceptions.\n\n**Explanation:** By analyzing the student evaluations, the study aims to uncover the underlying language ideologies and racial biases contributing to the social exclusion of nonnative English speaking instructors. Understanding these perceptions can highlight the biases present and inform strategies to address and challenge these ideologies, thus reducing unjust negative evaluations based on accents and race.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate the impacts of language ideologies across different disciplines beyond mathematics to understand if similar biases exist towards NNES instructors in other fields.\n- Conduct longitudinal studies to examine how these language ideologies and associated biases evolve over time and impact the career progression of NNES instructors.\n- Explore interventions or training programs that can mitigate the biases students may hold against NNES instructors and improve classroom communication.\n- Analyze the role of institutional policies in either perpetuating or mitigating language bias and exclusion among NNES instructors at English-medium institutions in the US.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4309674289",
    "x": 2022.284274992812,
    "y": -19.671497403415156,
    "title": "Survey of Hallucination in Natural Language Generation",
    "authors": [
      "Ziwei Ji",
      "Nayeon Lee",
      "Rita Frieske"
    ],
    "first_author": "Ziwei Ji",
    "first_author_surname": "Ji",
    "year": 2022,
    "cited_by_count": 2392,
    "venue": "",
    "size": 44.543574699809,
    "color": "hsl(14, 70%, 60%)",
    "label": "Ji ,2022",
    "rag_problem": "Natural language generation models often generate hallucinated text, which is unfaithful or nonsensical relative to the input source, negatively impacting performance in applications like abstractive summarization, dialogue generation, and machine translation.",
    "rag_method": "Development of metrics and mitigation methods aimed at detecting and reducing hallucinations in NLG outputs.\n\n**Explanation:** Metrics such as information extraction-based, QA-based, and model-based approaches attempt to quantify hallucination by measuring faithfulness to the input source or factual consistency. Mitigation methods include data augmentation, model architecture improvements, attention adjustments, and training strategies designed to reduce the likelihood of hallucination, thereby enhancing the reliability and accuracy of generated text.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our survey highlights that while hallucination is relatively easy to detect in settings like abstractive summarization and NMT, the mitigation methods for hallucinations in certain areas such as dialogue systems and especially in GQA and VL tasks remain very preliminary.\n- Despite summarizing contributors to hallucination, our survey does not provide standardized or definitive solutions for addressing the diverse challenges, which remain open research areas.\n- There is a need for different mitigation strategies tailored to intrinsic versus extrinsic hallucinations, indicating that our approach may not cover all necessary strategies for effective mitigation across varied NLG applications.",
    "rag_future_work": "- Develop methods for accurately identifying and categorizing hallucinatory sub-strings in language metrics, focusing on differentiating between intrinsic and extrinsic hallucinations to enhance explainability and quality.\n- Advance dialogue generation systems by integrating fact-checking for increased factual consistency, including verifiable claim detection and evidence retrieval from external sources, not only for evaluation purposes but also as part of the system's operational model.\n- Simplify and diversify existing methods of searching for and evaluating hallucinatory content in machine translation by exploring less computationally expensive algorithms and applying them to architectures like CNNs and transformers.\n- Innovate in hallucination detection and mitigation in Vision-Language (VL) tasks by conducting empirical and theoretical analyses across various applications, developing more generalizable evaluation metrics, and exploring controlled generation with visual grounding to reduce hallucination effects.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 119
  },
  {
    "id": "W4402665833",
    "x": 2023.9904772093564,
    "y": -61.110395841910986,
    "title": "Large Language Models for Software Engineering: A Systematic Literature Review",
    "authors": [
      "Xinyi Hou",
      "Yanjie Zhao",
      "Yue Liu"
    ],
    "first_author": "Xinyi Hou",
    "first_author_surname": "Hou",
    "year": 2024,
    "cited_by_count": 444,
    "venue": "",
    "size": 36.642256032629064,
    "color": "hsl(0, 70%, 60%)",
    "label": "Hou ,2024",
    "rag_problem": "There is a lack of comprehensive understanding and systematic review on the application of Large Language Models (LLMs) in Software Engineering (SE), specifically concerning their effectiveness, limitations, and trends.",
    "rag_method": "Conduct a Systematic Literature Review (SLR) of 395 research papers to map current uses, classify LLM architectures, analyze dataset handling methods, evaluate performance strategies, and identify SE tasks LLMs have been applied to.\n\n**Explanation:** By systematically reviewing a vast number of papers, the authors provide an overview of how LLMs are being used in SE, categorize different LLMs by architecture, describe data preprocessing techniques, and summarize applications in various SE tasks. This approach highlights current trends, areas of success, and gaps, thereby offering a 'state-of-the-art' landscape and future research directions.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method may suffer from study selection bias due to subjective judgment during the manual verification stage, which can affect the accuracy of the quality assessment of the included papers.\n- There is a potential risk of mislabeling papers during the automated selection process due to incomplete or ambiguous BibTeX records, which may necessitate additional manual verification to ensure accuracy.",
    "rag_future_work": "- Develop advanced automation tools for software development that utilize LLMs to auto-generate code snippets, optimize systems, and provide personalized, context-aware developer assistance, enhancing the development cycle and software quality.\n- Enhance software testing processes by integrating LLMs to improve test case generation, bug classification, and defect prediction, which can aid in the early discovery of errors and reduce development costs.\n- Integrate specialized code representation methods and domain-specific programming knowledge into LLMs to generate code that is not only functionally accurate but also secure and compliant with programming standards.\n- Advance the use of LLMs in formal analysis methodologies, including formal verification, to ensure rigorous code analysis and improve the robustness and reliability of software products.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 62
  },
  {
    "id": "W2347127863",
    "x": 2017.2928269142133,
    "y": -4.704665695155956,
    "title": "Stance and Sentiment in Tweets",
    "authors": [
      "Saif M. Mohammad",
      "Parinaz Sobhani",
      "Svetlana Kiritchenko"
    ],
    "first_author": "Saif M. Mohammad",
    "first_author_surname": "Mohammad",
    "year": 2017,
    "cited_by_count": 420,
    "venue": "",
    "size": 36.381851175486126,
    "color": "hsl(49, 70%, 60%)",
    "label": "Mohammad ,2017",
    "rag_problem": "Difficulty in detecting stance towards a target when sentiment and stance towards different entities interact, and existing datasets lack annotations for both sentiment and stance.",
    "rag_method": "Creation of a dataset annotated for both stance and sentiment, leveraging sentiment features, distant supervision techniques, and word embeddings to develop a stance detection system.\n\n**Explanation:** The dataset provides annotated data that accounts for tweets expressing stance without explicit mention of the target and includes sentiment annotation, allowing researchers to assess the correlation between sentiment and stance. The dataset enables the application of a machine learning framework that uses sentiment lexicons and external tweet data for improved stance detection. The incorporation of word embeddings obtained from relevant tweet corpora further enhances the system's ability to detect stance, especially when sentiment alone is insufficient.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method currently lacks the use of sophisticated features such as those derived from dependency parse trees and entity-entity relationship knowledge, which could improve performance.\n- We have not yet integrated more advanced classifiers, like deep architectures, that could jointly model stance, target of opinion, and sentiment, potentially limiting the method's effectiveness.\n- The current approach requires stance-labeled instances for each target, indicating a limitation in its ability to generalize to new targets without direct label data.\n- Our approach does not yet model how stance is conveyed or how it changes over time, which could affect its dynamic adaptability in real-time applications.",
    "rag_future_work": "- Explore the use of advanced features and classifiers, such as those derived from dependency parse trees and deep architectures, to jointly model stance, target of opinion, and sentiment.\n- Develop stance detection systems that can generalize across targets, using stance-labeled instances from other targets within the same domain.\n- Investigate how stance is expressed and analyze how the distribution of stance towards a target evolves over time.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 25
  },
  {
    "id": "W4389265550",
    "x": 2023.4490020375008,
    "y": -6.224080931290388,
    "title": "A comparison review of transfer learning and self-supervised learning: Definitions, applications, advantages and limitations",
    "authors": [
      "Zehui Zhao",
      "Laith Alzubaidi",
      "Jinglan Zhang"
    ],
    "first_author": "Zehui Zhao",
    "first_author_surname": "Zhao",
    "year": 2023,
    "cited_by_count": 256,
    "venue": "",
    "size": 27.250917927703256,
    "color": "hsl(7, 70%, 60%)",
    "label": "Zhao ,2023",
    "rag_problem": "Scarcity of labeled training data in deep learning leads to insufficient model performance and generalization capability.",
    "rag_method": "Use transfer learning and self-supervised learning to overcome data scarcity problems.\n\n**Explanation:** Transfer learning reduces the need for large amounts of labeled data by leveraging model parameters pre-trained on one task and reapplying them to different but related tasks; while self-supervised learning maximizes the use of available large amounts of unlabeled data by automatically generating pseudo-labels from unlabeled data for training. Both approaches can effectively improve model performance under data scarcity conditions.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles with the scarcity of labelled training data which limits the performance and generalisation of deep learning models despite advances in overcoming this challenge.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2014583745",
    "x": 2013.6593639530195,
    "y": -0.051073077826719326,
    "title": "Online portfolio selection",
    "authors": [
      "Bin Li",
      "Steven C. H. Hoi"
    ],
    "first_author": "Bin Li",
    "first_author_surname": "Li",
    "year": 2014,
    "cited_by_count": 245,
    "venue": "",
    "size": 27.086545699560318,
    "color": "hsl(71, 70%, 60%)",
    "label": "Li ,2014",
    "rag_problem": "The challenge of optimizing portfolio allocation over multiple periods to maximize expected cumulative wealth growth in a volatile financial market.",
    "rag_method": "Implementation of various strategies like 'Follow-the-Winner', 'Follow-the-Loser', and 'Pattern-Matching', which use historical market data and statistical predictions to adjust allocations dynamically.\n\n**Explanation:** 'Follow-the-Winner' algorithms dynamically increase the weights of successful stocks based on historical performance, leveraging Capital Growth Theory to align with optimal strategies. 'Follow-the-Loser' algorithms utilize mean reversion principles, transferring wealth from gaining assets to poorly performing ones, which empirically results in better performance. 'Pattern-Matching' approaches predict future market distributions using historical patterns, optimizing the portfolio for anticipated conditions. These methods allow for portfolio adjustments that aim to maximize cumulative wealth by appropriately responding to market changes.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The paper identifies the need for further exploration as many research problems in online portfolio selection remain unsolved, indicating that the current methodologies, including those surveyed, do not provide comprehensive solutions.\n- Despite outlining various algorithmic approaches, the paper acknowledges the complexity of connecting these to the Capital Growth Theory, suggesting limitations in fully capturing the essence of underlying trading ideas.\n- Although many algorithms are surveyed, the paper admits that quite a few of them still face open challenges, which could imply limitations in their practical application or theoretical foundation.",
    "rag_future_work": "- Investigate novel prediction methods that can enhance accuracy and efficiency in online portfolio selection, addressing the challenges faced in this step.\n- Develop improved portfolio optimization algorithms that can better handle the complexities and uncertainties inherent in financial markets.\n- Explore integration approaches that can seamlessly combine prediction and optimization steps to create more robust and adaptive portfolio selection strategies.\n- Examine real-world applications and continuous adaptation mechanisms to refine algorithm performance in dynamic market environments.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 47
  },
  {
    "id": "W2970183140",
    "x": 2018.5896225541487,
    "y": 14.553055304189076,
    "title": "Learning the Extraction Order of Multiple Relational Facts in a Sentence with Reinforcement Learning",
    "authors": [
      "Xiangrong Zeng",
      "Shizhu He",
      "Daojian Zeng"
    ],
    "first_author": "Xiangrong Zeng",
    "first_author_surname": "Zeng",
    "year": 2019,
    "cited_by_count": 141,
    "venue": "",
    "size": 25.021756425941348,
    "color": "hsl(35, 70%, 60%)",
    "label": "Zeng ,2019",
    "rag_problem": "Existing multiple relation extraction methods do not consider the extraction order of relational facts in a sentence, which influences the accuracy and efficacy of extracting complete sets of relational triplets due to dependency between triplets.",
    "rag_method": "The authors propose a sequence-to-sequence model with reinforcement learning to dynamically learn and optimize the extraction order of relational triplets.\n\n**Explanation:** The sequence-to-sequence model allows for flexible generation of triplets, while reinforcement learning adjusts the extraction order to maximize the reward, which represents the number and accuracy of valid triplets extracted. This method does not predefine extraction order but encourages the model to find an optimal order that improves extraction quality by evaluating rewards based on generated triplet validity.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method trained with reinforcement learning achieves high precision but suffers from relatively low recall, indicating that it generates fewer triplets despite aiming to extract all from a sentence.\n- The approach currently can only copy one word for each entity, typically the last word, which is inadequate for extracting complete multi-word entities.",
    "rag_future_work": "- Explore methods to extract complete entities rather than individual words by integrating BIO tag prediction into the encoder, allowing entities to be recognized with the help of BIO tags.\n- Investigate a two-step approach to generate entities by first obtaining the head word and subsequently the tail word, which could enhance the model's ability to capture multi-word entities more accurately.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 23
  },
  {
    "id": "W2950601686",
    "x": 2018.7060586069774,
    "y": 17.219267555254913,
    "title": "Exploring Sequence-to-Sequence Learning in Aspect Term Extraction",
    "authors": [
      "Dehong Ma",
      "Sujian Li",
      "Fangzhao Wu"
    ],
    "first_author": "Dehong Ma",
    "first_author_surname": "Ma",
    "year": 2019,
    "cited_by_count": 141,
    "venue": "",
    "size": 25.021756425941348,
    "color": "hsl(35, 70%, 60%)",
    "label": "Ma ,2019",
    "rag_problem": "Sequence labeling methods for Aspect Term Extraction (ATE) struggle to fully utilize the overall meaning of the sentence and have limitations in processing label dependencies.",
    "rag_method": "Formalize ATE as a Sequence-to-Sequence (Seq2Seq) learning task using gated unit networks and position-aware attention mechanisms.\n\n**Explanation:** Seq2Seq learning encodes the entire sentence into a fixed-length vector, allowing the decoder to utilize comprehensive sentence information during label prediction. The gated unit networks ensure precise mapping between words and labels by integrating the word's contextual representation into the label generation process. Position-aware attention emphasizes adjacent words, crucial for accurate aspect term identification, by adjusting attention weights based on word proximity. Together, these mechanisms enhance the model's ability to capture dependencies between labels, making full use of sentence context for improved ATE accuracy.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method, despite achieving comparable performance, lacks detailed analysis on how it can be generalized to other sequence labeling tasks, possibly limiting its applicability beyond aspect term extraction.\n- The approach's reliance on the immediate word context and representation may limit its effectiveness in sentences with complex structures where broader context is required.",
    "rag_future_work": "- Apply the proposed sequence-to-sequence learning approach to other sequence labeling tasks, such as named entity recognition and word segmentation, to evaluate its generalizability and performance across different tasks.\n- Investigate improvements to the integration of word representation and adjacent word information, potentially refining the PAA and GUN model, to enhance the accuracy and robustness of the approach in the ATE task and beyond.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 17
  },
  {
    "id": "W3175225269",
    "x": 2021.4135112719875,
    "y": 6.193407106624531,
    "title": "A Unified Generative Framework for Various NER Subtasks",
    "authors": [
      "Hang Yan",
      "Tao Gui",
      "Junqi Dai"
    ],
    "first_author": "Hang Yan",
    "first_author_surname": "Yan",
    "year": 2021,
    "cited_by_count": 241,
    "venue": "",
    "size": 27.024945177793196,
    "color": "hsl(21, 70%, 60%)",
    "label": "Yan ,2021",
    "rag_problem": "Current approaches to Named Entity Recognition (NER) require different methods for each of the three subtypes: flat, nested, and discontinuous NER. This leads to complexity and inefficiency in tagging schemas or span enumeration.",
    "rag_method": "A unified generative framework that formulates NER subtasks as an entity span sequence generation task, leveraging a sequence-to-sequence (Seq2Seq) model with a pointer mechanism.\n\n**Explanation:** By converting NER tasks into a generative sequence generation problem, the unified Seq2Seq framework can handle all three NER subtasks ‚Äî flat, nested, and discontinuous ‚Äî using a standardized approach. This eliminates the need for complex tagging schemas or span enumeration processes required by previous methods because the sequence generation method can encode entity spans directly. The pointer mechanism simplifies entity extraction by directly generating the sequence of entity pointer indexes, making it applicable across various NER forms without specialized adjustment.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Explore the usage of non-autoregressive methods to potentially speed up the decoding process during the evaluation phase, thus addressing the current limitation of slow inference.\n- Investigate alternative training strategies that optimize both training speed and evaluation efficiency without compromising model performance.\n- Integrate advanced techniques or architectures to leverage parallel computation further, minimizing the need for autoregressive token generation in NER subtasks.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 26
  },
  {
    "id": "W2981852735",
    "x": 2018.8748343978934,
    "y": -17.688285456628638,
    "title": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer",
    "authors": [
      "Colin Raffel",
      "Noam Shazeer",
      "Adam P. Roberts"
    ],
    "first_author": "Colin Raffel",
    "first_author_surname": "Raffel",
    "year": 2019,
    "cited_by_count": 3692,
    "venue": "",
    "size": 51.23968727013672,
    "color": "hsl(35, 70%, 60%)",
    "label": "Raffel ,2019",
    "rag_problem": "The diversity and complexity of transfer learning techniques in NLP make it difficult to compare algorithms and understand existing methods.",
    "rag_method": "A unified 'text-to-text' framework that converts all text-based language problems into a text-to-text format using a single model, objective, training procedure, and decoding process.\n\n**Explanation:** By adopting a text-to-text format, the framework allows for consistent application across diverse NLP tasks, easing comparison of different approaches and promoting a systematic study of transfer learning techniques.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method relies heavily on large models to achieve better performance, which can be a limitation in scenarios where smaller or less expensive models are necessary, such as client-side inference or federated learning.\n- The exploration of different unsupervised objectives did not reveal significant performance differences, suggesting that the current method may not yield substantial improvements for all tasks and might require exploring completely different approaches to leveraging unlabeled data.\n- The method may not be optimized for low-resource or computationally constrained environments, limiting its applicability in settings where resources are scarce.",
    "rag_future_work": "- Explore entirely different methods for leveraging unlabeled data, as additional gains from current denoising objectives may be limited.\n- Investigate methods to achieve strong performance with smaller, cheaper models, making transfer learning more applicable to low-resource and cost-sensitive environments.\n- Continue research on techniques such as model distillation, parameter sharing, and conditional computation to enhance the efficiency and impact of transfer learning in various applications.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 61
  },
  {
    "id": "W3093871477",
    "x": 2019.739994841828,
    "y": -7.975216891385773,
    "title": "Beyond English-Centric Multilingual Machine Translation",
    "authors": [
      "Angela Fan",
      "Shruti Bhosale",
      "Holger Schwenk"
    ],
    "first_author": "Angela Fan",
    "first_author_surname": "Fan",
    "year": 2020,
    "cited_by_count": 466,
    "venue": "",
    "size": 36.86890649624139,
    "color": "hsl(28, 70%, 60%)",
    "label": "Fan ,2020",
    "rag_problem": "Existing multilingual translation models are predominantly English-Centric, meaning they primarily translate from and to English, which does not reflect global translation needs and leads to lower performance for translations between non-English languages.",
    "rag_method": "Development of a Many-to-Many multilingual translation model that can directly translate between any pair of 100 languages without pivoting through English.\n\n**Explanation:** By creating a training dataset that covers thousands of language pairs with supervised data through large-scale mining, the model directly accommodates non-English languages. This approach leverages parallel corpora construction and backtranslation to improve data quality for low-resource pairs, and utilizes techniques such as dense scaling and sparse mixture-of-experts to enhance model capacity. This results in significant performance improvements for non-English language directions, with gains of more than 10 BLEU in some non-English directions.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method still requires substantial improvements for very low-resource languages, such as African languages like Xhosa and Zulu, due to limited available monolingual resources that affect the quantity and quality of the mined data.\n- The use of large-scale mined training data poses challenges, as our method sometimes includes both simplified and traditional Chinese text, as well as tokenized and untokenized text, which complicates the data filtering process and can affect translation quality.\n- Multilingual translation in our approach can be affected by domain mismatch, as discussions differ across global regions, which complicates the curation of high-quality training sets necessary for effective translation.",
    "rag_future_work": "- Enhance translation capabilities for very low-resource languages through the integration of curated data, higher quality small datasets, mined data, and monolingual resources to develop improved translation systems.\n- Improve the cleanliness and quality of mined training data by enhancing data filtering methods, tackling challenges such as code switching and domain mismatch, which affect the training of high-quality multilingual translation models.\n- Investigate multi-source self-ensembling techniques as an efficient method for improving translation accuracy in zero-shot translation scenarios, promising reduced computational requirements compared to standard ensembling.\n- Expand the capabilities of multilingual translation systems by exploring densely and sparsely scaling model parameters, introducing language-specific parameters, and utilizing novel random re-routing schemes for handling large datasets efficiently.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 50
  },
  {
    "id": "W3017454464",
    "x": 2019.667705069933,
    "y": -2.477700870362571,
    "title": "Improving Massively Multilingual Neural Machine Translation and Zero-Shot Translation",
    "authors": [
      "Biao Zhang",
      "Philip Williams",
      "Ivan Titov"
    ],
    "first_author": "Biao Zhang",
    "first_author_surname": "Zhang",
    "year": 2020,
    "cited_by_count": 208,
    "venue": "",
    "size": 26.474075634553067,
    "color": "hsl(28, 70%, 60%)",
    "label": "Zhang ,2020",
    "rag_problem": "Multilingual neural machine translation models underperform compared to bilingual models due to insufficient modeling capacity, leading to reduced translation quality across varying languages.",
    "rag_method": "Enhance model capacity by deepening NMT architectures and incorporating language-specific components such as language-aware layer normalization and linear transformations.\n\n**Explanation:** Deep NMT architectures can handle more complex dependencies and induce abstract representations, while language-specific components relax the representation constraint by considering linguistic differences, thus allowing for more expressive and flexible translation representations. This combination improves the generalization capability across multiple languages, thereby enhancing multilingual NMT performance.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method struggles with modeling capacity as adding more languages results in reduced translation quality. This becomes particularly severe in massively multilingual settings.\n- There is still the issue of off-target translation, where models incorrectly translate into a wrong language, especially on zero-shot directions, due to the lack of parallel data.\n- Despite improvements with language-specific components, the multilingual approach still falls behind bilingual models in terms of translation quality, indicating limitations in fully optimizing multilingual settings.",
    "rag_future_work": "- Develop lightweight alternatives to Language-Aware Neural Translation (LALT) to minimize model parameters, aiming to make neural machine translation more efficient.\n- Explore novel strategies to surpass the upper limit of Random Online Backtranslation (ROBT) for greater zero-shot translation improvements, potentially incorporating generative modeling techniques.\n- Further study and utilize the OPUS-100 dataset to enhance understanding and capabilities in massively multilingual settings, providing a benchmark for future research advancements.\n- Investigate methods to mitigate off-target translations and improve the performance of zero-shot directions in multilingual neural machine translation systems.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 19
  },
  {
    "id": "W3176023514",
    "x": 2021.4001925212863,
    "y": 15.04785014067232,
    "title": "Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter",
    "authors": [
      "Wei Liu",
      "Xiyan Fu",
      "Yue Zhang"
    ],
    "first_author": "Wei Liu",
    "first_author_surname": "Liu",
    "year": 2021,
    "cited_by_count": 162,
    "venue": "",
    "size": 25.540009211686506,
    "color": "hsl(21, 70%, 60%)",
    "label": "Liu ,2021",
    "rag_problem": "Existing methods for Chinese sequence labeling do not integrate lexicon information into the bottom layers of BERT, thereby not fully utilizing the representation power of BERT.",
    "rag_method": "Lexicon Enhanced BERT (LEBERT), which integrates external lexicon knowledge directly into BERT layers using a Lexicon Adapter layer.\n\n**Explanation:** LEBERT introduces a Lexicon Adapter layer between Transformer layers of BERT, allowing deep fusion of lexicon knowledge at the bottom-level. This integration facilitates a more thorough interaction between lexicon features and BERT's representations, enhancing representation power and improving sequence labeling accuracy.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method does not fully exploit the representation power of BERT because the external lexicon features are not integrated into the bottom level of the model, potentially limiting performance gains from deeper integration.\n- The approach may struggle to effectively combine discrete lexicon features with the continuous contextual representations from BERT, as the integration occurs at a higher level rather than the initial layers.",
    "rag_future_work": "- Investigate alternative layer settings for Lexicon Adapter integration to address potential overfitting issues observed with multi-layer adaptation.\n- Explore additional strategies for incorporating lexicon features into BERT to enhance interaction and improve sequence labeling performance without compromising model robustness.\n- Develop techniques to better understand the impact of shallow layer adaptation and its contribution to enhanced performance in Chinese sequence labeling tasks.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 19
  },
  {
    "id": "W2962863357",
    "x": 2000.5694269610583,
    "y": 4.053525769648142,
    "title": "",
    "authors": [
      "Antonio Valerio Miceli Barone",
      "Barry Haddow",
      "Ulrich Germann"
    ],
    "first_author": "Antonio Valerio Miceli Barone",
    "first_author_surname": "Barone",
    "year": null,
    "cited_by_count": 92,
    "venue": "",
    "size": 23.431458676182935,
    "color": "#808080",
    "label": "Barone ,None",
    "rag_problem": "Âú®Á•ûÁªèÊú∫Âô®ÁøªËØë‰∏≠ÔºåÂΩì‰ΩøÁî®‰∏Ä‰∏™ËÆ≠ÁªÉÂú®Â§ßÂûãÈ¢ÜÂüüÂ§ñÊï∞ÊçÆÈõÜ‰∏äÁöÑÁé∞ÊúâÊ®°ÂûãÊù•ÈÄÇÂ∫î‰∏Ä‰∏™Â∞èÂûãÈ¢ÜÂüüÂÜÖÊï∞ÊçÆÈõÜÊó∂ÔºåÂÆπÊòìÂá∫Áé∞ËøáÊãüÂêàÈóÆÈ¢ò„ÄÇ",
    "rag_method": "ÂºïÂÖ•tuneoutÔºå‰∏ÄÁßçÊñ∞ÁöÑÊ≠£ÂàôÂåñÊäÄÊúØÔºå‰ª•ÂèäÂÖ∂‰ªñÊ≠£ÂàôÂåñÊâãÊÆµÂ¶ÇdropoutÂíåL2-regularizationÔºåÁî®‰∫éÂáèÂ∞ëËøáÊãüÂêàÂπ∂ÊîπÂñÑËøÅÁßªÂ≠¶‰π†„ÄÇ\n\n**Explanation:** Ê≠£ÂàôÂåñÊäÄÊúØÈÄöËøáÂ¢ûÂä†Ê®°ÂûãËÆ≠ÁªÉËøáÁ®ã‰∏≠ÁöÑÈöèÊú∫ÊÄßÂíåÈôêÂà∂Ê®°ÂûãÂèÇÊï∞Êù•ÂáèÂ∞ëËøáÊãüÂêàÁé∞Ë±°„ÄÇÂÖ∂‰∏≠Ôºåtuneout‰Ωú‰∏∫‰∏ÄÁßçÊñ∞ÁöÑÊ≠£ÂàôÂåñÊâãÊÆµÔºåÈÄöËøáÂä®ÊÄÅË∞ÉÊï¥ËÆ≠ÁªÉËøáÁ®ã‰∏≠Ê®°ÂûãÁöÑÊõ¥Êñ∞Êù•ÊúâÊïàÂáèÂ∞ëÂØπÂ∞èÊï∞ÊçÆÈõÜÁöÑËøáÁ°¨ËÆ∞ÂøÜÔºå‰ªéËÄåÊîπÂñÑÊ®°ÂûãÁöÑÊ≥õÂåñËÉΩÂäõÂπ∂Â¢ûÂº∫ÂÖ∂Âú®È¢ÜÂüüÂÜÖÊï∞ÊçÆÈõÜ‰∏äÁöÑË°®Áé∞„ÄÇ",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Despite the introduction of tuneout, a novel regularization technique, our method still faces challenges related to overfitting when adapting from a large out-of-domain dataset to a small in-domain dataset.\n- Our method relies heavily on the effectiveness of regularization techniques like dropout and L2-regularization, which may not fully address all aspects of transfer learning limitations inherent in small dataset adaptation scenarios.",
    "rag_future_work": "- Investigate additional regularization techniques: Future work could involve exploring other regularization methods beyond dropout and L2-regularization to further mitigate overfitting in domain adaptation scenarios.\n- Evaluate the performance on different language pairs: Testing the proposed techniques on various language pairs would help assess the generalizability and effectiveness of the methods across different translation tasks.\n- Conduct experiments with larger in-domain datasets: Increasing the size of the in-domain datasets in experiments may provide insights into the scalability of the adaptation techniques and their impact on translation quality.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2095705004",
    "x": 2013.7497495268315,
    "y": -13.96062715437701,
    "title": "Dropout: a simple way to prevent neural networks from overfitting",
    "authors": [
      "Nitish Srivastava",
      "Geoffrey E. Hinton",
      "Alex Krizhevsky"
    ],
    "first_author": "Nitish Srivastava",
    "first_author_surname": "Srivastava",
    "year": 2014,
    "cited_by_count": 34161,
    "venue": "",
    "size": 60,
    "color": "hsl(71, 70%, 60%)",
    "label": "Srivastava ,2014",
    "rag_problem": "Deep neural networks with a large number of parameters are prone to overfitting, which decreases their generalization ability on unseen data.",
    "rag_method": "Dropout technique, which involves randomly dropping units and their connections during the training phase of the neural network.\n\n**Explanation:** By randomly dropping units during training, dropout prevents units from relying too heavily on specific inputs, thus reducing the network's dependency on particular neurons or weights. This results in a more robust model that is less prone to overfitting, as it effectively simulates training a large ensemble of networks and averaging their predictions without the computational overhead.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Dropout adds noise to the training process, which may complicate convergence and require more iterations or adjusted learning rate schedules to stabilize learning.\n- The randomness introduced by dropout can lead to variability in model performance between different training runs, which may necessitate multiple training iterations to ensure consistent results.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2919290281",
    "x": 2018.7330460044036,
    "y": -0.9067296407551352,
    "title": "Massively Multilingual Neural Machine Translation",
    "authors": [
      "Roee Aharoni",
      "Melvin Johnson",
      "Orhan Fƒ±rat"
    ],
    "first_author": "Roee Aharoni",
    "first_author_surname": "Aharoni",
    "year": 2019,
    "cited_by_count": 395,
    "venue": "",
    "size": 28.875449048167095,
    "color": "hsl(35, 70%, 60%)",
    "label": "Aharoni ,2019",
    "rag_problem": "Scaling neural machine translation (NMT) models to support an extremely large number of languages remains a challenge, particularly around balancing model capacity with translation accuracy.",
    "rag_method": "Develop massively multilingual many-to-many NMT models that incorporate up to 102 languages with optimized model architectures like the Transformer.\n\n**Explanation:** By expanding NMT models to simultaneously train on a large number of languages (102 languages to-and-from English), the authors enhance the model's ability to learn from multiple sources and languages, facilitating better resource allocation and performance across diverse language pairs. This approach benefits from transfer learning, where knowledge from high-resource languages improves low-resource language translation. Additionally, by leveraging the English-centric setup, the models benefit from widely available parallel English data, improving overall generalization and reducing overfitting in multi-way parallel settings.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The many-to-many models are inferior in performance when translating out-of-English compared to one-to-many models, likely due to English being over-represented, which limits model capacity for other target languages.\n- The approach does not explore extensive hyperparameter tuning, which may be necessary to address the diversity in training batches and improve performance across different multilingual settings.",
    "rag_future_work": "- Investigate semi-supervised learning methods to enhance performance in massively multilingual NMT settings, leveraging unannotated data alongside limited supervised data.\n- Explore strategies to mitigate performance degradation that occurs as the number of languages in the model increases, ensuring more consistent translation quality across all languages.\n- Utilize massively multilingual NMT models for multilingual transfer learning to improve translation performance in new language pairs by transferring knowledge from other trained languages.\n- Enhance and understand zero-shot performance in multilingual NMT, enabling models to accurately translate between language pairs not explicitly seen during training.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 14
  },
  {
    "id": "W2130942839",
    "x": 2013.9072440485365,
    "y": -8.363485555368936,
    "title": "Sequence to Sequence Learning with Neural Networks",
    "authors": [
      "Ilya Sutskever",
      "Oriol Vinyals",
      "Quoc V. Le"
    ],
    "first_author": "Ilya Sutskever",
    "first_author_surname": "Sutskever",
    "year": 2014,
    "cited_by_count": 13295,
    "venue": "",
    "size": 60,
    "color": "hsl(71, 70%, 60%)",
    "label": "Sutskever ,2014",
    "rag_problem": "DNNs cannot naturally handle sequence-to-sequence mapping tasks due to the requirement for fixed input and output dimensionality.",
    "rag_method": "Use a multilayered Long Short-Term Memory (LSTM) network that maps input sequences to fixed-dimensional vectors and then decodes the target sequence from these vectors.\n\n**Explanation:** LSTM networks are capable of handling long-range temporal dependencies due to their architecture, which can maintain information over extended sequences. By mapping input sequences to fixed-dimensional vectors, LSTMs overcome the constraint of fixed dimensionality. The decoding LSTM then produces the target sequence from these vectors, allowing the model to handle variable-length input and output sequences.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method might initially be assumed to struggle with long sentences due to the limited memory of LSTMs, aligning with other reports of poor performance on similar models. However, specific training approaches, like reversing the dataset, were necessary to mitigate this issue.\n- Our approach may still have inherent limitations related to the model architecture, which initially led to concerns about handling long sequences effectively.",
    "rag_future_work": "- Explore further optimizations of the sequence to sequence approach to enhance translation accuracies beyond those achieved with the current relatively unoptimized model.\n- Investigate the ability and limitations of LSTM models on long sentence translation, considering alternative architectures and memory enhancements to further improve performance.\n- Experimentally verify the potential for training a standard RNN on reversed source sentence datasets to simplify learning and enable effective translation without requiring LSTM architectures.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 21
  },
  {
    "id": "W4406302454",
    "x": 2025.212610338503,
    "y": -13.831707180841311,
    "title": "Security and Privacy Challenges of Large Language Models: A Survey",
    "authors": [
      "Badhan Chandra Das",
      "M. Hadi Amini",
      "Yanzhao Wu"
    ],
    "first_author": "Badhan Chandra Das",
    "first_author_surname": "Das",
    "year": 2025,
    "cited_by_count": 122,
    "venue": "",
    "size": 24.482012124499917,
    "color": "hsl(0, 70%, 60%)",
    "label": "Das ,2025",
    "rag_problem": "Large Language Models (LLMs) are vulnerable to security attacks such as prompt injection, leading them to produce harmful or inappropriate content.",
    "rag_method": "Utilization of prevention and detection-based defense mechanisms, including paraphrasing and perplexity-based detection.\n\n**Explanation:** Prevention mechanisms like paraphrasing disrupt the sequence of injected data, reducing the effectiveness of prompt injection attacks. Detection methods like perplexity-based detection identify compromised prompts by analyzing the complexity and coherence of the input, allowing the system to flag suspicious or altered prompts.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method lacks a comprehensive evaluation of defense mechanisms across a diverse range of learning tasks beyond text classification, such as text summarization and prompt-based learning, making it difficult to ensure robust defenses against backdoor attacks in these areas.\n- The use of explainability in our approach could potentially make the models more vulnerable to security and privacy attacks, as revealing the internal components and dynamics might aid attackers in devising more effective strategies.\n- The high cost and limited access to commercialized large language models (LLMs) like GPT-3 pose challenges in conducting thorough vulnerability and security assessments, which impedes the ability to develop effective attack and defense methodologies.\n- Our method primarily focuses on existing attacks on small NLP models, and there is a notable gap in testing these approaches on larger LLMs, limiting the generalizability and applicability of the findings to real-world large-scale models.",
    "rag_future_work": "- Develop real-time privacy monitoring systems to enhance the resilience of privacy-preserving LLMs, by exploring robust detection techniques against various security and privacy attacks.\n- Conduct thorough evaluations of less-explored defense techniques like secure multi-party computation (SMPC) to assess their effectiveness against LLM vulnerabilities and explore their potential applications.\n- Research new strategies for flexible self-reminding systems and expert frameworks that improve safety, trustworthiness, and accountability in LLMs to protect against jailbreaking attacks.\n- Investigate the role of Explainable AI (XAI) in enhancing transparency and interpretability in LLMs while ensuring it doesn't inadvertently increase vulnerabilities, particularly in the context of backdoor and membership inference attacks.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 40
  },
  {
    "id": "W4394630908",
    "x": 2024.1354562550189,
    "y": -36.00571542915869,
    "title": "A contemporary review on chatbots, AI-powered virtual conversational agents, ChatGPT: Applications, open challenges and future research directions",
    "authors": [
      "Avyay Casheekar",
      "A. Lahiri",
      "Kanishk Rath"
    ],
    "first_author": "Avyay Casheekar",
    "first_author_surname": "Casheekar",
    "year": 2024,
    "cited_by_count": 116,
    "venue": "",
    "size": 24.294095585065428,
    "color": "hsl(0, 70%, 60%)",
    "label": "Casheekar ,2024",
    "rag_problem": "ChatbotsÂíåAIËôöÊãü‰ºöËØù‰ª£ÁêÜÂú®ÁêÜËß£Â§çÊùÇÁöÑ‰∫∫Á±ªËØ≠Ë®ÄÂíå‰∏ä‰∏ãÊñáÂèòÂåñÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄß„ÄÇ",
    "rag_method": "ÂºïÂÖ•Âü∫‰∫éÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàÂ¶ÇChatGPTÔºâÁöÑAIÊäÄÊúØÔºåËøô‰∫õÊ®°ÂûãÂÖ∑ÊúâÂú®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉÁöÑËÉΩÂäõÔºåÂèØ‰ª•Êõ¥Â•ΩÂú∞ÊçïÊçâËØ≠Ë®ÄÁöÑÂ§çÊùÇÊÄßÂíå‰∏ä‰∏ãÊñá„ÄÇ\n\n**Explanation:** Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ¶ÇChatGPTÈÄöËøáÂú®Â§ßËßÑÊ®°ÁöÑÊñáÊú¨ËØ≠ÊñôÂ∫ì‰∏äËøõË°åËÆ≠ÁªÉÔºåÂ≠¶‰π†‰∫Ü‰∏∞ÂØåÁöÑËØ≠Ë®ÄÁªìÊûÑÂíåËØ≠Â¢ÉÊ®°Âºè„ÄÇÂõ†Ê≠§ÔºåÂÆÉ‰ª¨ËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞ÁêÜËß£ÂíåÂõûÂ∫îÂ§çÊùÇÁöÑËá™ÁÑ∂ËØ≠Ë®ÄËæìÂÖ•ÔºåÁõ∏ÊØî‰∫é‰º†ÁªüËßÑÂàôÊàñÊ®°ÊùøÈ©±Âä®ÁöÑÁ≥ªÁªüÔºåÂÖ∑ÊúâÊõ¥Âº∫ÁöÑÁÅµÊ¥ªÊÄßÂíåÁ≤æÂáÜÂ∫¶„ÄÇ",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method may struggle with understanding context over extended conversations, affecting the accuracy of responses in prolonged user interactions.\n- Scaling the solution to understand various dialects and languages still presents a challenge and may impact the global applicability of the system.\n- While capable of generating coherent text, the method may occasionally produce repetitive or irrelevant outputs, which affects the quality and user satisfaction.\n- There might be limitations in handling ambiguity or interpreting nuanced linguistic features during interactions, causing potential misunderstandings in complex dialogues.",
    "rag_future_work": "- Explore the development of more context-aware and emotionally intelligent chatbots to enhance user interactions and satisfaction.\n- Investigate methods for improving the privacy and security aspects of chatbots, ensuring user data is adequately protected.\n- Develop robust frameworks for evaluating chatbot performance across different industry applications to standardize their effectiveness.\n- Research on reducing bias in chatbot responses to create more equitable and fair conversational agents.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4409283601",
    "x": 2025.006974047368,
    "y": -10.864116670494306,
    "title": "Towards conversational diagnostic artificial intelligence",
    "authors": [
      "Tao Tu",
      "Mike Schaekermann",
      "Anil Palepu"
    ],
    "first_author": "Tao Tu",
    "first_author_surname": "Tu",
    "year": 2025,
    "cited_by_count": 104,
    "venue": "",
    "size": 23.88747788196023,
    "color": "hsl(0, 70%, 60%)",
    "label": "Tu ,2025",
    "rag_problem": "Approximating the expertise of clinicians in diagnostic dialogue, which is essential for effective history-taking and diagnosis in medicine, is an outstanding challenge in the development of conversational diagnostic AI systems.",
    "rag_method": "The development of AMIE (Articulate Medical Intelligence Explorer), an LLM-based AI system optimized for diagnostic dialogue, which uses a self-play-based simulated environment with automated feedback to improve and scale its learning.\n\n**Explanation:** AMIE utilizes a self-play methodology where the AI system engages in simulated dialogues with AI patient agents and receives automated feedback for refining its approach. This iterative learning process enhances AMIE's diagnostic dialogue capabilities across various conditions and contexts, enabling it to more accurately emulate the complex communication skills and diagnostic reasoning seen in skilled clinicians.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method employed a text-chat interface, which was unfamiliar to primary care physicians for remote consultation, indicating that the study may not accurately represent usual practice in telemedicine.\n- Despite AMIE's high diagnostic accuracy in tests, the research acknowledges that there are important limitations highlighted through various ablations, suggesting gaps in real-world clinical translation.",
    "rag_future_work": "- Future research could further explore the question of clinicians' familiarity with telemedicine, including monitoring for the impact of a learning curve and analyzing performance variations according to the extent of familiarity with telemedicine.\n- Comprehensive evaluation of conversational diagnostic models for equity, fairness, and bias is essential, aiming to mitigate the risk of bias amplification and ensure that these models do not propagate inequities in healthcare.\n- Further work is needed to ensure the robustness of medical LLMs in multilingual settings, particularly focusing on their performance in minority languages to address the lack of systematic benchmarks.\n- Developing participatory evaluation frameworks and employing red-teaming strategies can help identify vulnerabilities and security gaps in large language models, contributing to the iterative refinement and transparency in model reporting practices.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 60
  },
  {
    "id": "W4403203873",
    "x": 2024.1394929262315,
    "y": -27.716520683508456,
    "title": "A survey on LLM-based multi-agent systems: workflow, infrastructure, and challenges",
    "authors": [
      "Xinyi Li",
      "S. Wang",
      "Siqi Zeng"
    ],
    "first_author": "Xinyi Li",
    "first_author_surname": "Li",
    "year": 2024,
    "cited_by_count": 101,
    "venue": "",
    "size": 23.77855583024865,
    "color": "hsl(0, 70%, 60%)",
    "label": "Li ,2024",
    "rag_problem": "Lack of a systematic and comprehensive framework for constructing and analyzing LLM-based multi-agent systems.",
    "rag_method": "The paper proposes a unified framework comprising five key components: profile, perception, self-action, mutual interaction, and evolution.\n\n**Explanation:** By establishing a general structure that encapsulates much of the previous work, the framework allows for clearer synthesis and organization, thus enabling more systematic construction and analysis of LLM-based multi-agent systems. This structure aids in delineating the processes of agent creation, environmental perception, decision-making, agent communication, and evolutionary improvement.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- LLM-based multi-agent systems are still in their early stages, meaning they may lack maturity and stability compared to more established systems.\n- Despite their robust capabilities, LLM-based agents harbor numerous concealed risks that need further exploration and mitigation strategies.",
    "rag_future_work": "- Developing collective intelligence in AI agents to enhance their capabilities, making them more sophisticated and versatile in mimicking human interaction and perception.\n- Expanding MAS applications across various domains, such as healthcare and traffic management, by improving adaptability and flexibility in multi-modal environments.\n- Exploring efficient data processing frameworks and algorithms to address challenges in data fusion, real-time processing, and decision-making, ensuring MAS can adapt to dynamic environments.\n- Ensuring security and privacy protection in MAS development through continuous technological innovation and interdisciplinary collaboration.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 102
  },
  {
    "id": "W4406658975",
    "x": 2024.4741257513465,
    "y": -8.29341623117276,
    "title": "Current applications and challenges in large language models for patient care: a systematic review",
    "authors": [
      "Felix Busch",
      "Lena Hoffmann",
      "Christopher Rueger"
    ],
    "first_author": "Felix Busch",
    "first_author_surname": "Busch",
    "year": 2025,
    "cited_by_count": 93,
    "venue": "",
    "size": 23.47164673491102,
    "color": "hsl(0, 70%, 60%)",
    "label": "Busch ,2025",
    "rag_problem": "Large language models (LLMs) used in patient care suffer from a lack of optimization for medical domains, leading to potential misinformation and non-reproducibility of medical advice.",
    "rag_method": "Fine-tuning LLMs with enriched medical training data to develop specialized models such as Meditron and BioMistral.\n\n**Explanation:** By adapting LLMs to include medical knowledge during their training process, these models are better equipped to provide accurate and domain-specific responses. This fine-tuning addresses the limitations related to their general model design, ensuring alignment with medical standards and reducing the chances of incorrect outputs.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our review excluded research focused solely on clinicians, limiting the scope to patient care applications and potentially overlooking important implications for healthcare professionals.\n- We were unable to conduct a meta-analysis of LLM performance due to the varied study designs and evaluation methods, hindering a comprehensive assessment of the models' effectiveness in clinical settings.\n- The review has a cutoff date of January 2022, potentially missing relevant publications on predecessor LLM models and the latest advancements, which impacts the relevance and applicability of our findings.\n- The rapid development of LLMs poses challenges in keeping the systematic review up to date, necessitating continuous revisions and updates to include newer models and emerging limitations.",
    "rag_future_work": "- Future studies should extend the synthesis approach to include LLM applications explicitly focused on healthcare professionals, as current reviews have primarily concentrated on patient care applications.\n- Developing a meta-analysis of LLM performance in clinical settings is crucial due to the varying study designs and evaluation methods currently used; this will help standardize assessments as the field evolves.\n- Research should investigate patient acceptance of LLMs in healthcare to ensure successful implementation, addressing concerns about reduced interpersonal relationships and potential dehumanization in medicine.\n- Investigating the feasibility and effectiveness of consistent human oversight and validation of LLM-generated content in clinical settings is necessary to reduce risks of incomplete or inappropriate medical advice.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 17
  },
  {
    "id": "W3184144760",
    "x": 2021.159259389909,
    "y": -4.414815393093717,
    "title": "Persistent Anti-Muslim Bias in Large Language Models",
    "authors": [
      "Abubakar Abid",
      "Maheen Farooqi",
      "James Zou"
    ],
    "first_author": "Abubakar Abid",
    "first_author_surname": "Abid",
    "year": 2021,
    "cited_by_count": 380,
    "venue": "",
    "size": 28.730351960548546,
    "color": "hsl(21, 70%, 60%)",
    "label": "Abid ,2021",
    "rag_problem": "Large language models like GPT-3 demonstrate a persistent and severe anti-Muslim bias, which manifests in creative and varied ways, often associating Muslims with violence.",
    "rag_method": "Introducing positive adjectives related to Muslims into the prompts helps reduce the frequency of violent language completions.\n\n**Explanation:** By inserting positive adjectives into the prompts, the language model's focus is shifted away from violence, redirecting it towards more neutral or positive narratives. This intervention exploits the associative nature of language models, where the context influences the generated completions. Although it does not entirely eliminate bias, it notably reduces the occurrence of violent language when generating text related to Muslims.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method of reducing bias by introducing positive associations into the context has been applied manually, and it unintentionally redirects the language model‚Äôs focus to very specific topics, indicating that it may not be a general or comprehensive solution.\n- The current approach to mitigating bias remains unautomated and unoptimized, and it is unclear whether automation and optimization of this process can achieve better results.\n- The method still struggles with overcoming the innate ability of language models like GPT-3 to creatively manifest underlying biases, making these biases harder to detect and mitigate.",
    "rag_future_work": "- Investigate methods for reducing bias in GPT-3 without redirecting completions towards specific directions, as current techniques using positive adjectives tend to focus on particular themes such as financial or materialistic matters.\n- Explore the development of more generalized debiasing techniques that are effective across multiple tasks and biases beyond anti-Muslim sentiment, considering GPT-3's performance in various natural language processing tasks.\n- Conduct a comparative analysis of bias severity and the effectiveness of debiasing strategies across different religious and demographic groups to develop a more comprehensive understanding of bias in language models.\n- Research the underlying mechanisms in language models that lead to the formation and reinforcement of specific biases, such as the persistent association of \"Muslim\" with violence, to inform future model architecture and training improvements.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 9
  },
  {
    "id": "W4287855127",
    "x": 2021.7634086691935,
    "y": 12.336627453329415,
    "title": "Why Knowledge Distillation Amplifies Gender Bias and How to Mitigate from the Perspective of DistilBERT",
    "authors": [
      "Jaimeen Ahn",
      "Hwaran Lee",
      "Jin-Hwa Kim"
    ],
    "first_author": "Jaimeen Ahn",
    "first_author_surname": "Ahn",
    "year": 2022,
    "cited_by_count": 15,
    "venue": "",
    "size": 16.81813430024161,
    "color": "hsl(14, 70%, 60%)",
    "label": "Ahn ,2022",
    "rag_problem": "Knowledge distillation increases gender bias in the smaller distilled models compared to the original larger models.",
    "rag_method": "Implementing a mixup technique during the knowledge distillation process.\n\n**Explanation:** The mixup technique involves blending representations when gender-related words appear, such that the model learns generalized gender-related information rather than focusing on biased data patterns inherent in the teacher model. This blend acts as a regularizer, preventing the distilled model from amplifying biases prevalent in the training data.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The study used sub-samples of the pre-training corpus, which limits the exploration of experimental results with the entire dataset.\n- There is uncertainty regarding the reasons for SEAT score increases when mixup is applied to the output embeddings, contrary to expectations that embeddings between genders should be closer.",
    "rag_future_work": "- Explore the impact of using the entire pre-training corpus on SEAT and GLUE scores to validate findings more comprehensively.\n- Investigate the underlying reasons for the increase in SEAT scores when mixup is applied to output embeddings, despite the expectation of reduced gender bias.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 18
  },
  {
    "id": "W4287887510",
    "x": 2021.5080639372802,
    "y": 14.895093431295045,
    "title": "Challenges in Measuring Bias via Open-Ended Language Generation",
    "authors": [
      "Afra Feyza Aky√ºrek",
      "Muhammed Yusuf Kocyigit",
      "Sejin Paik"
    ],
    "first_author": "Afra Feyza Aky√ºrek",
    "first_author_surname": "Aky√ºrek",
    "year": 2022,
    "cited_by_count": 14,
    "venue": "",
    "size": 16.57562773035012,
    "color": "hsl(14, 70%, 60%)",
    "label": "Aky√ºrek ,2022",
    "rag_problem": "The measurement of social biases in language models through text completion is prone to yielding contradicting results under different experimental settings.",
    "rag_method": "The paper provides recommendations for reporting biases in open-ended language generation to achieve a more comprehensive understanding of biases exhibited by a given language model.\n\n**Explanation:** By analyzing the effects of various prompt sets, metrics, tools, and sampling strategies, the authors recognize the inconsistency in bias measurements. They propose guidelines to standardize how biases are reported, thus ensuring that the measurement is more reliable and reflective of the true biases present, regardless of the experimental setup.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method still faces challenges with ensuring the comprehensive coverage of all types of bias in open-ended language generation, as identifying every potential bias is inherently complex.\n- The approach struggles with generalization across different contexts and domains, where biases may vary significantly, limiting the applicability of the findings to specific cases.\n- Despite promising results, our technique requires substantial computational resources, which might not be feasible for all users looking to implement these bias measurement methods.\n- The current method's dependency on human evaluations introduces subjectivity, which may skew results if evaluators' perceptions of bias differ.",
    "rag_future_work": "- Develop more robust prompt sets that minimize variability in bias measurements across different experimental settings, ensuring more reliable comparisons of biases across language models.\n- Investigate the effects of different metrics and sampling strategies on bias quantification, aiming to establish standardized practices in assessing biases in language generation models.\n- Explore the integration of multiple automated tools to provide a comprehensive understanding of biases, potentially revealing nuanced insights not captured by a single tool or metric.\n- Conduct longitudinal studies to monitor changes in bias expression as models evolve, which could inform ongoing efforts to mitigate bias in new language model architectures.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4385571817",
    "x": 2023.4433593200497,
    "y": 31.989268712833926,
    "title": "Exploiting Biased Models to De-bias Text: A Gender-Fair Rewriting Model",
    "authors": [
      "Chantal Amrhein",
      "Florian Schottmann",
      "Rico Sennrich"
    ],
    "first_author": "Chantal Amrhein",
    "first_author_surname": "Amrhein",
    "year": 2023,
    "cited_by_count": 6,
    "venue": "",
    "size": 13.711850151610111,
    "color": "hsl(7, 70%, 60%)",
    "label": "Amrhein ,2023",
    "rag_problem": "Natural language generation models often amplify gender biases present in training data, especially in morphologically complex languages where rule-based de-biasing methods are ineffective.",
    "rag_method": "Backward Augmentation approach where gender-fair texts are used as training targets, creating artificial biased texts for training data.\n\n**Explanation:** By reversing the augmentation direction, this approach utilizes human-written gender-fair segments from large monolingual corpora as target-side data, thus training a model to convert biased text into a gender-fair version without needing complex rule-based de-biasing. This addresses the weaknesses of rules that could introduce target-side noise and are difficult to implement for morphologically complex languages.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles with producing perfect gender-fair rewrites, as indicated by a 13.18% word error rate in comparison to reference texts, which suggests that the model's outputs still contain noticeable inaccuracies.\n- The current approach does not resolve the practical question of whether users would prefer potentially erroneous gender-fair rewrites over error-free but biased texts, which leaves uncertainty about user acceptance of the model's output in real-world applications.\n- Although the model successfully rewrites biased texts to be more gender-fair, its effectiveness is limited by errors related to non-grammatical forms on the pseudo source side, implying a potential area for improvement in ensuring grammatical correctness.\n- The merging algorithm could be improved to consider grammatical acceptability, indicating that current implementations may not fully address the intricacies of language processing to produce flawless gender-fair rewrites.",
    "rag_future_work": "- Investigate the application of the gender-fair rewriting model to other languages beyond German, exploring its effectiveness across different linguistic contexts and structures.\n- Explore integrating the gender-fair rewriting approach with other NLP tools and systems to enhance the comprehensive handling of biases in broader applications.\n- Research the possibility of automating the discovery of bias directions and augmentation strategies, reducing reliance on handcrafted rules and enhancing model scalability.\n- Conduct further human evaluation campaigns across diverse demographic groups to evaluate perceptions of gender fairness and model effectiveness in real-world scenarios.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 43
  },
  {
    "id": "W4401857375",
    "x": 2024.0557501810247,
    "y": -53.25706425119481,
    "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
    "authors": [
      "Wenqi Fan",
      "Yujuan Ding",
      "Liangbo Ning"
    ],
    "first_author": "Wenqi Fan",
    "first_author_surname": "Fan",
    "year": 2024,
    "cited_by_count": 295,
    "venue": "",
    "size": 27.781796923673223,
    "color": "hsl(0, 70%, 60%)",
    "label": "Fan ,2024",
    "rag_problem": "Large Language Models (LLMs) suffer from hallucinations and lack access to the most recent, domain-specific, or reliable external knowledge, which limits their performance in tasks like question answering, domain-specific content generation, and fact-checking.",
    "rag_method": "Retrieval-Augmented Generation (RAG) or Retrieval-Augmented Large Language Models (RA-LLMs) integrate external retrieval methods with LLMs to bring in up-to-date, domain-specific, and authoritative information into the generation process.\n\n**Explanation:** RAG integrates retrieval techniques with LLMs, which allows them to dynamically query and access external databases or the internet for relevant information, rather than solely relying on the static and possibly outdated internal knowledge of LLMs. This helps in reducing hallucinations and improving accuracy by supplementing the generation process with reliable information fetched based on specific queries during inference.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The current research on retrieval-augmented large language models (RA-LLMs) is still in its early stages, indicating that there may be unexplored or underdeveloped areas that need further investigation to fully realize their potential.\n- Our method depends heavily on the effectiveness of the retrieval mechanism to combat issues like hallucination and outdated internal knowledge, suggesting a limitation if the retrieval process is not optimized.",
    "rag_future_work": "- Exploration of multi-modal RA-LLMs: Future work could focus on expanding retrieval-augmented generation to incorporate various data modalities such as images, videos, and audio, enabling LLMs to leverage richer contextual information for more precise language generation.\n- Development in understanding users' needs: By integrating multiple modalities, RA-LLMs can work towards a comprehensive understanding of user requirements, leading to fine-grained and high-quality outputs across diverse domains such as healthcare and drug discovery.\n- Enhancement of contextual intelligence: Further research could enhance the capability of RA-LLMs to derive more accurate and insightful outputs by fusing different data modalities, thereby improving their application in complex fields that require multi-dimensional data interpretation.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 46
  },
  {
    "id": "W4401834466",
    "x": 2024.035478139214,
    "y": -43.35412858716714,
    "title": "A survey on large language models for recommendation",
    "authors": [
      "Likang Wu",
      "Zhi Zheng",
      "Zhaopeng Qiu"
    ],
    "first_author": "Likang Wu",
    "first_author_surname": "Wu",
    "year": 2024,
    "cited_by_count": 218,
    "venue": "",
    "size": 26.649693935676403,
    "color": "hsl(0, 70%, 60%)",
    "label": "Wu ,2024",
    "rag_problem": "Integrating external knowledge into recommendation systems while addressing data sparsity and personalization issues.",
    "rag_method": "Large Language Models (LLMs) are utilized as powerful feature extractors that capture high-quality representations of textual features and extensive external knowledge.\n\n**Explanation:** LLMs are pretrained on vast amounts of data and possess the ability to generalize to unseen items due to their extensive coverage of factual information, domain expertise, and reasoning capabilities. By leveraging these capabilities, LLMs enhance recommendation system accuracy and relevance, even in the face of data sparsity, by extracting and utilizing rich linguistic representations for user and item profiling.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The survey may lack comprehensive insights into the practical implementation challenges faced when applying large language models to recommendation systems.\n- Although systematic, the survey might not extensively address the specific performance limitations or shortcomings of generative LLMs in recommendation scenarios.",
    "rag_future_work": "- Explore the integration of multi-modal inputs for real-time, personalized recommendations using LLMs to enhance adaptability and precision in diverse domains.\n- Investigate the incorporation of ethical considerations such as fairness, accountability, and transparency into LLM-based recommendation systems.\n- Advance research in the domain adaption techniques of LLMs, including the refinement of fine-tuning, prompting, prompt tuning, and instruction tuning for improved recommendation accuracy.\n- Identify and address challenges highlighted by the survey, which can lead to innovations in generative LLMs specifically tailored for recommendation systems.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 26
  },
  {
    "id": "W4400118952",
    "x": 2024.043135873808,
    "y": -40.36146425921344,
    "title": "When large language models meet personalization: perspectives of challenges and opportunities",
    "authors": [
      "Jing Chen",
      "Zheng Liu",
      "Xu Huang"
    ],
    "first_author": "Jing Chen",
    "first_author_surname": "Chen",
    "year": 2024,
    "cited_by_count": 196,
    "venue": "",
    "size": 26.251889874624364,
    "color": "hsl(0, 70%, 60%)",
    "label": "Chen ,2024",
    "rag_problem": "Traditional personalization systems, such as recommender systems, often act as passive mediums of information filtering and are limited in providing personalized services beyond content recommendation.",
    "rag_method": "Integrating large language models (LLMs) to act as general-purpose interfaces that can proactively explore user intent, compile requests into plans, and leverage external tools to deliver personalized services.\n\n**Explanation:** LLMs can actively engage with users, allowing for more direct communication and understanding of user preferences through natural language. This deeper understanding allows LLMs to create personalized plans and utilize external tools (like search engines and APIs) to execute these plans, broadening personalization from recommendation to assistant-level interactions.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Large Language Models (LLMs) face challenges in memorizing specific knowledge in private and specialized domains without sufficient training, leading to inaccurate responses and difficulty in controlling behavior within a specific domain.\n- In personalized content creation, there are security and privacy risks, including the potential for LLMs to generate misleading information and memorize sensitive user data, necessitating the development of privacy-preserving techniques during the training process.\n- The search space in recommender systems is complex, posing challenges for automated learning approaches in effectively exploring and optimizing diverse and domain-specific components, which remains a limitation of current methods integrating LLMs.\n- Despite their advanced reasoning capabilities, LLMs still struggle with directly making decisions on challenging technical problems purely through prompting, requiring integration with search strategies like genetic algorithms to achieve better performance.",
    "rag_future_work": "- Investigate methods to reduce the bias inherent in large language models when applied to personalized content, ensuring fair and equitable interaction across diverse demographics.\n- Explore techniques to enhance data privacy and security in personalized AI systems, protecting user data while maintaining the system's personalization capabilities.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 32
  },
  {
    "id": "W4404239715",
    "x": 2023.949994864716,
    "y": -38.08976655711546,
    "title": "Large language models for generative information extraction: a survey",
    "authors": [
      "Derong Xu",
      "Wei Chen",
      "Wenjun Peng"
    ],
    "first_author": "Derong Xu",
    "first_author_surname": "Xu",
    "year": 2024,
    "cited_by_count": 134,
    "venue": "",
    "size": 24.831803827030022,
    "color": "hsl(0, 70%, 60%)",
    "label": "Xu ,2024",
    "rag_problem": "Information extraction traditionally requires multiple independent models, each trained separately for specific tasks such as Named Entity Recognition, Relation Extraction, and Event Extraction, leading to high resource costs and inefficiencies.",
    "rag_method": "Utilization of large language models that can perform generative information extraction, modeling all IE tasks through a universal framework.\n\n**Explanation:** Large language models (LLMs) like GPT-4 are capable of understanding and generating text, allowing them to capture inter-task dependencies and handle various IE tasks consistently. By using generative methods, LLMs can process schemas with millions of entities without significant performance degradation and perform zero-shot and few-shot learning, thus reducing the need for multiple, independently trained models.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The paper indicates that ChatGPT still struggles with event extraction (EE) tasks due to the need for complex instructions and a lack of robustness, suggesting a limitation in the ability to handle intricate information extraction tasks effectively.\n- ChatGPT's performance mostly falls short compared to BERT-based models in the standard information extraction (IE) setting, highlighting a limitation in achieving competitive performance in traditional IE benchmarks.\n- The study notes the presence of \"unannotated spans\" as a predominant error type, suggesting potential issues with data annotation quality impacting the effectiveness of large language models in precise information extraction tasks.",
    "rag_future_work": "- Develop more efficient and scalable LLM architectures specifically tailored for generative information extraction to handle large and diverse datasets more effectively.\n- Explore techniques to improve the contextual understanding and reasoning capabilities of LLMs in generative information extraction tasks to enhance accuracy and relevance.\n- Investigate methods for better integration of external knowledge sources with LLMs to provide more comprehensive and informed generative outputs.\n- Research ways to increase the transparency and explainability of LLM-based generative information extraction systems to boost user trust and facilitate easier debugging and refinement.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 40
  },
  {
    "id": "W3042856524",
    "x": 2020.2055714434837,
    "y": -0.488399054026614,
    "title": "A Graph Neural Network Framework for Social Recommendations",
    "authors": [
      "Wenqi Fan",
      "Yao Ma",
      "Qing Li"
    ],
    "first_author": "Wenqi Fan",
    "first_author_surname": "Fan",
    "year": 2020,
    "cited_by_count": 193,
    "venue": "",
    "size": 26.194228165009108,
    "color": "hsl(28, 70%, 60%)",
    "label": "Fan ,2020",
    "rag_problem": "Social recommendation systems struggle to integrate complex social relationships and user-item interactions effectively for accurate predictions.",
    "rag_method": "Utilizing Graph Neural Networks (GNNs) to represent and learn from graph-based data such as user-user and user-item graphs.\n\n**Explanation:** Graph Neural Networks are adept at capturing and learning from graph structures, including node features and their interconnections. By applying GNNs, the framework can effectively model the complex relationships inherent in social networks, improving the quality and accuracy of recommendations by incorporating both social influences and item affinities in a unified graph-based model.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4367046619",
    "x": 2022.653719682327,
    "y": 16.62113492919899,
    "title": "Fairly Adaptive Negative Sampling for Recommendations",
    "authors": [
      "Xiao Chen",
      "Wenqi Fan",
      "Jingfan Chen"
    ],
    "first_author": "Xiao Chen",
    "first_author_surname": "Chen",
    "year": 2023,
    "cited_by_count": 34,
    "venue": "",
    "size": 19.759389833620283,
    "color": "hsl(7, 70%, 60%)",
    "label": "Chen ,2023",
    "rag_problem": "Uniform Negative Sampling often leads to data bias by oversampling the majority item groups as negative instances, resulting in item-side group unfairness and disparate recommendation performance.",
    "rag_method": "The Fairly Adaptive Negative Sampling (FairNeg) method dynamically adjusts the group-level negative sampling distribution based on perceived performance disparity, aiming to balance the recall performance across item groups.\n\n**Explanation:** FairNeg first identifies the group-wise performance disparity using a proxy metric and then adjusts the negative sampling probabilities so that disadvantaged groups, which suffer from low recall performance, have their negative sampling probabilities decreased. This prevents them from being overwhelmed by negative gradients during training. By adopting an adaptive momentum update strategy, FairNeg smooths the optimization process, further stabilizing the fairness enhancement across groups.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate the compatibility of the FairNeg framework with a broader range of group fairness metrics in recommendation systems. This involves adapting the framework to address diverse fairness concerns and ensuring it can be applied to different fairness evaluation criteria.\n- Explore enhancements to the adaptive momentum mechanism used in FairNeg to further optimize the balance between fairness and representation learning. This could involve refining the algorithm to dynamically adjust better to various data distributions and model requirements.\n- Conduct studies to understand the long-term impacts of incorporating fairness-aware mechanisms on user satisfaction and engagement in recommendation systems. This would involve assessing the practical benefits and any potential trade-offs from a user perspective over extended periods.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 31
  },
  {
    "id": "W4392394652",
    "x": 2023.766222473822,
    "y": -33.28221951490011,
    "title": "Generative Artificial Intelligence in Education: From Deceptive to Disruptive.",
    "authors": [
      "Marc Alier",
      "Francisco Jos√© Garc√≠a‚ÄêPe√±alvo",
      "Jorge D. Camba"
    ],
    "first_author": "Marc Alier",
    "first_author_surname": "Alier",
    "year": 2024,
    "cited_by_count": 108,
    "venue": "",
    "size": 24.027963316079024,
    "color": "hsl(0, 70%, 60%)",
    "label": "Alier ,2024",
    "rag_problem": "Educators face challenges integrating Generative Artificial Intelligence into educational settings due to ethical considerations and lack of best practices.",
    "rag_method": "Establishing a framework for managing and integrating GenAI that includes ethical guidelines and best practice recommendations.\n\n**Explanation:** By creating a structured framework that addresses ethical concerns, educators can confidently utilize GenAI technologies in their curriculum while ensuring responsible usage. Best practice guidelines help educators understand how GenAI can enhance learning experiences without compromising ethical standards.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method may struggle with effectively integrating GenAI tools in diverse educational settings due to varying infrastructure and resource availability, which could limit widespread adoption and scalability.\n- Ethical considerations related to the use of GenAI in education are not yet fully addressed by our approach, leading to potential challenges in ensuring fair and unbiased usage.\n- The adaptability of our method across different educational contexts and curricula requires further exploration, as uniform application may not cater to specific educational needs and practices.\n- Our approach may face difficulties in providing comprehensive best practices for managing GenAI, given the rapidly evolving nature of technology and its applications in education.",
    "rag_future_work": "- Explore the ethical implications of using GenAI in education more thoroughly, focusing on safeguards and guidelines to prevent misuse and ensure fairness.\n- Develop robust best practices for the integration of GenAI in classrooms, addressing the diverse needs of students and teachers.\n- Investigate the long-term impact of GenAI on learning outcomes and educational equity to provide data-driven recommendations for educators and policymakers.\n- Examine the challenges in managing GenAI technologies in educational settings to develop strategies that maximize benefits while minimizing potential risks.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4392938070",
    "x": 2024.3966067965582,
    "y": -25.028629735880216,
    "title": "ChatGPT in medicine: prospects and challenges: a review article",
    "authors": [
      "Songtao Tan",
      "Xin Xin",
      "Di Wu"
    ],
    "first_author": "Songtao Tan",
    "first_author_surname": "Tan",
    "year": 2024,
    "cited_by_count": 82,
    "venue": "",
    "size": 23.004004241277045,
    "color": "hsl(0, 70%, 60%)",
    "label": "Tan ,2024",
    "rag_problem": "Âú®ÂåªÂ≠¶È¢ÜÂüüÂ∫îÁî®ChatGPTÂàùÊúüÔºåËôΩÁÑ∂ÂÖ∂ÊΩúÂäõÂ∑®Â§ßÔºå‰ΩÜ‰πü‰º¥Èöè‰∫Ü‰∏ÄÁ≥ªÂàóÁöÑ‰ΩøÁî®Âíå‰º¶ÁêÜÂõ∞Â¢É„ÄÇ",
    "rag_method": "ÈÄöËøáÊ∑±ÂÖ•ÁöÑÈ¢ÜÂüüÁ†îÁ©∂ÂíåÂÆûÈ™åÔºåÊòéÁ°ÆChatGPTÂú®ÂåªÂ≠¶Âú∫ÊôØ‰∏≠ÁöÑÂ∫îÁî®ËæπÁïåÂèäÊúÄ‰Ω≥ÂÆûË∑µ„ÄÇ\n\n**Explanation:** ÈÄöËøáÂ§ßÈáèÁöÑÁ†îÁ©∂ÂíåÂÆûÈ™åÔºåÁïåÂÆö‰∫ÜChatGPTÂèØ‰ª•ÂÆâÂÖ®‰∏îÊúâÊïàÂ∫îÁî®ÁöÑÂåªÂ≠¶È¢ÜÂüüÂíåÂú∫ÊôØÔºåÂáèÂ∞ë‰∫ÜÂÖ∂ÊΩúÂú®ËØØËØØÂØºÂíå‰∏çÂΩìÁöÑ‰ø°ÊÅØÁîüÊàêÈóÆÈ¢ò„ÄÇÂÖ∑‰ΩìÂ∫îÁî®ÂÆû‰æãÂíåÁªèÈ™åÂ∏ÆÂä©ÂÆö‰πâ‰∫ÜÂÖ∂ÂÆûÈôÖ‰ΩøÁî®Êó∂ÁöÑ‰º¶ÁêÜÂíåÊäÄÊúØËæπÁïåÔºå‰ª•Á°Æ‰øùÂú®ÂåªÂ≠¶È¢ÜÂüü‰∏≠‰ΩøÁî®Êó∂ÁöÑÂÆâÂÖ®ÊÄßÂíåÊúâÊïàÊÄß„ÄÇ",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- ChatGPT, while having incredible potential, still arouses concerns about its applicability and reliability within medical contexts.\n- Despite the satisfactory results achieved in the application of ChatGPT in medicine, the challenges it presents remain persistent, necessitating further exploration and refinement.",
    "rag_future_work": "- Explore the integration of ChatGPT with electronic health records to improve patient data management and communication between healthcare providers.\n- Investigate the use of ChatGPT for patient education and engagement, focusing on personalized medicine and tailoring advice to individual patient needs.\n- Develop frameworks to address ethical concerns and biases in AI-driven medical advice to ensure equitable and accurate patient care.\n- Conduct longitudinal studies to assess the long-term impact and effectiveness of using ChatGPT in various medical applications, including diagnostics and patient monitoring.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4392976364",
    "x": 2024.5115226939593,
    "y": -22.693199951358956,
    "title": "Large Language Models and User Trust: Consequence of Self-Referential Learning Loop and the Deskilling of Health Care Professionals",
    "authors": [
      "Avishek Choudhury",
      "Zaira S. Chaudhry"
    ],
    "first_author": "Avishek Choudhury",
    "first_author_surname": "Choudhury",
    "year": 2024,
    "cited_by_count": 81,
    "venue": "",
    "size": 22.95845765122037,
    "color": "hsl(0, 70%, 60%)",
    "label": "Choudhury ,2024",
    "rag_problem": "The integration of large language models (LLMs) in healthcare can lead to a self-referential learning loop, where the models primarily learn from AI-generated content rather than human-generated data, potentially reducing the diversity and accuracy of information.",
    "rag_method": "Develop protocols and guidelines ensuring that LLMs are trained on balanced datasets containing both AI-generated and human-sourced data.\n\n**Explanation:** By ensuring that training datasets include diverse sources, models can avoid the pitfall of a self-referential loop. This helps maintain the accuracy and diversity of information from which LLMs learn, thereby enhancing their reliability and trustworthiness in clinical settings.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate methods to enhance clinician trust in large language models, ensuring that AI-generated content does not inadvertently undermine professional confidence.\n- Examine the implications of dependence on AI-generated content for healthcare decisions, focusing on maintaining the balance between human oversight and automated aid.\n- Explore initiatives to retrain or support healthcare professionals in adapting to AI-integrated environments, addressing potential deskilling in clinical settings.\n- Analyze the long-term effects of self-referential learning loops in LLMs, particularly their impact on clinical decision-making and overall healthcare delivery quality.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4399365040",
    "x": 2023.87481826051,
    "y": -15.34594136902161,
    "title": "(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice",
    "authors": [
      "Inyoung Cheong",
      "King Xia",
      "K. J. Kevin Feng"
    ],
    "first_author": "Inyoung Cheong",
    "first_author_surname": "Cheong",
    "year": 2024,
    "cited_by_count": 56,
    "venue": "",
    "size": 21.591957881526618,
    "color": "hsl(0, 70%, 60%)",
    "label": "Cheong ,2024",
    "rag_problem": "LLMs providing legal advice risk unauthorized practice of law, lack confidentiality and professional accountability, and could lead to real-world consequences due to inaccurate guidance.",
    "rag_method": "Implementation of a case-based deliberation method to engage legal experts, which generates contextual considerations and dimensions for responsible LLM guidance in legal advice.\n\n**Explanation:** By using case-based deliberation with legal experts, the researchers gather specific considerations that highlight when an LLM response is appropriate and when it might breach legal and ethical boundaries. This method identifies key dimensions, such as user characteristics, query nature, AI capabilities, and social impacts, which collectively inform responsible LLM policy and strategies to avoid unauthorized practice, ensure non-disclosure, and provide safe and effective legal information.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method predominantly focused on legal experts familiar with the US legal system, which may not account for ethical considerations relevant to other legal systems and cultures.\n- The participants' evaluations are based on their current experience with existing LLM technologies like GPT-4, which may not reflect future technological advancements in LLM capabilities.\n- We did not involve clients of legal services in the study, which limits the understanding of how clients perceive the appropriateness of LLM legal advice compared to expert opinions.\n- Our taxonomy of dimensions for evaluating LLM responses is not fully linked to how these dimensions affect the appropriateness of those responses, necessitating further empirical research.",
    "rag_future_work": "- Investigate ethical considerations of AI-assisted legal services across different legal systems and cultures to understand how perceptions of LLM appropriateness vary globally.\n- Explore how technological advancements influence experts' evaluations of LLM legal advice, keeping pace with evolving AI capabilities.\n- Conduct empirical analysis involving clients of legal services to compare their perceptions with expert-informed results, particularly regarding the appropriateness of LLM responses.\n- Develop larger-scale empirical studies to assess public evaluations of LLM response appropriateness across diverse legal cases and response scenarios, refining the taxonomy of dimensions affecting these evaluations.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 35
  },
  {
    "id": "W4399788390",
    "x": 2024.2419502952039,
    "y": -12.324904318464489,
    "title": "ChatGPT: perspectives from human‚Äìcomputer interaction and psychology",
    "authors": [
      "Jiaxi Liu"
    ],
    "first_author": "Jiaxi Liu",
    "first_author_surname": "Liu",
    "year": 2024,
    "cited_by_count": 55,
    "venue": "",
    "size": 21.52545087679132,
    "color": "hsl(0, 70%, 60%)",
    "label": "Liu ,2024",
    "rag_problem": "Previous research focuses predominantly on the technical principles and social impact of ChatGPT, but neglects its effects on human-computer interaction and user psychology.",
    "rag_method": "Comprehensive analysis of ChatGPT's impacts in the fields of human-computer interaction (HCI), psychology, and society.\n\n**Explanation:** By exploring ChatGPT's impact from an HCI and psychological perspective, the paper addresses the gap in the literature and offers insights into how ChatGPT influences user experience and interaction design, as well as its psychological effects on users.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- ChatGPT has the potential to amplify harmful ideologies and societal divisions by disseminating content that normalizes prejudice or intolerance, highlighting the need for careful oversight and ethical guidance in AI development.\n- The anthropomorphic nature of ChatGPT-generated outputs may undermine individuals' self-esteem and professional identity, especially in workplace settings where it can affect feelings of significance, competence, and uniqueness.\n- ChatGPT has only achieved medium or passing performance in medical tests, making it unreliable for clinical deployment as it was not designed for such applications.\n- While enhancing user experience, ChatGPT poses risks such as fostering dependence and reducing interpersonal connection, raising concerns about privacy and the weakening of social identity.",
    "rag_future_work": "- Explore the ethical implications of ChatGPT to ensure its responsible deployment in social relationships and communication systems by conducting comprehensive ethical reviews.\n- Investigate ways to enhance ChatGPT's user experience in human-computer interaction, particularly focusing on improving conversational capabilities and addressing privacy concerns in customer service and education sectors.\n- Evaluate the psychological effects of using ChatGPT as a support tool, assessing risks such as dependency and reduced interpersonal connection, to create guidelines that mitigate these issues.\n- Enhance the underlying infrastructure, such as its Transformer model and RLHF processes, to improve context relevance and human resonance in responses within conversational interfaces.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 77
  },
  {
    "id": "W4323655724",
    "x": 2023.159554854789,
    "y": -37.05484947592127,
    "title": "ChatGPT for good? On opportunities and challenges of large language models for education",
    "authors": [
      "Enkelejda Kasneci",
      "Kathrin Se√üler",
      "Stefan K√ºchemann"
    ],
    "first_author": "Enkelejda Kasneci",
    "first_author_surname": "Kasneci",
    "year": 2023,
    "cited_by_count": 3647,
    "venue": "",
    "size": 51.17634408134742,
    "color": "hsl(7, 70%, 60%)",
    "label": "Kasneci ,2023",
    "rag_problem": "Educators face the challenge of using traditional teaching methods that may not effectively engage students in the digital age.",
    "rag_method": "Introduce ChatGPT as an interactive tool to enhance the learning experience.\n\n**Explanation:** ChatGPT, as a large language model, provides an interactive platform where students can engage in conversations that emulate real-world dialogue, offering instant feedback and diverse perspectives. This makes learning more engaging and caters to the digital habits of modern students, potentially increasing interest and improving retention rates.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method might still struggle with effectively personalizing educational content across different cultural and individual contexts, which remains a challenge for large language models.\n- There is a limitation in the method's ability to ensure the accuracy and reliability of the information provided, posing potential risks in educational settings.\n- Our approach faces challenges in maintaining ethical standards and addressing biases that are inherent in data-driven models, requiring further refinement and regulation.\n- The method requires ongoing updates and improvements to adapt to the evolving educational demands and technological advancements, indicating a need for continuous development.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4362655923",
    "x": 2023.2511737307277,
    "y": 6.383942393556121,
    "title": "Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models",
    "authors": [
      "Yiheng Liu",
      "Tianle Han",
      "Siyuan Ma"
    ],
    "first_author": "Yiheng Liu",
    "first_author_surname": "Liu",
    "year": 2023,
    "cited_by_count": 123,
    "venue": "",
    "size": 24.512437777963793,
    "color": "hsl(7, 70%, 60%)",
    "label": "Liu ,2023",
    "rag_problem": "ChatGPT models, despite their advanced capabilities, face biases in political, ideological, and other areas due to the influence of pre-training data, which can lead to biased or harmful content.",
    "rag_method": "Reinforcement Learning from Human Feedback (RLHF) is integrated into ChatGPT development to align language models with human preferences and values.\n\n**Explanation:** RLHF enhances ChatGPT's adaptability by ensuring the outputs are aligned with human values, reducing biases inherent from pre-training data. By learning from human feedback, the model adjusts its parameters to generate more appropriate and less biased content, tackling the issue of bias significantly.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- ChatGPT may generate responses that are overly verbose or exhibit a lack of precision, making it less effective for tasks requiring concise and accurate answers.\n- The model can sometimes produce inaccurate or misleading information, as it does not possess real-time awareness or verification capabilities.\n- There is a risk of the model generating biased or inappropriate content due to the limitations in understanding nuanced context or maintaining neutrality.\n- ChatGPT currently struggles with understanding and maintaining long-term context, which can affect coherence in extended conversations or narratives.",
    "rag_future_work": "- Developing localized training and deployment procedures for large language models (LLMs) in specific domains, such as medicine, to address issues of domain-specific accuracy and data privacy.\n- Exploring the expansion of ChatGPT's influence beyond NLP, specifically into computer vision, brain-inspired AI, and robotics, to harness their potential in facilitating complex human-robot interactions.\n- Utilizing the zero-shot learning capabilities of LLMs to enable rapid adaptation in fields with limited labeled data, such as medical informatics and robotics, thereby reducing the dependence on extensive data labeling for new task training.\n- Further customization of LLM training processes based on domain-specific data to enhance model accuracy in sensitive fields like medical applications where low error tolerance is crucial.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 34
  },
  {
    "id": "W2941187122",
    "x": 2019.3872576418812,
    "y": 6.403216566889299,
    "title": "Drug repurposing in oncology: Compounds, pathways, phenotypes and computational approaches for colorectal cancer",
    "authors": [
      "Patrycja Nowak‚ÄêSliwinska",
      "L√©onardo Scapozza",
      "Ariel Ruiz i Altaba"
    ],
    "first_author": "Patrycja Nowak‚ÄêSliwinska",
    "first_author_surname": "Nowak‚ÄêSliwinska",
    "year": 2019,
    "cited_by_count": 181,
    "venue": "",
    "size": 25.954303215056044,
    "color": "hsl(35, 70%, 60%)",
    "label": "Nowak‚ÄêSliwinska ,2019",
    "rag_problem": "The process of drug repurposing in oncology is time consuming and inefficient due to the vast number of existing drugs and diseases combined with the heterogeneity of cancers like colorectal cancer.",
    "rag_method": "Utilize computational approaches to systematize and streamline the process of identifying existing drugs that can be repurposed for treating colorectal cancer, focusing on compounds, pathways, and phenotypes.\n\n**Explanation:** By employing computational methods, researchers can efficiently analyze large datasets of drugs and disease characteristics to predict which existing drugs might be effective against colorectal cancer. These methods can incorporate molecular and phenotypic data to identify promising repurposing candidates, thus reducing the time and resource burden associated with traditional drug discovery and development processes.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method faces challenges due to the vast number of existing drugs and diseases, which can make the repurposing process time-consuming and inefficient.\n- We struggle with the heterogeneity of patients and diseases, particularly in oncology, which complicates the straightforward application of drug repurposing strategies.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2984946521",
    "x": 2019.3242740324538,
    "y": 12.274326965311115,
    "title": "The plasminogen activator inhibitor-1 paradox in cancer: a mechanistic understanding",
    "authors": [
      "Marta H. Kubala",
      "Yves A. DeClerck"
    ],
    "first_author": "Marta H. Kubala",
    "first_author_surname": "Kubala",
    "year": 2019,
    "cited_by_count": 167,
    "venue": "",
    "size": 25.65353892513127,
    "color": "hsl(35, 70%, 60%)",
    "label": "Kubala ,2019",
    "rag_problem": "Â∞ΩÁÆ°PAI-1Âú®ÁôåÁóá‰∏≠ÈÄöÂ∏∏‰∏éÊÅ∂ÊÄßËÇøÁò§ÁöÑËøõÂ±ïÂíå‰∏çËâØÈ¢ÑÂêéÁõ∏ÂÖ≥Ôºå‰ΩÜÂÆÉÁöÑÂÖ∑‰Ωì‰ΩúÁî®Êú∫Âà∂Âíå‰∏∫‰Ωï‰ºöÂ≠òÂú®ÊÇñËÆ∫ÊÄßÂΩ±Âìç‰ªç‰∏çÊòéÁ°Æ„ÄÇ",
    "rag_method": "ËÆ∫ÊñáÂèØËÉΩÊèêÂá∫‰∫Ü‰∏ÄÁßçÊñ∞ÁöÑÊú∫Âà∂ÊàñÊ®°ÂûãÊù•Ëß£ÈáäPAI-1Â¶Ç‰Ωï‰ª•ÂèåÈáçÊñπÂºèÂΩ±ÂìçËÇøÁò§ËøõÂ±ïÔºåÊØîÂ¶ÇÈÄöËøáÂàÜÂ≠ê‰ø°Âè∑ÈÄöË∑ØÁöÑËØ¶ÁªÜËß£ÊûêÊù•Êè≠Á§∫PAI-1ÁöÑ‰∏çÂêåËßíËâ≤„ÄÇ\n\n**Explanation:** ÈÄöËøáÊè≠Á§∫PAI-1Âú®‰∏çÂêåËÇøÁò§ÁéØÂ¢ÉÊàñÈò∂ÊÆµÂèØËÉΩÊâßË°å‰∏çÂêåÁöÑÁîüÁâ©Â≠¶ÂäüËÉΩÔºå‰ª•ÂèäÂÆÉÂ¶Ç‰ΩïÈÄöËøáÁâπÂÆö‰ø°Âè∑ÈÄöË∑ØÊàñ‰∏éÂÖ∂‰ªñÂàÜÂ≠êÁõ∏‰∫í‰ΩúÁî®ÊñΩÂä†‰∏çÂêåÁöÑÁîüÁêÜÊïàÊûúÔºåÂèØ‰ª•Ëß£ÂºÄËøôÁßçÊÇñËÆ∫„ÄÇËøô‰∫õÊú∫Âà∂Â∏ÆÂä©Ëß£Èáä‰∫ÜPAI-1Âú®Êüê‰∫õÁôåÁóáÁéØÂ¢É‰∏ã‰øÉËøõËÇøÁò§ËøõÂ±ïÔºåËÄåÂú®ÂÖ∂‰ªñÁéØÂ¢É‰∏ãÂç¥ÊäëÂà∂ÂÖ∂ÂèëÂ±ïÁöÑÂéüÂõ†„ÄÇ",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4389042124",
    "x": 2022.5706398346067,
    "y": 4.065664986137107,
    "title": "Unlocking hidden potential: advancements, approaches, and obstacles in repurposing drugs for cancer therapy",
    "authors": [
      "Freya R. Weth",
      "Georgia B. Hoggarth",
      "Anya F. Weth"
    ],
    "first_author": "Freya R. Weth",
    "first_author_surname": "Weth",
    "year": 2023,
    "cited_by_count": 137,
    "venue": "",
    "size": 24.914390616816753,
    "color": "hsl(7, 70%, 60%)",
    "label": "Weth ,2023",
    "rag_problem": "High costs and low approval rates of novel cancer drugs make it challenging to develop effective and affordable cancer treatments.",
    "rag_method": "Drug repurposing as a strategy to use known drugs for new cancer indications, reducing development time and costs.\n\n**Explanation:** By repurposing existing drugs, the initial safety and pharmacokinetic data is already available, allowing these drugs to bypass the lengthy initial phases of drug development. This reduces time and development costs compared to novel drugs, which can take over a decade and billions of dollars to bring to market. Less time and money are spent on developing and testing, enabling more effective and affordable cancer therapies.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The pharmaceutical research and testing process is not well-suited for the testing of combination therapies, limiting the effectiveness and efficiency of drug repurposing strategies for cancer treatment.\n- There is currently a lack of infrastructure to support the broad testing of drug combinations, which can hinder the advancement of repurposing drugs as a widely applicable treatment option for cancer.",
    "rag_future_work": "- Develop combination therapies that target the most frequently mutated proteins and pathways in cancer in order to enhance treatment outcomes and make therapies more accessible and effective.\n- Explore drug repurposing as a preventative strategy for cancer in at-risk but healthy populations, addressing a critical public health concern.\n- Innovate the pharmaceutical research and testing process to better assist the development and evaluation of combination therapies, which could be more appropriate for many cancer patients.\n- Create cost-effective alternatives to current expensive cancer drugs to ensure more sustainable healthcare solutions and improve the cost-effectiveness ratio in treatment outcomes.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 18
  },
  {
    "id": "W2896674942",
    "x": 2018.2662601958525,
    "y": 3.3802340429951987,
    "title": "The Dominant Role of Forkhead Box Proteins in Cancer",
    "authors": [
      "Duc‚ÄêHiep Bach",
      "Nguyen Phuoc Long",
      "Thi-Thu-Trang Luu"
    ],
    "first_author": "Duc‚ÄêHiep Bach",
    "first_author_surname": "Bach",
    "year": 2018,
    "cited_by_count": 96,
    "venue": "",
    "size": 23.589694589948706,
    "color": "hsl(42, 70%, 60%)",
    "label": "Bach ,2018",
    "rag_problem": "The difficulty in understanding the multifaceted roles of Forkhead box (FOX) proteins in cancer, which complicates the development of targeted cancer therapies.",
    "rag_method": "The paper provides an overview and summarization of key FOX protein subfamilies and their implications in various biological and pathophysiological processes within the context of cancer.\n\n**Explanation:** By systematically categorizing and summarizing the roles of different FOX protein subfamilies (such as FOXA, FOXC, FOXM1, FOXO, and FOXP), the paper offers insights into their specific functions and context-dependent behaviors in cancer. This increases the understanding of how these transcription factors contribute to cancer progression, which can guide future research and development of targeted therapies.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2174775663",
    "x": 2014.5309323838894,
    "y": 7.252914323505221,
    "title": "Automatic semantic classification of scientific literature according to the hallmarks of cancer",
    "authors": [
      "Simon Baker",
      "Ilona Silins",
      "Yufan Guo"
    ],
    "first_author": "Simon Baker",
    "first_author_surname": "Baker",
    "year": 2015,
    "cited_by_count": 108,
    "venue": "",
    "size": 24.027963316079024,
    "color": "hsl(64, 70%, 60%)",
    "label": "Baker ,2015",
    "rag_problem": "Researchers face difficulties in efficiently categorizing scientific literature according to the specific hallmarks of cancer due to the complexity and enormous volume of data.",
    "rag_method": "The paper introduces an automatic semantic classification system that organizes scientific literature based on the hallmarks of cancer.\n\n**Explanation:** By utilizing semantic classification technologies, the system reduces the complexity involved in identifying relevant literature related to the hallmarks of cancer. This technology automatically processes and classifies vast amounts of scientific data into the established principles of cancer biology, enabling researchers to swiftly locate pertinent information without manually sift through volumes of papers.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Develop a more comprehensive semantic classification system that integrates newly identified hallmarks of cancer, beyond the existing 10 principles. This would help enhance understanding of evolving biological capabilities in human tumors.\n- Improve technology for semantic classification by incorporating advanced machine learning models to better handle the complexity and volume of scientific literature in cancer research.\n- Explore cross-domain applications of the semantic classification system, such as its use in related fields like genomics, to uncover broader biological insights and foster interdisciplinary research.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2262329766",
    "x": 2016.1431032106082,
    "y": 4.265017605302918,
    "title": "Aspirin and colorectal cancer: the promise of precision chemoprevention",
    "authors": [
      "David A. Drew",
      "Yin Cao",
      "Andrew T. Chan"
    ],
    "first_author": "David A. Drew",
    "first_author_surname": "Drew",
    "year": 2016,
    "cited_by_count": 444,
    "venue": "",
    "size": 36.642256032629064,
    "color": "hsl(56, 70%, 60%)",
    "label": "Drew ,2016",
    "rag_problem": "Standard chemoprevention approaches with aspirin for colorectal cancer are not tailored to individual risk and may have varied efficacy or unnecessary side effects.",
    "rag_method": "Precision chemoprevention with aspirin aims to tailor doses and regimen based on individual genetic and molecular risk factors.\n\n**Explanation:** By integrating genetic and molecular profiling into the decision-making process of aspirin use, precision chemoprevention allows for a more targeted approach. This method focuses on identifying individuals who are at higher risk and may benefit more significantly from aspirin, while reducing exposure and potential side effects for those at lower risk. The approach promises enhanced efficacy and safety by ensuring aspirin is only used where most appropriate, optimizing the balance of risk and benefit.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2034269086",
    "x": 2000.0277538110836,
    "y": -3.728546952895976,
    "title": "The Hallmarks of Cancer",
    "authors": [
      "Douglas Hanahan",
      "Robert A. Weinberg"
    ],
    "first_author": "Douglas Hanahan",
    "first_author_surname": "Hanahan",
    "year": 2000,
    "cited_by_count": 28162,
    "venue": "",
    "size": 60,
    "color": "hsl(169, 70%, 60%)",
    "label": "Hanahan ,2000",
    "rag_problem": "The existing hallmarks of cancer do not account for the dysregulation of differentiation, which is a universal feature in all types of cancer and contributes significantly to cancer progression.",
    "rag_method": "Propose the inclusion of dysregulated differentiation as a new hallmark of cancer, highlighting its universal presence in cancer types, mechanistic distinctiveness, and clinical utility for prognosis and therapy.\n\n**Explanation:** Dysregulated differentiation disrupts the division of labor among cells in multicellular organisms, leading to a fitness advantage for cancer cells by promoting proliferation over differentiation. This disruption is mechanistically distinct from other hallmarks, such as evading growth suppressors, because it represents a breakdown in cellular cooperation rather than simply a loss of growth inhibition. Including dysregulated differentiation as a hallmark provides a more comprehensive framework for understanding cancer biology and leverages the clinical utility for prognosis and therapeutics, such as differentiation therapy that targets this mechanism effectively in certain cancers like acute promyelocytic leukemia.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method struggles to distinguish well-differentiated tumors from benign changes such as hyperplasia using light microscopy, making it difficult to identify dysregulated differentiation without molecular tools.\n- Identifying the lack of terminal differentiation in well-differentiated cancers remains a challenge until new molecular tools become widely available for clinical application, limiting the immediate practical use of the method.\n- Despite acknowledging the universality of differentiation dysregulation in cancers, the ability to distinguish fully recovered proliferative abilities in fully differentiated cells, such as beta cells, is still not adequately addressed.",
    "rag_future_work": "- Investigate the molecular mechanisms underlying dysregulated differentiation in various types of cancer to better understand its role as a potential hallmark.\n- Explore therapeutic approaches targeting dysregulated differentiation in cancers beyond acute promyelocytic leukemia and neuroblastoma.\n- Analyze the genomic signatures associated with dysregulated differentiation to improve prognosis and tailor personalized treatment strategies.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 16
  },
  {
    "id": "W2117692326",
    "x": 2010.5353678668182,
    "y": -3.7904000703397207,
    "title": "Hallmarks of Cancer: The Next Generation",
    "authors": [
      "Douglas Hanahan",
      "Robert A. Weinberg"
    ],
    "first_author": "Douglas Hanahan",
    "first_author_surname": "Hanahan",
    "year": 2011,
    "cited_by_count": 64282,
    "venue": "",
    "size": 60,
    "color": "hsl(92, 70%, 60%)",
    "label": "Hanahan ,2011",
    "rag_problem": "Áî±‰∫éÂØπÁôåÁóáÁîüÁâ©Â≠¶ÁâπÂæÅËÆ§ËØÜÊúâÈôêÔºåÂú®ÁôåÁóáÁöÑËØäÊñ≠ÂíåÊ≤ªÁñó‰∏≠Èù¢‰∏¥Ê†áÂáÜÂåñ‰∏éÁ≤æÂáÜÊÄßÁöÑÈóÆÈ¢ò„ÄÇ",
    "rag_method": "Êõ¥Êñ∞ÂíåÊâ©Â±ïÁôåÁóáÁöÑÁîüÁâ©Â≠¶ÁâπÂæÅÔºàhallmarksÔºâÔºå‰∏∫ÁôåÁóáÁ†îÁ©∂Êèê‰æõ‰∏Ä‰∏™Áªü‰∏ÄÁöÑÊ°ÜÊû∂„ÄÇ\n\n**Explanation:** ÈÄöËøáËØÜÂà´ÂíåÂÆö‰πâÁôåÁóáÁöÑÊñ∞ÁîüÁâ©Â≠¶ÁâπÂæÅÔºåÁ†îÁ©∂‰∫∫ÂëòËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÁôåÁóáÁöÑÂ§çÊùÇÊÄßÔºå‰ªéËÄåÂà∂ÂÆöÊõ¥ÂÖ∑ÈíàÂØπÊÄßÁöÑËØäÊñ≠ÂíåÊ≤ªÁñóÁ≠ñÁï•ÔºåËøôÁßçÊ°ÜÊû∂Â∏ÆÂä©Ê†áÂáÜÂåñÁ†îÁ©∂Âπ∂ÊèêÈ´ò‰∏¥Â∫äÂ∫îÁî®ÁöÑÂáÜÁ°ÆÊÄß„ÄÇ",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Develop more comprehensive models to capture the complexity of cancer by integrating new biological insights and technologies.\n- Investigate the role of the tumor microenvironment in cancer development and progression to identify novel therapeutic targets.\n- Explore the implications of cancer genomics and epigenomics for personalized treatment approaches.\n- Evaluate the potential impact of emerging technologies, such as artificial intelligence, in early cancer detection and treatment optimization.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2594411204",
    "x": 2017.4122402397402,
    "y": 7.07620308750066,
    "title": "Identification of stable housekeeping genes in response to ionizing radiation in cancer research",
    "authors": [
      "Gopal Iyer",
      "Albert R. Wang",
      "S. Brennan"
    ],
    "first_author": "Gopal Iyer",
    "first_author_surname": "Iyer",
    "year": 2017,
    "cited_by_count": 36,
    "venue": "",
    "size": 19.968196198492016,
    "color": "hsl(49, 70%, 60%)",
    "label": "Iyer ,2017",
    "rag_problem": "Ionizing radiation (IR) affects gene expression stability, making it difficult to use traditional housekeeping genes (HKGs) for normalization in cancer research.",
    "rag_method": "The use of geNorm and Normfinder algorithms to systematically identify a set of stable housekeeping genes across different IR doses and time points in various cancer cell lines.\n\n**Explanation:** By applying geNorm and Normfinder algorithms, the study identifies HKGs that show consistent and stable expression levels despite changes induced by IR. This allows accurate normalization of gene expression data by reducing variability that could lead to inaccurate results. GeNorm aids by iteratively excluding unstable genes to find those with the most stable expression, while Normfinder estimates intra- and inter-group variations to select stable genes, ensuring reliable normalization under varying IR conditions.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method's reliance on raw Cq values for normalization is limited due to variability in RNA extraction and reverse transcription yields across different ionizing radiation treatment groups.\n- There is inconsistency in the stability of housekeeping gene expression across different cancer types and subtypes, which reduces the reliability of Cq values for normalization within the same cancer subtype.\n- Traditional housekeeping genes such as GAPDH, G6PD, and ACTB showed higher variability in response to ionizing radiation treatments, impacting their suitability for consistent normalization.",
    "rag_future_work": "- Investigate the influence of different doses of ionizing radiation on the stability of housekeeping genes across a wider range of cancer cell lines and tissue types to improve the generalizability of findings in gene expression normalization.\n- Evaluate the potential of using multiple housekeeping genes with different cellular functions for more accurate normalization in various cancer types and treatment conditions, especially those involving heterogeneous populations like patient-derived cancer cells.\n- Explore the application of advanced statistical algorithms beyond geNorm and Normfinder to better accommodate variability in raw Cq values and enhance the reliability of normalization in RNA expression studies.\n- Conduct further studies to validate the selected housekeeping genes across additional treatment conditions, including combinations of radiation with molecular targeting agents, to assess the impact on normalization effectiveness and overall experimental outcomes.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 19
  },
  {
    "id": "W2003516452",
    "x": 2012.4518145839352,
    "y": -6.074288682146886,
    "title": "The BioGRID interaction database: 2013 update",
    "authors": [
      "Andrew Chatr‚Äêaryamontri",
      "Bobby‚ÄêJoe Breitkreutz",
      "Sven Heinicke"
    ],
    "first_author": "Andrew Chatr‚Äêaryamontri",
    "first_author_surname": "Chatr‚Äêaryamontri",
    "year": 2012,
    "cited_by_count": 1028,
    "venue": "",
    "size": 40.57954812896285,
    "color": "hsl(85, 70%, 60%)",
    "label": "Chatr‚Äêaryamontri ,2012",
    "rag_problem": "Researchers require a comprehensive, manually curated database of genetic and protein interactions across multiple model organisms to facilitate biomedical research.",
    "rag_method": "BioGRID offers an open access archive that provides over 500,000 manually annotated interactions from more than 30 model organisms.\n\n**Explanation:** BioGRID addresses the need for a comprehensive database by curating interactions from primary biomedical literature, ensuring data accuracy and relevance. By offering a centralized resource, it simplifies access to interaction data, enabling researchers to carry out cross-species comparison and analysis, which is essential for various biomedical research projects.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2230749025",
    "x": 2016.3681744648966,
    "y": 0.381295470038148,
    "title": "Weakly supervised learning of biomedical information extraction from curated data",
    "authors": [
      "Suvir Jain",
      "Rahul Kashyap",
      "Tsung-Ting Kuo"
    ],
    "first_author": "Suvir Jain",
    "first_author_surname": "Jain",
    "year": 2016,
    "cited_by_count": 452,
    "venue": "",
    "size": 36.725945277235574,
    "color": "hsl(56, 70%, 60%)",
    "label": "Jain ,2016",
    "rag_problem": "Curated biomedical data often lacks explicit mentions and their locations in texts, which makes them unsuitable for supervised machine learning.",
    "rag_method": "The authors propose using weakly supervised learning based on cost-sensitive learning from noisy labels by implementing a committee of weak classifiers that estimate the reliability of curated data matches.\n\n**Explanation:** The approach leverages curated data as indirect training examples by employing a committee of weak classifiers to estimate the reliability of matched text passages to curated entries. By using the EM algorithm to estimate label reliability, the system assigns costs to training examples, which are then used for cost-sensitive learning. This enables the extraction of relevant information without needing explicit annotation, addressing the lack of directly annotated data in curated sources.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our approach struggles with entity normalization, as some entities have more challenging representations to consolidate, making it difficult to achieve consistent normalization across various forms of the same entity.\n- The method does not effectively handle data from pharmacogenomic studies, where medications are not specific and target diseases may offer a stronger signal for information extraction.\n- The Conditional Random Field (CRF) model used in the passage extractor underperforms compared to dictionary matching, detecting fewer mentions of new diseases and traits not present in the training dictionary.",
    "rag_future_work": "- Investigate the possibility of defining standard passage extractors and weak learners that can be used to extract biomedical entities, attributes, and relations. This would facilitate rapid development and enhance the portability of NLP systems across different domains for biomedical literature mining.\n- Explore the scalability of the current approach to larger and more diverse datasets in order to evaluate its robustness and adaptability to various biomedical information extraction tasks.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 41
  },
  {
    "id": "W2090247362",
    "x": 2012.3301677888467,
    "y": 3.5608446589870457,
    "title": "Computational tools for prioritizing candidate genes: boosting disease gene discovery",
    "authors": [
      "Yves Moreau",
      "L√©on-Charles Tranchevent"
    ],
    "first_author": "Yves Moreau",
    "first_author_surname": "Moreau",
    "year": 2012,
    "cited_by_count": 417,
    "venue": "",
    "size": 36.34826151201683,
    "color": "hsl(85, 70%, 60%)",
    "label": "Moreau ,2012",
    "rag_problem": "Identifying and prioritizing candidate genes associated with diseases is a complex and time-consuming process.",
    "rag_method": "The development and application of computational tools designed to efficiently prioritize candidate genes for disease association.\n\n**Explanation:** Computational tools utilize algorithms and data analysis techniques to systematically evaluate large datasets of genetic information. By using predefined criteria and machine learning models, these tools can rank genes based on their likelihood of being associated with specific diseases. This automated prioritization significantly reduces the time and effort compared to manual analysis and surfaces the most promising gene candidates, thereby boosting the discovery of disease genes.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2032069669",
    "x": 2011.4594273289154,
    "y": 5.882308375888913,
    "title": "Biomedical text mining and its applications in cancer research",
    "authors": [
      "Fei Zhu",
      "Preecha Patumcharoenpol",
      "Cheng Zhang"
    ],
    "first_author": "Fei Zhu",
    "first_author_surname": "Zhu",
    "year": 2012,
    "cited_by_count": 245,
    "venue": "",
    "size": 27.086545699560318,
    "color": "hsl(85, 70%, 60%)",
    "label": "Zhu ,2012",
    "rag_problem": "The vast amount of biomedical literature on cancer research makes it challenging for researchers to manually process and extract relevant information efficiently.",
    "rag_method": "The implementation of biomedical text mining techniques to automatically process large volumes of literature, identifying and extracting pertinent information such as genes, mutations, and treatments related to cancer.\n\n**Explanation:** Biomedical text mining uses algorithms and natural language processing to automate the extraction of key information from extensive datasets. This reduces the manual workload on researchers, allowing them to focus on analyzing results and making informed decisions based on consolidated data. By systematically processing and organizing information, researchers can quickly access and utilize essential data specific to their research needs.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2040610660",
    "x": 2008.4991799593547,
    "y": 2.9669662068415095,
    "title": "Text mining and manual curation of chemical-gene-disease networks for the Comparative Toxicogenomics Database (CTD)",
    "authors": [
      "Thomas C. Wiegers",
      "Allan Peter Davis",
      "Kevin Bretonnel Cohen"
    ],
    "first_author": "Thomas C. Wiegers",
    "first_author_surname": "Wiegers",
    "year": 2009,
    "cited_by_count": 141,
    "venue": "",
    "size": 25.021756425941348,
    "color": "hsl(106, 70%, 60%)",
    "label": "Wiegers ,2009",
    "rag_problem": "The manual curation process for the Comparative Toxicogenomics Database (CTD) cannot keep up with the growing volume and complexity of published biological data, making it inefficient and limited in scope.",
    "rag_method": "Integration of a prototype text-mining solution using existing tools into the CTD workflow to assist in prioritizing and identifying relevant journal articles for manual curation.\n\n**Explanation:** The text-mining prototype automates the identification of relevant chemical, gene, and disease terms in journal articles, which helps re-rank articles for curation by their relevance and data richness. This reduces the time curators spend on irrelevant articles, enhancing productivity and ensuring more complete data coverage in CTD.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The text-mining solution developed does not fully address the specific needs of the CTD curation workflow, indicating a gap between available tools and the CTD's unique requirements.\n- A significant portion (40%) of the journal articles identified for potential curation are not curatable, suggesting inefficiencies in the initial filtration process despite the minimal time spent by biocurators on their rejection.\n- Precision in curated actors is high, but recall is lower, which indicates that while most identified entities are correct, some significant information might be missing, affecting the completeness of the data curated.\n- The increasing scope and volume of published data present a challenge for the method, as manual curation, combined with scalable but error-prone text mining, may struggle to keep up with the pace of data generation.",
    "rag_future_work": "- Explore optimization strategies for weighting criteria using multivariate analysis to improve the accuracy of identified journal articles and actor terms within the curation workflow.\n- Investigate enhancements to retrieval requirements by modifying parameters such as word length and removing confusable English words to refine search efficacy.\n- Extend text-mining capabilities to evaluate full texts of journal articles, which may improve identification of relevant sections for curation.\n- Assess the integration of additional recognition software, particularly focusing on disease tools within the EBI's Whatizit application, to address issues with response times experienced in the current system.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 24
  },
  {
    "id": "W2100751507",
    "x": 2010.4241983599745,
    "y": 3.6496506624374887,
    "title": "An Overview of BioCreative II.5",
    "authors": [
      "Florian Leitner",
      "Scott Mardis",
      "Martin Krallinger"
    ],
    "first_author": "Florian Leitner",
    "first_author_surname": "Leitner",
    "year": 2010,
    "cited_by_count": 106,
    "venue": "",
    "size": 23.958377111243056,
    "color": "hsl(99, 70%, 60%)",
    "label": "Leitner ,2010",
    "rag_problem": "The challenge of accurately capturing information about protein-protein interactions from scientific literature for curation purposes.",
    "rag_method": "The BioCreative II.5 challenge utilised Structured Digital Abstracts and automatic annotations from 15 text mining teams to evaluate and rank articles based on curatable protein-protein interactions, using a gold standard for reconciliation.\n\n**Explanation:** By employing Structured Digital Abstracts and automated annotation systems evaluated against a reconciled gold standard, the challenge provided a more efficient and accurate method for identifying and ranking articles that describe protein-protein interactions. This improves the quality and speed of data curation by systematically harnessing both human and machine inputs to ensure comprehensive coverage and accuracy.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method relies heavily on annotations from multiple sources (curators, authors, and automated systems), which may introduce inconsistencies in the gold standard used for evaluation.\n- The approach assessed by BioCreative II.5 struggles to comprehensively rank articles based purely on curatable protein-protein interactions, which might limit its effectiveness in diverse biological contexts.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2101727078",
    "x": 2008.0118626548376,
    "y": 3.7425547146493914,
    "title": "Evaluation of text-mining systems for biology: overview of the Second BioCreative community challenge",
    "authors": [
      "Martin Krallinger",
      "Alexander A. Morgan",
      "Larry Smith"
    ],
    "first_author": "Martin Krallinger",
    "first_author_surname": "Krallinger",
    "year": 2008,
    "cited_by_count": 209,
    "venue": "",
    "size": 26.492011457020638,
    "color": "hsl(113, 70%, 60%)",
    "label": "Krallinger ,2008",
    "rag_problem": "The difficulty of efficiently extracting biologically relevant information from the vast and growing amount of published literature in the field of genome sciences.",
    "rag_method": "The BioCreative challenge offers a framework for evaluating and improving text-mining and information extraction systems through defined tasks, shared datasets, and community involvement.\n\n**Explanation:** By hosting a collaborative initiative where systems are evaluated on common 'gold standard' datasets, BioCreative allows for performance estimation and comparison, thereby helping identify the most effective strategies and tools. This promotes advancements in text-mining tools tailored for biology, enabling efficient extraction of meaningful data from scientific articles.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- One limitation is that it remains unclear whether the performance improvements observed in the BioCreative II GM task would extend to full-text articles, rather than the abstract-based evaluations conducted.\n- The different tasks in the BioCreative challenge were not aligned in terms of data collections, which limits the ability to evaluate gene mention, gene normalization, and biological annotation extraction consistently across the same set of articles.\n- The GN task design was limited by only performing gene normalization on abstracts restricted to human genes, creating an artificial scenario that does not reflect the complexity of normalizing genes from multiple species.\n- There is uncertainty about how the methods developed for BioCreative II would scale when applied to the entire PubMed database, which includes articles from various scientific disciplines beyond those specifically targeted in the challenge.",
    "rag_future_work": "- Develop a common dataset for evaluating text-mining tasks such as gene mention, gene normalization, and biological annotation extraction to facilitate data alignment and improve comparison across different tasks.\n- Enhance gene normalization tools for full-text articles by addressing database incompleteness, potentially using intermediate bioinformatics sequence similarity steps to improve manual annotation efforts.\n- Expand and refine gene mention recognition approaches by focusing on new gene names and considering the inconsistent use of special characters in biomedical texts, aiming to achieve robust tokenization and improved performance in full-text articles.\n- Explore collaborative efforts across biology, bioinformatics, and natural language processing domains to identify alternative strategies for biologically important text-mining tasks, leveraging the BioCreative platform as a common ground for interdisciplinary exchange.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 21
  },
  {
    "id": "W2126276057",
    "x": 2005.3309592982625,
    "y": -0.182248777476137,
    "title": "Overview of BioCreAtIvE: critical assessment of information extraction for biology",
    "authors": [
      "Lynette Hirschman",
      "Alexander Yeh",
      "Christian Blaschke"
    ],
    "first_author": "Lynette Hirschman",
    "first_author_surname": "Hirschman",
    "year": 2005,
    "cited_by_count": 540,
    "venue": "",
    "size": 37.559778704658726,
    "color": "hsl(134, 70%, 60%)",
    "label": "Hirschman ,2005",
    "rag_problem": "The challenge of accurately extracting gene or protein names from biomedical text and mapping them to standardized gene identifiers.",
    "rag_method": "Task 1 of the BioCreAtIvE challenge which includes gene mention finding and normalization into unique gene identifiers.\n\n**Explanation:** By defining and organizing task 1a and task 1b, BioCreAtIvE established a structured framework for evaluating different systems designed to tackle the challenges of recognizing gene/protein names in biomedical text and normalizing them to standardized identifiers. This provided clear performance benchmarks and advanced the development of systems that can integrate into the curation workflow of biological databases.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Develop automated tools to assist in associating biological building blocks, such as sequence data, genes, and proteins, with published literature through ontologies or controlled vocabularies, speeding up currently manual processes.\n- Enhance the infrastructure and support the growth of a multidisciplinary community, facilitating collaboration and innovation to address the challenges posed by the rapid accumulation of biological data.\n- Improve text mining methodologies to cope with the increasing rate of raw data accumulation and the need for efficient data processing and organization in biological databases.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 21
  },
  {
    "id": "W2168905447",
    "x": 2005.0083509089407,
    "y": -3.5651038403083253,
    "title": "A survey of current work in biomedical text mining",
    "authors": [
      "Aaron Cohen"
    ],
    "first_author": "Aaron Cohen",
    "first_author_surname": "Cohen",
    "year": 2005,
    "cited_by_count": 766,
    "venue": "",
    "size": 39.19932602568019,
    "color": "hsl(134, 70%, 60%)",
    "label": "Cohen ,2005",
    "rag_problem": "The rapidly expanding volume of published biomedical research creates information overload, making it difficult for researchers to efficiently access and utilize critical information.",
    "rag_method": "Application of text mining technologies, including named entity recognition, text classification, terminology extraction, relationship extraction, and hypothesis generation.\n\n**Explanation:** Text mining technologies systematically process large quantities of text data to identify, categorize, and extract relevant information. Named entity recognition helps identify key biomedical entities. Text classification organizes research papers into categories for easier retrieval. Terminology extraction captures unique biomedical terms and concepts. Relationship extraction identifies interactions between entities, and hypothesis generation proposes potential new research directions based on existing data. These capabilities collectively reduce the burden of information overload by transforming unstructured data into an accessible and useful format.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4385988359",
    "x": 2023.4286256871058,
    "y": -22.16796440628325,
    "title": "Summary of ChatGPT-Related research and perspective towards the future of large language models",
    "authors": [
      "Yiheng Liu",
      "Tianle Han",
      "Siyuan Ma"
    ],
    "first_author": "Yiheng Liu",
    "first_author_surname": "Liu",
    "year": 2023,
    "cited_by_count": 638,
    "venue": "",
    "size": 38.341748684451055,
    "color": "hsl(7, 70%, 60%)",
    "label": "Liu ,2023",
    "rag_problem": "Large language models struggle with maintaining adaptability and performance across diverse domains due to insufficient training on varied data and lack of user feedback integration.",
    "rag_method": "Implementation of large-scale pre-training on extensive data from the world wide web, instruction fine-tuning, and Reinforcement Learning from Human Feedback (RLHF).\n\n**Explanation:** Large-scale pre-training allows models to capture a wide array of knowledge across various domains by leveraging huge datasets available online. Instruction fine-tuning refines the model's capability to follow human-guided tasks more effectively. Reinforcement Learning from Human Feedback (RLHF) further enhances performance by integrating critiques and preferences from real user interactions, thereby aligning model outputs closer to human expectations. Together, these mechanisms improve adaptability and ensure that the models remain proficient across different applications and domains.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Explore advancements in pre-training techniques to further improve the comprehensive knowledge capture from the world wide web for large language models.\n- Investigate enhanced methodologies in instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) to boost adaptability and performance of future LLMs.\n- Examine prospective applications and expand the use of ChatGPT-related models across diverse domains to maximize their impact and utility.\n- Address existing limitations by conducting in-depth analysis of current LLM capabilities and identifying areas for potential improvement.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4390023570",
    "x": 2022.5704755702486,
    "y": 0.8468953468379741,
    "title": "Review of large vision models and visual prompt engineering",
    "authors": [
      "Jiaqi Wang",
      "Zhengliang Liu",
      "Lin Zhao"
    ],
    "first_author": "Jiaqi Wang",
    "first_author_surname": "Wang",
    "year": 2023,
    "cited_by_count": 159,
    "venue": "",
    "size": 25.470207557312186,
    "color": "hsl(7, 70%, 60%)",
    "label": "Wang ,2023",
    "rag_problem": "The complexity and large-scale nature of visual models pose a challenge in tailoring these models for specific downstream tasks due to their rigid structure and parameter settings.",
    "rag_method": "Visual Prompt Engineering, which involves designing and optimizing task-specific input prompts for large visual models without altering their underlying parameters.\n\n**Explanation:** By using visual prompts as guiding signals in the input space, models can effectively adapt to new tasks by learning the task-specific features required for those tasks, thus achieving flexibility and reducing the need for extensive model retraining. This allows the model to leverage already pre-trained knowledge while fine-tuning on specific tasks, enhancing both efficiency and performance.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- While DenseCLIP provides an innovative approach for transferring large pre-trained models to dense tasks, the transferability of these models to dense tasks still remains a challenge that needs further exploration.\n- The CoOP method, despite offering automatic prompt adaptability, exhibits lower generalization performance compared to CLIP on new data, possibly due to overfitting on downstream tasks.",
    "rag_future_work": "- Develop methodologies to design well-crafted prompts that effectively guide large vision models in downstream tasks, enhancing their overall performance.\n- Integrate diverse and comprehensive datasets to expand AI systems' breadth of knowledge, reducing the reliance on domain-specific information and improving general artificial intelligence capabilities.\n- Employ interdisciplinary methodologies to enhance the utility of visual prompts, fostering innovation through collaboration among experts from various domains.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 22
  },
  {
    "id": "W4384484700",
    "x": 2022.9330488817996,
    "y": 9.25547466651178,
    "title": "Evaluating large language models on a highly-specialized topic, radiation oncology physics",
    "authors": [
      "Jason Holmes",
      "Zhengliang Liu",
      "Lian Zhang"
    ],
    "first_author": "Jason Holmes",
    "first_author_surname": "Holmes",
    "year": 2023,
    "cited_by_count": 122,
    "venue": "",
    "size": 24.482012124499917,
    "color": "hsl(7, 70%, 60%)",
    "label": "Holmes ,2023",
    "rag_problem": "Evaluating large language models on widely known topics does not accurately reflect their capabilities due to abundant training resources and familiarity with test data.",
    "rag_method": "Evaluate large language models on a highly-specialized topic, radiation oncology physics, which lacks widespread test preparation materials and is less familiar.\n\n**Explanation:** By testing LLMs on radiation oncology physics, a less common subject, the evaluation can reveal their true ability to understand and reason through specialized scientific and medical topics without relying on training from abundant prep resources. This approach ensures that the models are tested fairly and can highlight potential capabilities and limitations in specialized domains.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The study indicates that individual LLMs like ChatGPT (GPT-4) cannot compete with a small group of medical physicists working together, as humans exhibit a significant variation in capabilities and knowledge, which helps them achieve more accurate outcomes through collaboration.\n- The evaluation method using exams does not accurately represent the nuanced and detailed clinical work performed by medical physicists, potentially misrepresenting the equivalency between the capabilities of LLMs and medical physicists in real-world applications.\n- The high performance of GPT-4 on the certification-like exam could suggest a superficial understanding of highly specialized topics, which raises questions about whether the knowledge being assessed truly reflects deep comprehension or is merely memorization.",
    "rag_future_work": "- Explore the potential collaboration between radiation oncology experts and LLMs like ChatGPT (GPT-4), utilizing these models as highly knowledgeable assistants to enhance clinical practice and decision-making.\n- Develop evaluation methods to test the depth and superficiality of LLM knowledge, focusing on areas not covered by these models to ensure comprehensive and nuanced understanding in medical certification processes.\n- Investigate the impact of prompting strategies, such as asking LLMs to explain their reasoning before answering, on improving accuracy and potentially uncovering emergent properties related to deductive reasoning in large language models.\n- Consider revising the certification procedures within the radiation oncology and broader medical communities to adapt to the increasing capabilities of LLMs, thereby potentially reducing the emphasis on memorizing superficial knowledge.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 27
  },
  {
    "id": "W1966084915",
    "x": 2011.784016918579,
    "y": 0.006282904999793559,
    "title": "The remarkable, yet not extraordinary, human brain as a scaled-up primate brain and its associated cost",
    "authors": [
      "Suzana Herculano‚ÄêHouzel"
    ],
    "first_author": "Suzana Herculano‚ÄêHouzel",
    "first_author_surname": "Herculano‚ÄêHouzel",
    "year": 2012,
    "cited_by_count": 696,
    "venue": "",
    "size": 38.74982285376033,
    "color": "hsl(85, 70%, 60%)",
    "label": "Herculano‚ÄêHouzel ,2012",
    "rag_problem": "‰º†ÁªüËßÇÂøµËÆ§‰∏∫‰∫∫Á±ªÂ§ßËÑëÂõ†ÂÖ∂Áõ∏ÂØπ‰∫é‰ΩìÈáçËæÉÂ§ßÁöÑ‰ΩìÁßØÂíåÂ§çÊùÇÊÄßËÄåÊòæÂæóÂºÇÂ∏∏Ôºå‰ΩÜËøôÁßçËßÇÁÇπÁº∫‰πèÂÖ≥‰∫éÂÖ∂ÂÆûÈôÖÊºîÂåñÂíåËøê‰ΩúÊú∫Âà∂ÁöÑÁªÜËäÇ„ÄÇ",
    "rag_method": "Â∞Ü‰∫∫Á±ªÂ§ßËÑëËßÜ‰Ωú‰∏ÄÁßçËá™ÁÑ∂ÁöÑÊîæÂ§ßÁâàÁÅµÈïøÁ±ªÂ§ßËÑëÔºå‰ªéËÄåÈáçÊñ∞ÂÆö‰πâÂÖ∂‚ÄòÂºÇÂ∏∏‚ÄôÁâπÊÄß‰∏∫Ê≠£Â∏∏ÊºîÂåñËøáÁ®ã‰∏≠ÁöÑÁªìÊûú„ÄÇ\n\n**Explanation:** ÈÄöËøáÊØîËæÉ‰∫∫Á±ªÂ§ßËÑëÁöÑÁªìÊûÑÂíåÂäüËÉΩ‰∏éÂÖ∂‰ªñÁÅµÈïøÁ±ªÂä®Áâ©ÁöÑÁõ∏‰ººÊÄßÔºåÁ†îÁ©∂ÂèØËÉΩËß£ÊûêÂ§ßËÑëÁõ∏ÂØπ‰ΩìÁßØÂíåËÄóËÉΩÊØî‰æãËÉåÂêéÁöÑÊºîÂåñÊú∫Âà∂ÔºåÂ±ïÁ§∫Â§ßËÑëËôΩÂ§ß‰ΩÜ‰ªçÈÅµÂæ™ÁîüÁâ©ÊîæÂ§ßËßÑÂæãÔºå‰ªéËÄåÊåëÊàò‰º†ÁªüÁöÑ‚ÄòÂºÇÂ∏∏ÊÄß‚ÄôËßÇÁÇπ„ÄÇ",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2039154778",
    "x": 2009.0299092666298,
    "y": -0.100084988134158,
    "title": "The neural basis of multisensory integration in the midbrain: Its organization and maturation",
    "authors": [
      "Barry E. Stein",
      "Terrence R. Stanford",
      "Benjamin A. Rowland"
    ],
    "first_author": "Barry E. Stein",
    "first_author_surname": "Stein",
    "year": 2009,
    "cited_by_count": 205,
    "venue": "",
    "size": 26.419748697768185,
    "color": "hsl(106, 70%, 60%)",
    "label": "Stein ,2009",
    "rag_problem": "No clear research problem description found",
    "rag_method": "No clear method description found",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2168015293",
    "x": 2013.9653737229694,
    "y": 8.579247493387422,
    "title": "Parallel processing in the brain's visual form system: an fMRI study",
    "authors": [
      "Yoshihito Shigihara",
      "Semir Zeki"
    ],
    "first_author": "Yoshihito Shigihara",
    "first_author_surname": "Shigihara",
    "year": 2014,
    "cited_by_count": 21,
    "venue": "",
    "size": 18.0147393764268,
    "color": "hsl(71, 70%, 60%)",
    "label": "Shigihara ,2014",
    "rag_problem": "The traditional hierarchical model of form processing in the brain suggests a sequential elaboration of forms from simple to complex in visual areas V1, V2, and V3. This model does not fully explain the simultaneous activation of these areas by forms of varying complexity.",
    "rag_method": "The study suggests a parallel processing strategy for form construction in the brain, where different visual areas (V1-V3) are activated simultaneously by different forms, regardless of complexity.\n\n**Explanation:** By using fMRI data, the study found that simple lines, angles, and rhombuses activate visual areas V1-V3 simultaneously rather than sequentially. This simultaneous activation supports a model where these areas process visual forms in parallel, rather than waiting for inputs to be processed hierarchically. This helps to explain why complex forms do not necessarily produce stronger activation in 'later' visual areas compared to simple forms within 'earlier' areas.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The study cannot make strong claims about the use of a parallel processing strategy due to potential undetected small latency differences (in the 5-10 ms range) between the activation of different brain areas.\n- Technical limitations might prevent the detection of subtle timing differences in the activation of visual areas, which could impact the interpretation of parallel processing evidence.",
    "rag_future_work": "- Investigate the role of areas V2 and V3 in parallel processing alongside hierarchical processing in form perception to understand their contribution to constructing visual forms.\n- Explore potential sources outside of V1 that might contribute to parallel strategies in form elaboration in the primate visual cortex.\n- Conduct studies using technologies beyond fMRI, such as MEG, to gather more comprehensive evidence about the role of parallel processing in the visual form system.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 23
  },
  {
    "id": "W2044823787",
    "x": 2012.9327984402942,
    "y": 0.19182066971172618,
    "title": "Early and parallel processing of pragmatic and semantic information in speech acts: neurophysiological evidence",
    "authors": [
      "Natalia Egorova",
      "Yury Shtyrov",
      "Friedemann Pulverm√ºller"
    ],
    "first_author": "Natalia Egorova",
    "first_author_surname": "Egorova",
    "year": 2013,
    "cited_by_count": 71,
    "venue": "",
    "size": 22.469776821861107,
    "color": "hsl(78, 70%, 60%)",
    "label": "Egorova ,2013",
    "rag_problem": "Understanding the neurophysiological basis of speech acts, particularly how pragmatic and semantic information is processed in the brain during communication using speech acts like Naming and Requesting.",
    "rag_method": "Utilization of time-resolved event-related potential (ERP) technique to measure brain responses during speech acts performed with identical words in closely matched settings.\n\n**Explanation:** The ERP technique allows for precise measurement of the timing and amplitude of brain activity related to speech acts. By presenting stimuli in the form of single words used to perform Naming and Requesting, the study was able to identify distinct neurophysiological responses as early as 120 ms after word onset. This demonstrates that pragmatic information processing occurs in parallel with semantic processes, challenging previous assumptions of serial processing. Specifically, areas such as the fronto-central cortex and the right temporo-parietal junction (TPJ) showed distinct activation patterns for Requests, indicating engagement of action knowledge and social interaction processing.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate the role of the right temporo-parietal cortex in processing different types of speech acts further, as its involvement varies with speech act types like Requests and Naming.\n- Explore the specific contributions of the left perisylvian cortex and bilateral fronto-central cortex in linguistic and action sequence processing across various types of speech acts.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 24
  },
  {
    "id": "W2942091739",
    "x": 2018.6527043074896,
    "y": -6.361465169801561,
    "title": "BadNets: Evaluating Backdooring Attacks on Deep Neural Networks",
    "authors": [
      "Tianyu Gu",
      "Kang Liu",
      "Brendan Dolan-Gavitt"
    ],
    "first_author": "Tianyu Gu",
    "first_author_surname": "Gu",
    "year": 2019,
    "cited_by_count": 997,
    "venue": "",
    "size": 40.43587143842007,
    "color": "hsl(35, 70%, 60%)",
    "label": "Gu ,2019",
    "rag_problem": "Outsourced training of neural networks and use of pre-trained models introduce security risks where an adversary can embed hidden backdoors in a neural network model, leading to specific misclassification on attacker-chosen inputs, while still maintaining high accuracy on standard inputs.",
    "rag_method": "The study demonstrates how adversaries can effectively implement backdoors in neural networks by poisoning the training data in fully outsourced training or through compromised pre-trained models in transfer learning scenarios.\n\n**Explanation:** In outsourced training, an adversary can modify the training data to include backdoored patterns without altering the network architecture, making it indistinguishable from a benign model. By substituting or tampering a pre-trained model during download, adversaries embed triggers that cause misclassification of specific inputs, like misclassifying stop signs as speed limit signs when marked with a specific sticker or pattern.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method depends on the integrity of pre-trained models from sources like the Caffe Model Zoo and Keras Pre-trained Model Library, which may be compromised, affecting the reliability of our evaluations.\n- The approach lacks mechanisms to ensure the integrity and security of pre-trained models, thus relying on existing model suppliers to follow security standards without providing a direct solution.",
    "rag_future_work": "- Investigate methods to detect and mitigate backdoors in machine learning models acquired from online model zoos, addressing the security concerns introduced by outsourced training.\n- Develop strategies to enhance standard validation testing to identify stealthy backdoors within convolutional neural networks before deployment.\n- Explore techniques for creating robust machine learning models that resist backdoor attacks while maintaining high performance on regular inputs.\n- Extend the application of backdoor detection and prevention methods to more complex tasks beyond the MNIST digit recognition and traffic sign detection systems studied in the paper.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 36
  },
  {
    "id": "W3104723404",
    "x": 2017.8363130783075,
    "y": -5.566123226806863,
    "title": "A Survey of Cross-lingual Word Embedding Models",
    "authors": [
      "Sebastian Ruder",
      "Ivan Vuliƒá",
      "Anders S√∏gaard"
    ],
    "first_author": "Sebastian Ruder",
    "first_author_surname": "Ruder",
    "year": 2018,
    "cited_by_count": 543,
    "venue": "",
    "size": 37.58575260268789,
    "color": "hsl(42, 70%, 60%)",
    "label": "Ruder ,2018",
    "rag_problem": "The challenge of developing cross-lingual word embeddings that accurately map word semantics across multiple languages, particularly for low-resource languages.",
    "rag_method": "Mapping-based methods: These methods involve training monolingual word embeddings independently, followed by learning a linear transformation to map one language's embeddings to another language's embeddings.\n\n**Explanation:** Mapping-based approaches aim to learn a linear transformation matrix that projects word embeddings from a source language into the embedding space of a target language. This linear transformation leverages bilingual dictionaries or word alignments to find equivalent representations between languages, thus facilitating cross-lingual semantic comparability in a shared vector space.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The survey only provides an overview and typology of existing cross-lingual word embedding models, without introducing a novel model or approach.\n- It focuses primarily on word-level embeddings and may not adequately address challenges specific to sentence or document-level embeddings in multilingual contexts.\n- While it discusses evaluation methods, the paper does not propose new evaluation metrics or benchmarks for cross-lingual embeddings.",
    "rag_future_work": "- Develop models that incorporate non-linear mapping techniques to better capture language-specific differences, addressing the simplicity of existing linear transformation methods.\n- Create cross-lingual word embeddings for specialized domains with limited parallel data, exploring the use of comparable corpora and multi-modal contexts to enhance robustness.\n- Investigate methods to effectively handle polysemy in cross-lingual embeddings, aiming to reduce false nearest neighbors through improved sense disambiguation and contextual understanding.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 46
  },
  {
    "id": "W2294774419",
    "x": 2014.9010289518205,
    "y": -7.980898035683058,
    "title": "Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation",
    "authors": [
      "Xing Chao",
      "Dong Wang",
      "Chao Liu"
    ],
    "first_author": "Xing Chao",
    "first_author_surname": "Chao",
    "year": 2015,
    "cited_by_count": 414,
    "venue": "",
    "size": 36.31442990410902,
    "color": "hsl(64, 70%, 60%)",
    "label": "Chao ,2015",
    "rag_problem": "Inconsistency among objective functions for word embedding and transform learning, and distance measurement which causes suboptimal estimation for both word vectors and bilingual translation.",
    "rag_method": "Normalize the word vectors on a hypersphere and constrain the linear transform as an orthogonal transform.\n\n**Explanation:** By normalizing word vectors, they are fixed to unit length, ensuring that inner product calculations fall back to cosine distance, resolving inconsistencies in distance measurement. Constraining the linear transform as an orthogonal transform respects this normalization, ensuring transformed vectors maintain unit-length property, thereby achieving consistency between embedding, measurement, and transform objectives.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method is preliminary and has not been tested on other tasks beyond bilingual word translation and monolingual word similarity, leaving its generalizability uncertain.\n- The approach to handling orthogonal transforms between vector spaces with mismatched dimensions is described as ad-hoc, suggesting a lack of rigor or optimization in that aspect.",
    "rag_future_work": "- Investigate alternative objective functions: Future work could explore alternative objective functions that harmonize the learning of word vectors, the distance measurements, and the linear transformations to address the inconsistencies and potentially improve translation performance.\n- Improve dimensional alignment techniques: Further research could focus on developing more efficient methods to handle different dimensions in source and target vector spaces, potentially improving the normalization constraints without introducing additional padding.\n- Expand bilingual word translation accuracy: Subsequent studies could work on enhancing the accuracy of bilingual word translation by refining the methods used for learning linear transformations, especially in contexts involving low-resource languages or large vocabulary sets.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 14
  },
  {
    "id": "W2786464815",
    "x": 2018.0677070734696,
    "y": -3.2801463008926595,
    "title": "An efficient framework for learning sentence representations",
    "authors": [
      "Lajanugen Logeswaran",
      "Honglak Lee"
    ],
    "first_author": "Lajanugen Logeswaran",
    "first_author_surname": "Logeswaran",
    "year": 2018,
    "cited_by_count": 299,
    "venue": "",
    "size": 27.832234562481098,
    "color": "hsl(42, 70%, 60%)",
    "label": "Logeswaran ,2018",
    "rag_problem": "Existing sentence representation learning models are slow to train and often learn representations that depend heavily on the surface form of sentences, which may include irrelevant aspects not central to the semantic meaning.",
    "rag_method": "Proposing a discriminative objective that operates directly in the space of sentence embeddings, where the model identifies the correct target sentence from a set of sentence candidates, instead of reconstructing the surface form.\n\n**Explanation:** By shifting from a generative to a discriminative approach, the proposed mechanism focuses on identifying meaningful semantic embeddings rather than regenerating the entire sentence structure. This allows the model to disregard irrelevant aspects of sentence form and prioritize semantic content, leading to faster training and more effective capture of sentence semantics.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method demonstrates a trade-off between smaller embedding size and training efficiency, resulting in a marginal loss of performance in most downstream tasks.\n- The multi-channel model (MC-QT) trained with varying representation sizes on the BookCorpus dataset does not allow for strong conclusions about the quality of embeddings due to differences in classifiers' sizes for each embedding size.",
    "rag_future_work": "- Future work could explore high-level transformations of sentence representations, such as switching sentiment polarity and handling analogical relationships that involve several words.\n- There is potential to use labeled and structured data like paraphrase or machine translation data to improve sentence representation learning, enhancing their applicability to more complex NLP tasks.\n- Extending pre-trained encoders to support natural language inference (NLI) tasks could yield better sentence representations for diverse downstream tasks, leveraging the relationships of entailment, neutrality, and contradiction between sentence pairs.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 34
  },
  {
    "id": "W2952190837",
    "x": 2016.5996452163604,
    "y": -1.2104844436450717,
    "title": "Offline bilingual word vectors, orthogonal transformations and the\\n inverted softmax",
    "authors": [
      "Samuel Smith",
      "David H. P. Turban",
      "Steven Hamblin"
    ],
    "first_author": "Samuel Smith",
    "first_author_surname": "Smith",
    "year": 2017,
    "cited_by_count": 297,
    "venue": "",
    "size": 27.807100370623232,
    "color": "hsl(49, 70%, 60%)",
    "label": "Smith ,2017",
    "rag_problem": "The challenge of accurately aligning two sets of pre-trained bilingual word vectors offline, without relying heavily on expert bilingual dictionaries.",
    "rag_method": "The use of orthogonal transformations via Singular Value Decomposition (SVD) to align word vector spaces.\n\n**Explanation:** Orthogonal transformations, achieved through SVD, ensure that the vector norms are preserved while aligning the semantic spaces of two languages. This results in a robust transformation that can be executed without extensive bilingual training data, as the orthogonality helps in maintaining the intrinsic similarity between the word vectors even when derived from a pseudo-dictionary based on identical strings present in both languages.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method depends on a training dictionary of 5k English words and their Italian translations, which might limit its applicability in languages or domains with less overlap or available translation pairs.\n- While achieving 40% precision using a pseudodictionary, the precision shows a noticeable drop from 43%, indicating potential scalability challenges with reduced bilingual signal quality.",
    "rag_future_work": "- Investigate the application of sentence vectors for translation: Further explore how sentence vectors can improve translation results, particularly by achieving high precision when retrieving accurate translations from large corpora.\n- Enhance the robustness and precision of bilingual word vectors using alternative corpora: Examine the potential of using different datasets or non-expert bilingual signals to match or improve current translation precision metrics.\n- Explore dimensionality reduction techniques combined with orthogonal transformations: Assess how dimensionality reduction can be optimized alongside orthogonal transformations to potentially unify and improve various approaches for obtaining offline bilingual word vectors.\n- Research the effectiveness of pseudodictionaries for bilingual signal acquisition: Continue studying pseudodictionaries derived from identical word strings across languages, aiming to further verify and enhance their role in achieving precise bilingual vector spaces.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 16
  },
  {
    "id": "W2140406733",
    "x": 2007.5568042142445,
    "y": -0.3431609474031212,
    "title": "Learning Bilingual Lexicons from Monolingual Corpora",
    "authors": [
      "Aria Haghighi",
      "Percy Liang",
      "Taylor Berg-Kirkpatrick"
    ],
    "first_author": "Aria Haghighi",
    "first_author_surname": "Haghighi",
    "year": 2008,
    "cited_by_count": 313,
    "venue": "",
    "size": 28.0036182328809,
    "color": "hsl(113, 70%, 60%)",
    "label": "Haghighi ,2008",
    "rag_problem": "Conventional methods of learning bilingual lexicons typically require parallel corpora, which are not readily available for many language pairs, making the task resource-intensive and limiting the applicability to less-resourced languages.",
    "rag_method": "The authors propose a method to learn bilingual translation lexicons using purely monolingual features such as context counts and orthographic substrings, combined with a generative model based on canonical correlation analysis.\n\n**Explanation:** By leveraging monolingual corpora, the proposed method circumvents the need for parallel corpora. The use of canonical correlation analysis allows for the identification of latent matchings between languages, effectively aligning monolingual lexicons to create high-precision bilingual lexicons. This approach expands the applicability to a broader range of languages and corpus types without the dependency on parallel translations.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2950577311",
    "x": 2013.1909515086202,
    "y": -8.519486133523687,
    "title": "Efficient Estimation of Word Representations in Vector Space",
    "authors": [
      "Tom√°≈° Mikolov",
      "Kai Chen",
      "Greg S. Corrado"
    ],
    "first_author": "Tom√°≈° Mikolov",
    "first_author_surname": "Mikolov",
    "year": 2013,
    "cited_by_count": 11710,
    "venue": "",
    "size": 60,
    "color": "hsl(78, 70%, 60%)",
    "label": "Mikolov ,2013",
    "rag_problem": "Existing NLP systems use simple techniques that reach limits in performance because they treat words as atomic units without capturing similarities and complex relationships.",
    "rag_method": "The paper introduces two novel model architectures, Continuous Bag-of-Words (CBOW) and Continuous Skip-gram, which compute continuous vector representations for capturing word similarities and relationships.\n\n**Explanation:** The CBOW model predicts the current word based on the context surrounding it, while the Skip-gram model predicts context words based on the current word. Both models learn efficient word vectors that preserve syntactic and semantic regularities, enabling them to capture the complex relationships between words, such as semantic analogies. By leveraging distributed and simpler log-linear models without non-linear hidden layers, the architectures efficiently train on large datasets, overcoming the computational limitations of previous models.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The Skip-gram architecture, while performing better on semantic tasks compared to other models, works slightly worse on syntactic tasks than the CBOW model.\n- The complexity of the model is dominated by the N √ó D √ó H term, which implies computational challenges in terms of efficiency when dealing with large vocabulary sizes.",
    "rag_future_work": "- Explore applications of word vectors in automatic extension and verification of facts in Knowledge Bases, leveraging their potential to improve data accuracy and completeness.\n- Compare the proposed word vector techniques to Latent Relational Analysis and other existing methods to evaluate performance differences and potential benefits.\n- Enhance the quality of word vectors to serve as a foundational component for future natural language processing applications, ensuring robust and accurate semantic understanding.\n- Publish and further develop high-performance computing techniques for training word vectors, focusing on optimizing training speed and scalability on large datasets.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 21
  },
  {
    "id": "W2117130368",
    "x": 2008.491682108622,
    "y": -3.7020083582469816,
    "title": "A unified architecture for natural language processing",
    "authors": [
      "Ronan Collobert",
      "Jason Weston"
    ],
    "first_author": "Ronan Collobert",
    "first_author_surname": "Collobert",
    "year": 2008,
    "cited_by_count": 5151,
    "venue": "",
    "size": 57.774430530492786,
    "color": "hsl(113, 70%, 60%)",
    "label": "Collobert ,2008",
    "rag_problem": "The inefficiency and lack of coherence in handling multiple natural language processing tasks separately.",
    "rag_method": "A single convolutional neural network architecture using multitask learning via weight-sharing.\n\n**Explanation:** The architecture integrates multiple NLP tasks into one framework, allowing simultaneous predictions (like part-of-speech tagging and named entity recognition) using shared weights. This coherence and integration reduce redundancy and enhance performance by enabling the model to leverage shared insights across different tasks.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2146502635",
    "x": 2009.8788371365608,
    "y": -3.6803041626373076,
    "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.",
    "authors": [
      "John C. Duchi",
      "Elad Hazan",
      "Yoram Singer"
    ],
    "first_author": "John C. Duchi",
    "first_author_surname": "Duchi",
    "year": 2010,
    "cited_by_count": 8609,
    "venue": "",
    "size": 60,
    "color": "hsl(99, 70%, 60%)",
    "label": "Duchi ,2010",
    "rag_problem": "In online learning and stochastic optimization, traditional gradient methods often struggle to efficiently leverage very predictive yet infrequently observed features due to lack of adaptive mechanisms.",
    "rag_method": "The paper introduces a family of adaptive subgradient methods that utilize proximal functions to dynamically adjust gradient steps based on the geometry of previously observed data.\n\n**Explanation:** By incorporating proximal functions, these adaptive methods can adjust the influence of rarely observed but highly predictive features. This adjustment allows the algorithm to emphasize important data characteristics while minimizing irrelevant noise, improving learning efficiency and model accuracy over iteration.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W1662133657",
    "x": 2010.1857427992973,
    "y": -0.41986711335278437,
    "title": "From Frequency to Meaning: Vector Space Models of Semantics",
    "authors": [
      "Peter D. Turney",
      "Patrick Pantel"
    ],
    "first_author": "Peter D. Turney",
    "first_author_surname": "Turney",
    "year": 2010,
    "cited_by_count": 2827,
    "venue": "",
    "size": 49.86087240531468,
    "color": "hsl(99, 70%, 60%)",
    "label": "Turney ,2010",
    "rag_problem": "Computers lack an understanding of human language, which limits their ability to process and analyze text effectively.",
    "rag_method": "Use of Vector Space Models (VSMs) to represent semantics through mathematical structures like matrices and tensors.\n\n**Explanation:** VSMs represent documents, words, and semantics as vectors in a multidimensional space, where similarities in meaning are expressed as vector similarities. This mathematical representation allows computers to process text based on statistical patterns of word usage, moving from frequency counts of terms to capturing their semantic relevance.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method, based on Vector Space Models (VSMs), does not fully enable computers to understand the semantics of natural language, which is a fundamental challenge in achieving language comprehension.\n- VSMs are part of the solution but still insufficient on their own to bridge the gap between artificial language interfaces and natural human-computer language interactions.",
    "rag_future_work": "- Investigate the limits of distributional hypotheses: Future work could explore whether statistical patterns of word usage are sufficient to fully capture semantic meaning, thus determining the ultimate potential of vector space models (VSMs).\n- Develop methods to represent first-order predicate calculus: There is a need to discover how VSMs can be extended or adapted to represent arbitrary statements in first-order predicate calculus, a current limitation.\n- Improve word order sensitivity in VSMs: Research can focus on enhancing VSMs to account for word order, possibly through the use of composition models or pair-pattern matrices, which impact relational information in language.\n- Explore composition models inspired by different domains: Inspired by quantum mechanics, future work could develop new operators and composition models that integrate word order and contextual sensitivity into VSMs.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 56
  },
  {
    "id": "W4402916447",
    "x": 2024.1252894058357,
    "y": 7.513797072670161,
    "title": "Embodied Intelligence Toward Future Smart Manufacturing in the Era of AI Foundation Model",
    "authors": [
      "Lei Ren",
      "Jiabao Dong",
      "Shuai Liu"
    ],
    "first_author": "Lei Ren",
    "first_author_surname": "Ren",
    "year": 2024,
    "cited_by_count": 19,
    "venue": "",
    "size": 17.656606832130976,
    "color": "hsl(0, 70%, 60%)",
    "label": "Ren ,2024",
    "rag_problem": "Êô∫ËÉΩÂà∂ÈÄ†ËøáÁ®ã‰∏≠ÁöÑÂ§çÊùÇÂä®ÊÄÅÁéØÂ¢ÉÂ§ÑÁêÜËÉΩÂäõ‰∏çË∂≥Ôºå‰º†ÁªüÁ≥ªÁªüÈöæ‰ª•ÈÄÇÂ∫îÂø´ÈÄüÂèòÂåñÁöÑÂà∂ÈÄ†ÂèÇÊï∞ÂíåÁéØÂ¢É„ÄÇ",
    "rag_method": "ÂºïÂÖ•AIÂü∫Á°ÄÊ®°ÂûãËøõË°åÂÆûÊó∂Êï∞ÊçÆÂàÜÊûêÂíåÂÜ≥Á≠ñÔºå‰ª•ÈÄÇÂ∫îÂä®ÊÄÅÂà∂ÈÄ†ÁéØÂ¢É„ÄÇ\n\n**Explanation:** AIÂü∫Á°ÄÊ®°ÂûãÂÖ∑Â§áÂº∫Â§ßÁöÑÂ≠¶‰π†ÂíåÈ¢ÑÊµãËÉΩÂäõÔºåËÉΩÂ§üÂÆûÊó∂ÂàÜÊûêÊù•Ëá™Âà∂ÈÄ†ËøáÁ®ãÁöÑÂêÑÁßçÊï∞ÊçÆÊµÅÔºåÂπ∂ÂÆûÊó∂Ë∞ÉÊï¥Âà∂ÈÄ†Á≥ªÁªüÁöÑÂèÇÊï∞Ôºå‰ΩøÂÖ∂È´òÊïàÂ∫îÂØπÁéØÂ¢ÉÂèòÂåñÂíå‰∏çÁ°ÆÂÆöÊÄß„ÄÇËøôÁßçÊú∫Âà∂ÊèêÈ´ò‰∫ÜÁ≥ªÁªüÁöÑÈÄÇÂ∫îÊÄßÂíåÊô∫ËÉΩÂåñÊ∞¥Âπ≥„ÄÇ",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate the integration of AI foundation models with embodied intelligence systems to enhance automation processes in smart manufacturing. This could lead to more responsive and adaptable manufacturing environments.\n- Explore the development of scalable embodied AI frameworks that are capable of learning and adapting to diverse manufacturing tasks. This research would aim to improve efficiency and reduce the need for human intervention.\n- Analyze the potential for embodied intelligence to drive innovations in sustainable manufacturing practices. This involves leveraging AI models to optimize resource usage and minimize environmental impact.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4407837005",
    "x": 2025.3479597643495,
    "y": -5.6680936816058205,
    "title": "From screens to scenes: A survey of embodied AI in healthcare",
    "authors": [
      "Yihao Liu",
      "Xu Cao",
      "Tingting Chen"
    ],
    "first_author": "Yihao Liu",
    "first_author_surname": "Liu",
    "year": 2025,
    "cited_by_count": 14,
    "venue": "",
    "size": 16.57562773035012,
    "color": "hsl(0, 70%, 60%)",
    "label": "Liu ,2025",
    "rag_problem": "Limited integration of advanced AI technologies into clinical workflows, resulting in inefficiencies and restricted personalization in healthcare services.",
    "rag_method": "Embodied AI (EmAI) systems, integrating AI with robotics and sensor technologies.\n\n**Explanation:** EmAI systems enhance interaction with the physical environment, enabling autonomous operations such as robotic diagnostics, personalized care, and optimized treatments. These systems facilitate direct interaction through integrated multimodal AI models, robotics, and real-time feedback mechanisms across diverse healthcare applications, improving efficiency and personalization.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The development of Embodied AI systems in healthcare is still in its early stages, resulting in challenges such as limited application scope and complexities in integration into existing clinical workflows.\n- Current EmAI systems face substantial concerns around data privacy, system reliability, and ethical considerations, which need to be addressed for effective deployment in real-world healthcare settings.\n- Limitations in data acquisition and capabilities of multimodal large-scale models contribute to fragmented applications of EmAI, where systems may perform specific functions well but struggle with broader tasks, such as planning surgical procedures based on image analysis.",
    "rag_future_work": "- Focus on addressing data privacy and ethical considerations to ensure the safe deployment of EmAI systems in healthcare settings, while ensuring patient and public trust.\n- Advance the development of multimodal sensing capabilities and adaptive learning to enhance the interaction and reliability between humans and AI in medical scenarios.\n- Explore more comprehensive and versatile EmAI systems that can integrate capabilities across different medical domains, addressing the current limitations of fragmented applications.\n- Work on effectively integrating EmAI systems into existing clinical workflows to overcome operational challenges and improve system adoption in real-world healthcare environments.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 62
  },
  {
    "id": "W4407852113",
    "x": 2024.4619557321548,
    "y": -2.6672180051754504,
    "title": "Advancing oil and gas emissions assessment through large language model data extraction",
    "authors": [
      "Zhenlin Chen",
      "Roujia Zhong",
      "Wennan Long"
    ],
    "first_author": "Zhenlin Chen",
    "first_author_surname": "Chen",
    "year": 2025,
    "cited_by_count": 9,
    "venue": "",
    "size": 15.052073257070575,
    "color": "hsl(0, 70%, 60%)",
    "label": "Chen ,2025",
    "rag_problem": "Ê≤πÊ∞îË°å‰∏öÁöÑÊéíÊîæËØÑ‰º∞Èù¢‰∏¥Êï∞ÊçÆËé∑ÂèñÂíåÂàÜÊûêÁöÑÈöæÈ¢òÔºåËøô‰∫õÊï∞ÊçÆÈÄöÂ∏∏ÊòØÈùûÁªìÊûÑÂåñÂíåÂºÇÊûÑÁöÑÔºåÈöæ‰ª•‰ªéÂ§ßÈáèÊñáÊú¨‰∏≠ÊèêÂèñÂíåÂà©Áî®„ÄÇ",
    "rag_method": "Âà©Áî®Â§ßËØ≠Ë®ÄÊ®°ÂûãËøõË°åÊï∞ÊçÆÊèêÂèñÔºå‰ª•Ëá™Âä®ÂåñÂíåÈ´òÊïàÁöÑÊñπÂºè‰ªéÈùûÁªìÊûÑÂåñÊñáÊú¨‰∏≠ÊèêÂèñÁõ∏ÂÖ≥ÁöÑÊéíÊîæ‰ø°ÊÅØ„ÄÇ\n\n**Explanation:** Â§ßËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÁêÜËß£ÂíåÂ§ÑÁêÜËá™ÁÑ∂ËØ≠Ë®ÄÊñáÊú¨ÔºåËá™Âä®ËØÜÂà´‰∏éÊéíÊîæÁõ∏ÂÖ≥ÁöÑ‰ø°ÊÅØÔºåÂπ∂Â∞ÜÂÖ∂ÁªìÊûÑÂåñ‰∏∫ÂèØÂàÜÊûêÁöÑÊï∞ÊçÆ„ÄÇËøôÁßçÊñπÂºèÂáèÂ∞ë‰∫Ü‰∫∫Â∑•Â§ÑÁêÜÊâÄÈúÄÁöÑÊó∂Èó¥ÂíåÈîôËØØÁéáÔºåÊèêÈ´ò‰∫ÜÊï∞ÊçÆÁöÑÂÖ®Èù¢ÊÄßÂíåÁ≤æÁ°ÆÂ∫¶Ôºå‰ªéËÄåÊîπÂñÑ‰∫ÜÊéíÊîæËØÑ‰º∞ÁöÑË¥®Èáè„ÄÇ",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4409385489",
    "x": 2024.552391036324,
    "y": 0.2880299515763144,
    "title": "A comprehensive survey on integrating large language models with knowledge-based methods",
    "authors": [
      "Wenli Yang",
      "Lilian Some",
      "Michael Bain"
    ],
    "first_author": "Wenli Yang",
    "first_author_surname": "Yang",
    "year": 2025,
    "cited_by_count": 8,
    "venue": "",
    "size": 14.6561760966799,
    "color": "hsl(0, 70%, 60%)",
    "label": "Yang ,2025",
    "rag_problem": "Large Language Models (LLMs) face challenges in maintaining accuracy and contextual relevance due to their reliance on static training data, which does not update with new information, leading to potential inaccuracies and knowledge drift over time.",
    "rag_method": "Integrating LLMs with knowledge-based systems, including knowledge graphs and retrieval-augmented generation (RAG) techniques.\n\n**Explanation:** By linking LLMs to external, updated sources like knowledge graphs and employing RAG methods, the models can dynamically access and process real-time information, reducing reliance on outdated, pre-trained data. This integration allows LLMs to ground their outputs in verifiable, structured knowledge, improving accuracy and reducing the risk of knowledge drift.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method faces challenges in dynamic knowledge management, which can affect the efficiency and relevance of knowledge updates during the integration process.\n- There are ongoing difficulties in ensuring model adaptability, which limits the seamless application of integrated systems to rapidly changing real-world scenarios.",
    "rag_future_work": "- Future research should focus on refining integration techniques to enhance the synergy between Large Language Models (LLMs) and knowledge-based systems for improved AI functionality.\n- There is a need to optimize retrieval processes within integrated systems to ensure efficient and accurate access to relevant information.\n- Ensuring that knowledge bases remain current and relevant is crucial for maintaining the reliability and applicability of AI systems across diverse sectors.\n- Exploring dynamic knowledge management and model adaptability can address challenges in maintaining the effectiveness of integrated AI systems.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 71
  },
  {
    "id": "W4408757728",
    "x": 2025.1511959567836,
    "y": 2.578136881327973,
    "title": "Robot learning in the era of foundation models: a survey",
    "authors": [
      "Xuan Xiao",
      "Jiahang Liu",
      "Zhipeng Wang"
    ],
    "first_author": "Xuan Xiao",
    "first_author_surname": "Xiao",
    "year": 2025,
    "cited_by_count": 8,
    "venue": "",
    "size": 14.6561760966799,
    "color": "hsl(0, 70%, 60%)",
    "label": "Xiao ,2025",
    "rag_problem": "The need for robot to transition from traditional fixed automation to general artificial intelligence due to increasing task complexity and environmental variability.",
    "rag_method": "Integration of Large Language Models (LLMs) and multi-modal foundation models into robot learning.\n\n**Explanation:** LLMs and multi-modal foundation models bring a level of human-like intelligence to robots, allowing them to understand complex tasks and engage in zero-shot reasoning. By incorporating modalities such as 2D&3D vision, LiDAR, and voice, robots improve their perception capabilities, effectively closing the perception-action loop and enhancing their generalization and environmental adaptability.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method still faces challenges related to technical aspects, which may impact the effectiveness of robot learning based on foundation models.\n- The ethical aspects of integrating robot learning with foundation models have yet to be fully addressed, posing potential constraints on how these models can be utilized and deployed.\n- The development of exclusive foundation models tailored for robots is still an open challenge, suggesting that current models may not fully meet the specific needs of robot tasks.\n- Dynamic data interaction with the environment remains a limitation in our approach, hinting at the need for more sophisticated models and techniques to handle real-time data integration effectively.",
    "rag_future_work": "- Research on robot hardware and software decoupling is needed to enhance flexibility and interoperability, allowing for more seamless integration of foundation models in diverse robotic systems.\n- Developing dynamic data systems for better interaction between robots and their environments can significantly improve real-time decision-making capabilities and adaptability.\n- The creation of exclusive foundation models tailored specifically for robotic applications could lead to enhanced performance and specialized capabilities in tasks like manipulation and navigation.\n- Investigating ethical aspects of robot learning and foundational models can guide the implementation of more responsible and socially aware robotic applications.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 50
  },
  {
    "id": "W4391759824",
    "x": 2024.0807639520929,
    "y": -10.009410653435912,
    "title": "An Iterative Optimizing Framework for Radiology Report Summarization With ChatGPT",
    "authors": [
      "Chong Ma",
      "Zihao Wu",
      "Jiaqi Wang"
    ],
    "first_author": "Chong Ma",
    "first_author_surname": "Ma",
    "year": 2024,
    "cited_by_count": 48,
    "venue": "",
    "size": 21.02370030322022,
    "color": "hsl(0, 70%, 60%)",
    "label": "Ma ,2024",
    "rag_problem": "Automatic Impression Generation (AIG) in radiology reports using deep learning models, such as BERT, requires substantial amounts of annotated medical data, leading to poor generalization performance, and high demand on data quantity and hardware resources.",
    "rag_method": "The ImpressionGPT framework leverages Large Language Models like ChatGPT, utilizing dynamic prompt generation and iterative optimization to improve AIG tasks without requiring large volumes of domain-specific training data or hardware-intensive finetuning.\n\n**Explanation:** The dynamic prompt generation uses a similarity search to find related reports and construct prompts that include semantically similar examples, enhancing the capture of domain-specific semantic content. The iterative optimization feeds back on the LLM outputs to refine tasks, allowing the model to self-optimize through interactive feedback and become more accurate over iterations. This approach bridges the gap between general-purpose language models and specialized domain tasks, achieving high performance without exhaustive data requirements.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method, ImpressionGPT, although optimized for radiology report summarization, may still struggle with fully capturing the nuances of highly specialized medical terminology and context, which can affect the precision of summaries.\n- The iterative optimization algorithm proposed with our method might involve increased computational resources or time due to the repeated processes required to refine ChatGPT‚Äôs outputs specifically for radiology contexts.\n- While designed to enhance performance specifically for radiology tasks, the adaptability of ImpressionGPT to other specialized domains not covered by the training dataset remains uncertain, indicating limited generalizability beyond radiology reports.",
    "rag_future_work": "- Optimize the prompt design to better incorporate domain-specific data from both public and local sources while addressing data privacy and safety concerns, especially in multi-institution scenarios.\n- Investigate the use of knowledge graphs in prompt design to align with existing domain knowledge, such as relationships among different diseases, for improved contextual relevance.\n- Introduce human experts like radiologists in the prompt optimization process to evaluate and refine the model's outputs interactively, fostering a human-in-the-loop approach.\n- Develop better evaluation criteria that capture higher-level semantic information from text to improve the assessment of model-generated responses beyond conventional metrics like Rouge scores.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 37
  },
  {
    "id": "W4285294723",
    "x": 2021.9133450456768,
    "y": -8.897560150549296,
    "title": "GLM: General Language Model Pretraining with Autoregressive Blank Infilling",
    "authors": [
      "Zhengxiao Du",
      "Yujie Qian",
      "Xiao Liu"
    ],
    "first_author": "Zhengxiao Du",
    "first_author_surname": "Du",
    "year": 2022,
    "cited_by_count": 802,
    "venue": "",
    "size": 39.41476461087856,
    "color": "hsl(14, 70%, 60%)",
    "label": "Du ,2022",
    "rag_problem": "Existing pretraining frameworks like BERT, GPT, and T5 are not flexible enough to perform competitively across all NLP tasks, including NLU, unconditional generation, and conditional generation.",
    "rag_method": "GLM employs autoregressive blank infilling with 2D positional encodings and allows variable-length and arbitrary-order span prediction.\n\n**Explanation:** GLM improves upon previous models by integrating autoregressive mechanisms to infer multiple masked tokens concurrently, contrary to BERT's independent masked token predictions. Its 2D positional encoding ensures rich positional information is retained for each token, supporting diverse generation tasks. With the autoregressive approach, GLM can adaptively handle both understanding and generation tasks by recreating masked spans efficiently, thus performing well across varied NLP tasks.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Despite its innovative autoregressive blank infilling approach, our method might still struggle with efficiently managing different span replacements compared to methods like UniLM, which utilizes a more varied attention mask strategy.\n- Our method's ability to handle dependencies between tokens in multiple lengths could pose challenges if length prediction is required, potentially necessitating more steps in comparison to BERT's prediction approach when lengths are unknown.",
    "rag_future_work": "- Develop a more efficient model for downstream generation tasks that surpasses UniLMv2, focusing on eliminating reliance on masked language modeling for finetuning.\n- Explore unified autoregressive pretraining approaches to better integrate tasks involving both natural language understanding (NLU) and generation to improve model versatility across different applications.\n- Investigate story-like visual explanations using multimedia resources such as movies and books to enhance model performance in multimodal contexts.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 27
  },
  {
    "id": "W3202729335",
    "x": 2021.4661295913597,
    "y": 9.646358900949073,
    "title": "Aspect Sentiment Quad Prediction as Paraphrase Generation",
    "authors": [
      "Wenxuan Zhang",
      "Yang Deng",
      "Xin Li"
    ],
    "first_author": "Wenxuan Zhang",
    "first_author_surname": "Zhang",
    "year": 2021,
    "cited_by_count": 174,
    "venue": "",
    "size": 25.806929515630454,
    "color": "hsl(21, 70%, 60%)",
    "label": "Zhang ,2021",
    "rag_problem": "Existing aspect-based sentiment analysis (ABSA) tasks focus on predicting partial sentiment elements and suffer from error propagation in multi-stage pipeline solutions, failing to provide a complete aspect-level sentiment structure by simultaneously predicting all four sentiment elements (aspect category, aspect term, opinion term, and sentiment polarity).",
    "rag_method": "The Aspect Sentiment Quad Prediction (ASQP) task is introduced, and a novel PARAPHRASE modeling paradigm is proposed to transform ASQP into a paraphrase generation problem using sequence-to-sequence (S2S) models.\n\n**Explanation:** The PARAPHRASE modeling allows ASQP to be solved in an end-to-end manner, eliminating error propagation inherent in pipeline approaches. By linearizing sentiment quads into natural language sequences, it exploits the rich semantics of sentiment elements and leverages the pretrained generative models like T5, thus improving prediction accuracy by generating coherent and comprehensive sentiment element quads directly from input sentences.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method still struggles with the inherent challenges of the ASQP task, indicating that it does not fully capture aspect-level opinion information.\n- The approach requires further exploration to improve the performance, suggesting room for enhancement in handling ABSA tasks.",
    "rag_future_work": "- Develop more sophisticated methods to address the challenges posed by Aspect Sentiment Quad Prediction (ASQP) tasks, enhancing the extraction of aspect-level opinion information.\n- Investigate novel approaches that can better capture and understand the complexities of aspect-based sentiment analysis (ABSA), potentially improving accuracy and insights.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 20
  },
  {
    "id": "W3176690085",
    "x": 2020.7729166565962,
    "y": 12.354812557726035,
    "title": "Towards Generative Aspect-Based Sentiment Analysis",
    "authors": [
      "Wenxuan Zhang",
      "Xin Li",
      "Yang Deng"
    ],
    "first_author": "Wenxuan Zhang",
    "first_author_surname": "Zhang",
    "year": 2021,
    "cited_by_count": 172,
    "venue": "",
    "size": 25.763738836971665,
    "color": "hsl(21, 70%, 60%)",
    "label": "Zhang ,2021",
    "rag_problem": "Existing aspect-based sentiment analysis (ABSA) methods ignore the rich label semantics and require extensive task-specific designs.",
    "rag_method": "The authors propose a unified generative framework using annotation-style and extraction-style modeling paradigms to formulate ABSA tasks as text generation problems.\n\n**Explanation:** By treating ABSA as a text generation problem, the framework fully utilizes the rich semantics of natural language labels by encoding them directly into the output, eliminating the need for task-specific designs and improving adaptability across different ABSA tasks. This approach makes use of the powerful sequence-to-sequence modeling capabilities to capture complex interactions among sentiment elements.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The annotation-style method introduced in our work struggles with more complex tasks like ASTE and TASD because it adds too much content into the target sentence, which complicates sequence-to-sequence learning.\n- Our approach is an initial attempt at transforming ABSA tasks into text generation problems, indicating room for development in designing more effective generation paradigms to improve overall task performance.",
    "rag_future_work": "- Design more effective generation paradigms to further enhance the transformation of ABSA tasks from classification to text generation, improving accuracy and efficiency.\n- Extend the concept of transforming classification tasks into text generation across various other tasks within sentiment analysis and natural language processing, exploring the potential benefits of such transformation.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 16
  },
  {
    "id": "W2834342720",
    "x": 2018.095852863712,
    "y": 8.936169601501556,
    "title": "Learning to Progressively Recognize New Named Entities with Sequence to Sequence Models",
    "authors": [
      "Lingzhen Chen",
      "Alessandro Moschitti"
    ],
    "first_author": "Lingzhen Chen",
    "first_author_surname": "Chen",
    "year": 2018,
    "cited_by_count": 10,
    "venue": "",
    "size": 15.410205801366397,
    "color": "hsl(42, 70%, 60%)",
    "label": "Chen ,2018",
    "rag_problem": "In Named Entity Recognition (NER), there is a challenge of recognizing new named entity categories in a progressive manner without relying on access to the original source data.",
    "rag_method": "The authors propose using a sequence to sequence model with a reshaping and re-parametrization approach in the output layer, combined with transfer learning, to recognize new named entity categories.\n\n**Explanation:** By reshaping and re-parametrizing the output layer of the sequence to sequence model trained on source data, the model can be adapted for new entity categories in target data. Transfer learning enables the model to leverage knowledge from the initial training to learn and identify new categories, even when the source data is unavailable. This allows the system to progressively adapt to changes and new requirements in entity recognition tasks.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate alternative methods for reshaping and re-parametrizing the output layer to improve the adaptability of the model when transferring to new Named Entity categories.\n- Explore the integration of additional contextual or external knowledge sources to enhance the model‚Äôs performance in recognizing new entities with limited labeled data.\n- Develop techniques to retain and utilize historical learning during the transfer process to minimize the loss of information from source data and improve the recognition of new categories.\n- Test the approach on a broader range of languages and domains to assess the model's robustness and generalizability across different NER challenges.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3034328552",
    "x": 2019.9329013616677,
    "y": -11.143901303970232,
    "title": "Dice Loss for Data-imbalanced NLP Tasks",
    "authors": [
      "Xiaoya Li",
      "Xiaofei Sun",
      "Yuxian Meng"
    ],
    "first_author": "Xiaoya Li",
    "first_author_surname": "Li",
    "year": 2020,
    "cited_by_count": 537,
    "venue": "",
    "size": 37.53366037271937,
    "color": "hsl(28, 70%, 60%)",
    "label": "Li ,2020",
    "rag_problem": "Many NLP tasks face severe data imbalance issues where negative examples significantly outnumber positive ones. This leads to a training-test discrepancy since the cross entropy (CE) loss used during training treats each example equally, which biases the model towards the majority class.",
    "rag_method": "Replace the standard cross-entropy loss with dice loss, which is based on the S√∏rensen-Dice coefficient. Dice loss gives equal importance to false positives and false negatives, making it better suited for imbalanced datasets.\n\n**Explanation:** Dice loss is inherently more aligned with the F1 score used in testing, as it considers the balance between precision and recall. By giving equal importance to false positives and false negatives, it reduces the bias towards the majority class present in data-imbalanced contexts. This alignment helps to reduce the training-test discrepancy, resulting in a model that performs better during testing, as evidenced by the improved F1 scores across various tasks.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Investigate the use of dice loss in tasks beyond binary classification in NLP to explore its effectiveness in handling multi-class imbalance scenarios.\n- Further develop dynamic weight adjusting strategies to optimize the influence of easy-negative examples and potentially extend these strategies to other machine learning domains.\n- Explore integrating dice loss with other loss functions such as focal loss to enhance model performance on complex data-imbalanced tasks within NLP.\n- Examine the impact of using dice loss in combination with neural architectures that can inherently manage data imbalance, such as attention mechanisms, to bolster handling of class imbalance in NLP tasks.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 23
  },
  {
    "id": "W3035044482",
    "x": 2020.4788853585749,
    "y": 5.846799825124782,
    "title": "Few-shot Slot Tagging with Collapsed Dependency Transfer and Label-enhanced Task-adaptive Projection Network",
    "authors": [
      "Yutai Hou",
      "Wanxiang Che",
      "Yongkui Lai"
    ],
    "first_author": "Yutai Hou",
    "first_author_surname": "Hou",
    "year": 2020,
    "cited_by_count": 181,
    "venue": "",
    "size": 25.954303215056044,
    "color": "hsl(28, 70%, 60%)",
    "label": "Hou ,2020",
    "rag_problem": "Few-shot slot tagging requires modeling label dependencies, but it's difficult to apply learned label dependencies to unseen domains due to discrepancies in label sets.",
    "rag_method": "Collapsed Dependency Transfer mechanism.\n\n**Explanation:** The Collapsed Dependency Transfer mechanism abstracts domain-specific labels into domain-independent labels and models dependencies between them. This abstraction allows transfer of label dependency information across different domains, overcoming the label set discrepancy issue in few-shot slot tagging.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method struggles with distinguishing similar labels, such as current location and geographic point of interest, indicating a need for better-separated label representations.\n- While collapsed dependency transfer addresses illegal label transitions, it is implied that these issues are prevalent, suggesting potential challenges in consistently maintaining proper label transitions across different domains.\n- The evaluation is limited to 1-shot and 5-shot slot tagging, which may restrict understanding of the method's effectiveness in scenarios with more extensive support sets or in more complex real-world applications.",
    "rag_future_work": "- Explore the application of the collapsed dependency transfer mechanism in other natural language processing tasks to assess its adaptability and effectiveness beyond slot tagging.\n- Investigate the potential of enhancing L-TapNet with more complex semantic representations to further improve label representation and tagging accuracy.\n- Develop more robust few-shot learning strategies that can cope with even more diverse domain-specific label sets to broaden the applicability of the proposed CRF model.\n- Study the integration of the proposed techniques with other models or frameworks to gauge possible improvements in processing efficiency or accuracy in task-oriented dialogue systems.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 28
  },
  {
    "id": "W4405586902",
    "x": 2023.4940946424508,
    "y": 10.159402245150916,
    "title": "Evaluating LLM-based generative AI tools in emergency triage: A comparative study of ChatGPT Plus, Copilot Pro, and triage nurses",
    "authors": [
      "Banu Arslan",
      "√áaƒüatay Nuhoƒülu",
      "Merve Osoydan Satƒ±cƒ±"
    ],
    "first_author": "Banu Arslan",
    "first_author_surname": "Arslan",
    "year": 2024,
    "cited_by_count": 19,
    "venue": "",
    "size": 17.656606832130976,
    "color": "hsl(0, 70%, 60%)",
    "label": "Arslan ,2024",
    "rag_problem": "Emergency triage requires rapid and accurate assessment of patient conditions, which can be challenging for human triage nurses due to workload and variability in cases.",
    "rag_method": "Utilizing LLM-based generative AI tools such as ChatGPT Plus and Copilot Pro to assist in the triage process.\n\n**Explanation:** Generative AI tools can analyze patient data and symptoms quickly, providing consistent recommendations based on vast medical databases and prior training. This helps reduce the cognitive load on human triage nurses and can improve the speed and accuracy of patient assessments. By comparing the AI tools' outcomes with those of experienced triage nurses, the study aims to highlight the advantages and potential areas for improvement in using AI for emergency triage.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4391143839",
    "x": 2023.5925365483738,
    "y": 15.32742377940761,
    "title": "Integrating Deep Learning with Symbolic Reasoning in TinyLlama for Accurate Information Retrieval",
    "authors": [
      "Xingyu Xiong",
      "Mingliang Zheng"
    ],
    "first_author": "Xingyu Xiong",
    "first_author_surname": "Xiong",
    "year": 2024,
    "cited_by_count": 15,
    "venue": "",
    "size": 16.81813430024161,
    "color": "hsl(0, 70%, 60%)",
    "label": "Xiong ,2024",
    "rag_problem": "Large Language Models (LLMs) have inherent limitations in processing contextually complex queries and ensuring factual accuracy in information retrieval.",
    "rag_method": "Integrating deep learning with symbolic reasoning within the TinyLlama model.\n\n**Explanation:** Deep learning excels at intuitive pattern recognition which allows the model to understand and generate relevant responses to complex queries. However, it often lacks the structured logic required for ensuring factual accuracy. By incorporating symbolic reasoning, which uses rule-based logic, the improved TinyLlama model can enhance its ability to retrieve information more accurately by cross-verifying the facts through structured reasoning processes alongside deep learning capabilities. This integration enables the model to leverage both pattern recognition and logic-based validation, thereby addressing the limitations of conventional LLMs.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method still struggles with processing extremely contextually complex queries, where the integration of deep learning and symbolic reasoning might not fully capture the intricacies involved.\n- The approach, while enhancing factual accuracy, may not completely ensure it under all circumstances, potentially leading to errors in information retrieval.",
    "rag_future_work": "- Investigate scaling the integration of deep learning and symbolic reasoning in larger and more complex language models to enhance their information retrieval capability.\n- Explore the application of TinyLlama's approach in different domains to evaluate the versatility and adaptability of its information retrieval capabilities across various types of queries.\n- Develop methodologies to improve contextual understanding and factual accuracy in scenarios where language models traditionally struggle, especially with contextually complex queries.\n- Study the potential of combining this approach with other recent advancements in artificial intelligence to further enhance information retrieval and processing capabilities.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4411183121",
    "x": 2025.3380858462494,
    "y": 8.559505566018915,
    "title": "LLM-Based Cyberattack Detection Using Network Flow Statistics",
    "authors": [
      "Leopoldo Guti√©rrez-Galeano",
      "Juan Jos√© Dom√≠nguez‚ÄêJim√©nez",
      "J√∂rg Sch√§fer"
    ],
    "first_author": "Leopoldo Guti√©rrez-Galeano",
    "first_author_surname": "Guti√©rrez-Galeano",
    "year": 2025,
    "cited_by_count": 4,
    "venue": "",
    "size": 12.447539682010172,
    "color": "hsl(0, 70%, 60%)",
    "label": "Guti√©rrez-Galeano ,2025",
    "rag_problem": "Existing cybersecurity tools are inadequate for detecting new and emerging types of cyberattacks.",
    "rag_method": "The paper proposes the use of large language models (LLMs) for cyberattack detection leveraging network flow statistics.\n\n**Explanation:** LLMs can analyze patterns and anomalies in network flows by learning from vast amounts of data, including diverse types of network statistics. Unlike traditional artificial neural networks designed from scratch, LLMs can incorporate broader contextual information, which helps them adapt to new cyberattack signatures and dynamics more effectively. This improves their ability to detect novel threats by identifying sophisticated patterns that may signal an attack, thereby addressing the issue of outdated detection methods.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4408417845",
    "x": 2025.2045118103363,
    "y": 11.360045780353065,
    "title": "Exploring the opportunities and challenges of using large language models to represent institutional agency in land system modelling",
    "authors": [
      "Yongchao Zeng",
      "Calum Brown",
      "Joanna Raymond"
    ],
    "first_author": "Yongchao Zeng",
    "first_author_surname": "Zeng",
    "year": 2025,
    "cited_by_count": 4,
    "venue": "",
    "size": 12.447539682010172,
    "color": "hsl(0, 70%, 60%)",
    "label": "Zeng ,2025",
    "rag_problem": "Èöæ‰ª•ÊúâÊïàÊ®°ÊãüÂÖ¨ÂÖ±ÊîøÁ≠ñÊú∫ÊûÑÂú®ÂúüÂú∞Á≥ªÁªü‰∏≠ÁöÑÂÜ≥Á≠ñËøáÁ®ã„ÄÇ",
    "rag_method": "‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÊù•Ê®°ÊãüÊîøÁ≠ñÂà∂ÂÆöËøáÁ®ãÔºåÈÄöËøáÂ∞ÜLLMÈ©±Âä®ÁöÑÊú∫ÊûÑ‰ª£ÁêÜÈõÜÊàêÂà∞Âü∫‰∫é‰ª£ÁêÜÁöÑÂúüÂú∞‰ΩøÁî®Ê®°Âûã‰∏≠„ÄÇ\n\n**Explanation:** Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãËÉΩÂ§üÊ®°ÊãüÂ§öÁßçÂÜ≥Á≠ñËøáÁ®ãÔºåÂåÖÊã¨ÊîøÁ≠ñÂÜ≥Á≠ñ„ÄÇÈÄöËøáÂ∞ÜËøô‰∫õÊ®°Âûã‰Ωú‰∏∫‰ª£ÁêÜÂú®ÂúüÂú∞‰ΩøÁî®Ê®°Âûã‰∏≠Ê®°ÊãüÊîøÁ≠ñÂà∂ÂÆöÔºåÂèØ‰ª•Êõ¥ÂáÜÁ°ÆÂú∞ÂèçÊò†Â§çÊùÇÁöÑ‰∫∫Á±ªÂÜ≥Á≠ñËøáÁ®ãÂèäÂÖ∂ÂØπÂúüÂú∞Á≥ªÁªüÁöÑÂΩ±ÂìçÔºå‰ªéËÄåÊèêÈ´òÊ®°ÂûãÁöÑÂáÜÁ°Æ‰ø°ÊÅØ„ÄÇ",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles with accurately capturing the complex and dynamic nature of institutional policy-making processes, which can limit its effectiveness in fully simulating real-world land system decision-making.\n- The integration of LLM-powered institutional agents within the agent-based land use model can result in computational inefficiencies, particularly when scaling up to larger systems.\n- Despite offering novel simulation capabilities, our approach still faces challenges in ensuring the interpretability and transparency of the decisions made by LLM agents, potentially affecting trust and validation of the model outputs.",
    "rag_future_work": "- Investigate the long-term impacts of LLM-powered institutional agents on land system dynamics and policy outcomes. This could involve simulating scenarios over extended periods to understand potential future changes and adaptations.\n- Develop methods to improve the interpretability and transparency of decisions made by LLM agents in the context of land system modelling to address concerns about black-box processes.\n- Explore the integration of multi-agent systems that include LLMs with other types of models or data sources to enhance the accuracy and scope of land-use predictions.\n- Assess the scalability and computational feasibility of using LLMs within larger, more complex agent-based models to ensure their practical application in diverse geographical and institutional contexts.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4381332452",
    "x": 2022.4799237406276,
    "y": -3.914368367305871,
    "title": "Generative artificial intelligence in the metaverse era",
    "authors": [
      "Zhihan Lv"
    ],
    "first_author": "Zhihan Lv",
    "first_author_surname": "Lv",
    "year": 2023,
    "cited_by_count": 244,
    "venue": "",
    "size": 27.07123998523039,
    "color": "hsl(7, 70%, 60%)",
    "label": "Lv ,2023",
    "rag_problem": "The metaverse requires vast amounts of unique and continuously updated content to engage users and create immersive experiences.",
    "rag_method": "Generative artificial intelligence (AI) autonomously generates new content, including text, images, audio, and video, for the metaverse.\n\n**Explanation:** Generative AI can produce diverse and novel content at scale, reducing the manual effort required for content creation. This capability addresses the need for constant content updates and variations, making it easier to maintain user engagement and create immersive experiences within the metaverse.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4390583680",
    "x": 2023.5269181650704,
    "y": 29.641631339557595,
    "title": "The Integration and Utilization of Artificial Intelligence (AI) in Supporting Older/Senior Lecturers to Adapt to the Changing Landscape in Translation Pedagogy",
    "authors": [
      "Nisar Ahmad Koka"
    ],
    "first_author": "Nisar Ahmad Koka",
    "first_author_surname": "Koka",
    "year": 2023,
    "cited_by_count": 7,
    "venue": "",
    "size": 14.213600725181209,
    "color": "hsl(7, 70%, 60%)",
    "label": "Koka ,2023",
    "rag_problem": "Older/senior lecturers find it challenging to adapt to the rapidly changing landscape of translation pedagogy due to the integration of AI tools, which require frequent updates and adjustments to instructional methods.",
    "rag_method": "Utilization of the Technology Acceptance Model (TAM) to assess and improve the acceptance, perceived utility, and ease of use of AI tools among older educators.\n\n**Explanation:** The TAM framework identifies and addresses two primary factors influencing technology adoption: Perceived Utility (PU) and Perceived Ease of Use (PEOU). By using TAM, the research identifies barriers such as complexity and lack of training, leading to the recommendation of targeted training programs for older lecturers. This approach facilitates a better understanding and easier adaptation to AI tools, aligning with educators' work performance and task facilitation perception.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method faces challenges due to the low engagement of older/senior lecturers with AI tools in teaching, influenced by factors such as age-related issues and inadequate training.\n- Ethical concerns, such as privacy and potential job displacement, limit the widespread adoption and integration of AI in translation pedagogy for senior educators.\n- Despite the acknowledged benefits, many senior lecturers find AI tools difficult to use, highlighting a need for targeted training to improve ease of use and maximize potential benefits.",
    "rag_future_work": "- Conduct a comprehensive study on the effectiveness of Continuous Professional Development (CPD) programs tailored for older/senior lecturers to ensure their proficiency in AI technologies and modern pedagogical methods.\n- Explore the development of more advanced AI tools specifically designed to support older educators in adapting to the evolving demands of translation pedagogy.\n- Investigate the long-term impacts of AI integration in translation pedagogy on the teaching effectiveness and adaptability of senior lecturers.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 33
  },
  {
    "id": "W4389456104",
    "x": 2023.0910033684027,
    "y": 35.20813697812012,
    "title": "Unifying Linguistic Landscapes",
    "authors": [
      "Ray Gutierrez"
    ],
    "first_author": "Ray Gutierrez",
    "first_author_surname": "Gutierrez",
    "year": 2023,
    "cited_by_count": 4,
    "venue": "",
    "size": 12.447539682010172,
    "color": "hsl(7, 70%, 60%)",
    "label": "Gutierrez ,2023",
    "rag_problem": "Persistent global language barriers that hamper communication.",
    "rag_method": "Utilization of neural networks in machine translation to achieve near-human-level accuracy.\n\n**Explanation:** Neural networks can learn complex patterns in language, improving the accuracy of translations and thereby reducing misunderstandings across different languages. This near-human-level translation accuracy helps break down communication barriers by ensuring that meanings are accurately conveyed between languages.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Despite achieving near-human-level accuracy, our method still struggles with potential misinterpretations and errors in translation, particularly in context-sensitive language nuances.\n- The integration of nanotechnology for real-time translation presents challenges related to the accurate capture and representation of nuanced interactions in augmented reality and wearable devices.\n- Our approach may face difficulties in ensuring the technology's equitable access and usability across diverse linguistic groups, potentially limiting its global impact.",
    "rag_future_work": "- Investigate the potential of neural networks to improve machine translation accuracy to consistently achieve near-human levels. This involves exploring advanced algorithms and models to handle more complex linguistic nuances.\n- Develop nanotechnology applications for real-time translation through augmented reality and wearable devices, aiming to create seamless communication tools that integrate into daily life.\n- Address the challenges posed by these emerging technologies, such as potential misuse and ethical concerns, by establishing guidelines and frameworks for responsible implementation.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4399213274",
    "x": 2024.2567733243295,
    "y": 48.860484631186274,
    "title": "Human Intelligence and Artificial Intelligence in Professional Translations ‚Äî Redesigning the Translator Profession",
    "authors": [
      "Felicia Constantin",
      "Anamaria-Mirabela Pop",
      "Monica-Ariana Sim"
    ],
    "first_author": "Felicia Constantin",
    "first_author_surname": "Constantin",
    "year": 2024,
    "cited_by_count": 4,
    "venue": "",
    "size": 12.447539682010172,
    "color": "hsl(0, 70%, 60%)",
    "label": "Constantin ,2024",
    "rag_problem": "The profession of translators faces a threat from AI models like ChatGPT, which can lead to a reduced demand for human translators and a shift towards post-editing rather than traditional human translation.",
    "rag_method": "Rethink the training and sustainability strategies for the translator profession, emphasizing the integration and usage of AI capabilities alongside human skills.\n\n**Explanation:** By redesigning the training curriculum for translators to include AI tools and techniques, human translators can use AI as an ally, enhancing the efficiency and accuracy of translations while maintaining their role for critical and context-sensitive content. This approach aims to shift the focus from translation to post-editing, leveraging AI's cost-effectiveness and scalability while preserving human translators' expertise for complex tasks.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method currently struggles with providing the same level of contextual understanding and emotional tone in translations as human translators, especially in critical or complex content, creative materials, or highly specialized domains.\n- AI-generated economic translations may still contain hesitations, ambiguities, or inaccuracies, necessitating verification and review by human users before utilization in professional contexts.\n- Despite AI's ability to process multiple languages effortlessly, the professional translations produced by paid AI programs still involve certain costs, including training costs for users.",
    "rag_future_work": "- Explore the effectiveness of AI-generated translations across various specific domains, including medical, legal, ethnic, and literary, to evaluate their quality compared to human translations.\n- Investigate ethical and technological performance aspects in AI translations to ensure that professionals maintain high standards and remain aware of the implications of relying on AI.\n- Develop educational programs and training/retraining for translators to adapt to the evolving demands of post-editing AI-generated content, focusing on enhancing philological skills and contextual understanding.\n- Examine the shift in the translation market from traditional human translation to focused post-editing roles, analyzing how this impacts job structures and professional development for translators.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 9
  },
  {
    "id": "W4394684629",
    "x": 2024.124620440916,
    "y": 55.710815159033245,
    "title": "Enhancing translation pedagogy through culture-specific terms",
    "authors": [
      "Matteo Sanesi"
    ],
    "first_author": "Matteo Sanesi",
    "first_author_surname": "Sanesi",
    "year": 2024,
    "cited_by_count": 3,
    "venue": "",
    "size": 11.609067150120806,
    "color": "hsl(0, 70%, 60%)",
    "label": "Sanesi ,2024",
    "rag_problem": "Culture-specific terms pose a challenge in translation, often lacking direct equivalents in other languages, leading to misreadings and cultural insensitivity.",
    "rag_method": "Developing a pedagogical framework focusing on cultural competence, specialized training, authentic resources, and practical exercises.\n\n**Explanation:** By enhancing cultural competence among translators through specialized training and exposure to authentic resources, translators can better grasp the cultural context and nuances of these terms. This framework provides translators-in-training with practical exercises that incorporate real-life scenarios, aiding in their ability to handle culture-specific terms effectively and ensuring accurate translations that respect both the source and target cultures.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The study is limited by the restricted range of the article format, which limits the breadth of comparative data and therefore restricts the correlation among various linguistic modes.\n- The misapplication of pedagogical techniques, stemming from subjective individual approaches, presents a challenge in standardizing translation strategies for broader classroom use.\n- The absence of a unified approach towards translating culture-specific terms indicates that while focusing on these terms is beneficial, a comprehensive solution is not yet available.\n- The subjective nature of translation presents a risk of misinterpretation across languages, as culture-specific terms often lack clarity and consistency in their equivalents across different languages.",
    "rag_future_work": "- Conduct a large-scale comparison of culture-specific terms across various languages to enrich translation pedagogy and establish fruitful correlations among linguistic modes.\n- Redefine translation pedagogy to incorporate universal educational approaches that address the subjective nature of current translation strategies and enhance pedagogical techniques globally.\n- Develop a unified pedagogical system that comprehends culture-specific terms, offering aspiring translators the necessary tools for effective translation processes.\n- Investigate the complex reality of culture-specific terms, focusing on analyzing their deep meanings and overcoming clarity issues when compared to equivalents in other languages.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 5
  },
  {
    "id": "W2083078026",
    "x": 2014.2822761743714,
    "y": 13.337308038626082,
    "title": "Ethical Aspects of Translation: Striking a Balance between Following Translation Ethics and Producing a TT for Serving a Specific Purpose",
    "authors": [
      "Rafat Y. Alwazna"
    ],
    "first_author": "Rafat Y. Alwazna",
    "first_author_surname": "Alwazna",
    "year": 2014,
    "cited_by_count": 10,
    "venue": "",
    "size": 15.410205801366397,
    "color": "hsl(71, 70%, 60%)",
    "label": "Alwazna ,2014",
    "rag_problem": "Translators face a conflict between adhering strictly to translation ethics, which demands preserving the original text's form and content, and adapting the translation to suit the cultural context and expectations of the target audience.",
    "rag_method": "Developing a balanced approach that respects translation ethics while allowing for adaptation to meet the target audience's expectations and cultural norms.\n\n**Explanation:** This approach requires translators to carefully consider the specific text type, the purpose of the translation, and the target audience. By doing so, translators can determine which elements of the original can be preserved and which need adaptation, allowing them to produce a translation that both respects the original text's integrity and serves its intended purpose effectively in the target culture.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- There is a challenge in reconciling translation norms with translation ethics, as norms can sometimes conflict with ethical considerations, particularly when ethics suggest following the source text verbatim.\n- Translators may face ethical dilemmas when required to translate offensive content, which may lead them to withdraw, indicating the difficulty in balancing personal ethics with professional obligations.\n- The approach of viewing translation as a space 'in between' two languages is problematic, as it can lead to misconceptions about the nature of translation engagement and collaboration.",
    "rag_future_work": "- Investigate the balance between following translation ethics and adapting the target text to cultural settings, with a focus on specific text types and audience expectations to enhance comprehension and functionality.\n- Develop strategies for translators to manage ethical dilemmas when faced with offensive content, including potential withdrawal options and guidelines for decision-making.\n- Explore the role of translator training programs in equipping translators with the skills needed to maintain ethical standards while producing culturally and functionally appropriate translations.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 7
  },
  {
    "id": "W3133702157",
    "x": 2020.4130005109453,
    "y": -19.55833819857894,
    "title": "On the Dangers of Stochastic Parrots",
    "authors": [
      "Emily M. Bender",
      "Timnit Gebru",
      "Angelina McMillan-Major"
    ],
    "first_author": "Emily M. Bender",
    "first_author_surname": "Bender",
    "year": 2021,
    "cited_by_count": 4223,
    "venue": "",
    "size": 56.65504331269374,
    "color": "hsl(21, 70%, 60%)",
    "label": "Bender ,2021",
    "rag_problem": "The increasing deployment of large language models in AI, which operate as black boxes, raises ethical concerns about transparency and potential misuse, as these models can reproduce biases and lack meaningful explainability.",
    "rag_method": "The 'Parrot Paper' argues for increased awareness and ethical questioning within the AI community, suggesting the need for improvements in transparency and accountability of large language models.\n\n**Explanation:** By highlighting the ethical issues and potential risks of using large 'black box' language models, the authors aim to stimulate a broader discussion in the AI community. The goal is to foster a more cautious and reflective approach to developing and deploying AI systems, ensuring that the implications of such models are critically examined and that efforts to improve their transparency are prioritized. This paper serves as a catalyst for urging researchers to consider the social impacts and ethical dimensions of AI technology beyond technical advancements.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The methodology described in the Parrot Paper lacks resilience and discourages the detection of weak signals and the sagacity required for serendipity, presenting a significant limitation.\n- The paper presents a reductionist view that limits scientific progress by neglecting alternative perspectives, hypotheses, and explanations, thereby offering a narrow scope for understanding.\n- The reliance on a curation-based approach is asserted to potentially overcome certain limitations, but the paper fails to provide any explanation or evidence on how it would achieve this.",
    "rag_future_work": "- Explore ways to measure and disclose both the environmental costs and benefits of running large language models to enable informed evaluations and decision-making regarding their trade-offs.\n- Investigate methods to improve model efficiency, chip design, and machine operation to reduce the environmental impact of large language models.\n- Develop smaller, curated sets of language models that minimize ambiguity and eliminate \"bad\" forms of language, enhancing the clarity and ethical use of language generated by AI.\n- Address the risks of confirmation bias in scientific presentation and reporting to prevent the public's distrust and misrepresentation of research findings.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 7
  },
  {
    "id": "W3183428091",
    "x": 2014.223350631073,
    "y": 11.450930405463334,
    "title": "TEACHING ETHICS AND CRITICAL THINKING IN CONTEMPORARY SCHOOLS",
    "authors": [
      "Bojan Borstner",
      "Smiljana Gartner"
    ],
    "first_author": "Bojan Borstner",
    "first_author_surname": "Borstner",
    "year": 2014,
    "cited_by_count": 11,
    "venue": "",
    "size": 15.737155198460755,
    "color": "hsl(71, 70%, 60%)",
    "label": "Borstner ,2014",
    "rag_problem": "The gap in educational curricula where ethics and critical thinking are minimally emphasized, despite their importance in making irreversible life and death decisions that affect both individuals and society as a whole.",
    "rag_method": "Integrating ethics and critical thinking into the educational curriculum using critical, reflective decision-making processes that encourage ethical exploration and understanding, rather than passive acceptance of established answers.\n\n**Explanation:** The integration of ethics and critical thinking into curricula addresses the gap by equipping students with essential skills like evaluating arguments and evidence, raising pertinent questions, and practicing through case studies. This approach encourages students to engage in critical, reflective morality, which is necessary for deep moral understanding and ethical decision-making. It moves away from intuitive and authority-based decision-making methods, promoting systematic thought processes that are more robust and sustainable for long-term ethical reasoning.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method may not effectively develop the skill of intuitive understanding in students, as it primarily focuses on analytical and deliberative decision-making up to the proficient level without adequately addressing the transition to expert-level intuition.\n- The approach might struggle in helping students move past the competent level since it provides limited guidance on cultivating holistic perception and situational awareness necessary for becoming proficient and eventually an expert.",
    "rag_future_work": "- Exploring and integrating systematic ethical judging into current educational frameworks could enhance critical thinking capabilities among students, as mentioned in the article's plan to elaborate on this method in future discussions.\n- Developing methodologies to engage students in ethically charged debates, such as the seminar task involving euthanasia, might refine their ability to justify decisions with self-reflection and rationale.\n- Investigating the application of ethics in diverse disciplinary contexts (e.g., medicine, economy, law) can foster a holistic understanding of ethical issues, thereby preparing students for real-world moral dilemmas.\n- Addressing potential contradictions and gaps in ethical codes and expert opinions could improve decision-making strategies that account for ambiguous or unanticipated professional scenarios.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 11
  },
  {
    "id": "W4244669226",
    "x": 2019.4453081801,
    "y": 11.028182622802214,
    "title": "The Routledge Handbook of Translation and Ethics",
    "authors": [
      "Koskinen, Kaisa 1966-",
      "Pokorn, Nike K. 1967-"
    ],
    "first_author": "Koskinen, Kaisa 1966-",
    "first_author_surname": "1966-",
    "year": 2020,
    "cited_by_count": 76,
    "venue": "",
    "size": 22.722055952976504,
    "color": "hsl(28, 70%, 60%)",
    "label": "1966- ,2020",
    "rag_problem": "Ethical dilemmas faced by various actors in the field of translation and interpreting, including the lack of comprehensive guidance on emerging ethical issues.",
    "rag_method": "The Routledge Handbook of Translation and Ethics provides a comprehensive overview and analysis of philosophical and theoretical underpinnings of ethical thinking in Translation Studies, along with coverage of emerging issues.\n\n**Explanation:** By compiling the insights and analyses from leading scholars and new voices, the handbook serves as a central resource for understanding and addressing ethical dilemmas in translation and interpreting. It offers theoretical foundations and practical discussions that can guide both practitioners and researchers in making ethical decisions, thus directly addressing the need for a more structured approach to ethics in this field.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4241903662",
    "x": 2018.4815650863554,
    "y": 9.332673461047829,
    "title": "Machine Translation and Global Research: Towards Improved Machine Translation Literacy in the Scholarly Community",
    "authors": [
      "Lynne Bowker",
      "Jairo Buitrago"
    ],
    "first_author": "Lynne Bowker",
    "first_author_surname": "Bowker",
    "year": 2019,
    "cited_by_count": 171,
    "venue": "",
    "size": 25.741955890909324,
    "color": "hsl(35, 70%, 60%)",
    "label": "Bowker ,2019",
    "rag_problem": "Â≠¶ÊúØÁïåÂØπÊú∫Âô®ÁøªËØëÊäÄÊúØÁöÑÁêÜËß£ÂíåÂ∫îÁî®‰∏çÂ§üÂÖÖÂàÜÔºåÂØºËá¥Âú®ÂÖ®ÁêÉÁ†îÁ©∂‰∏≠Èöæ‰ª•ÂáÜÁ°ÆËé∑ÂèñË∑®ËØ≠Ë®Ä‰ø°ÊÅØ„ÄÇ",
    "rag_method": "ÊèêÈ´òÂ≠¶ÊúØÁïåÁöÑÊú∫Âô®ÁøªËØëÁ¥†ÂÖªÔºå‰ΩøÁ†îÁ©∂‰∫∫ÂëòËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞‰ΩøÁî®Êú∫Âô®ÁøªËØëÂ∑•ÂÖ∑„ÄÇ\n\n**Explanation:** ÈÄöËøáÊèêÂçáÊú∫Âô®ÁøªËØëÁ¥†ÂÖªÔºåÁ†îÁ©∂‰∫∫ÂëòËÉΩÂ§üÁêÜËß£Êú∫Âô®ÁøªËØëÁöÑ‰ºòÁº∫ÁÇπ‰ª•ÂèäÈÄÇÁî®Âú∫ÊôØÔºå‰ªéËÄåÂú®ÂÖ®ÁêÉÁ†îÁ©∂‰∏≠Êõ¥ÂáÜÁ°ÆÂú∞Ëé∑ÂèñÂíå‰∫§ÊµÅ‰ø°ÊÅØ„ÄÇËøôÊúâÂä©‰∫éÂáèÂ∞ëË∑®ËØ≠Ë®ÄÊ≤üÈÄö‰∏≠ÁöÑËØØËß£ÂíåÈîôËØØÔºåÂ¢ûÂº∫ÂõΩÈôÖÂêà‰Ωú‰∏é‰ø°ÊÅØÂÖ±‰∫´ÁöÑË¥®Èáè„ÄÇ",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Develop comprehensive educational programs tailored for researchers to enhance machine translation literacy and ensure effective use of translation tools in academic publishing.\n- Conduct empirical studies to assess the impact of improved machine translation literacy on research outcomes and cross-cultural scholarly communication.\n- Explore advancements in machine translation technology that specifically address challenges faced by the scholarly community, such as technical jargon and nuanced language interpretation.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4399009725",
    "x": 2024.4031919141505,
    "y": -2.891639766710494,
    "title": "Reducing LLM Hallucination Using Knowledge Distillation: A Case Study with Mistral Large and MMLU Benchmark",
    "authors": [
      "Daniel McDonald",
      "Rachael Papadopoulos",
      "Leslie Benningfield"
    ],
    "first_author": "Daniel McDonald",
    "first_author_surname": "McDonald",
    "year": 2024,
    "cited_by_count": 28,
    "venue": "",
    "size": 19.05277460881511,
    "color": "hsl(0, 70%, 60%)",
    "label": "McDonald ,2024",
    "rag_problem": "Large language models often generate incorrect or fabricated information, a phenomenon known as hallucination, which undermines the reliability of AI-generated content.",
    "rag_method": "The authors apply knowledge distillation, transferring knowledge from a high-capacity teacher model to a smaller student model, to reduce hallucination in large language models.\n\n**Explanation:** Knowledge distillation involves training a student model to mimic the responses of a larger, more accurate teacher model. By aligning the student model's outputs more closely with the teacher's, the student model learns to generate more accurate content, thus reducing hallucination. The transfer of learned principles and subtleties from the teacher model increments the student model's understanding and reduces its propensity to generate fabricated content.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method primarily relies on the capability of the teacher model, and if this model has limitations or biases, they might be transferred to the student model despite the distillation process.\n- While knowledge distillation reduces hallucination rates, the method may still struggle with complex or nuanced queries where the teacher model itself is not adequately accurate or reliable.\n- This approach may require substantial computational resources for training both teacher and student models, which can limit accessibility for some applications or research institutions.",
    "rag_future_work": "- Investigate the impact of different teacher-student model architectures in knowledge distillation on reducing hallucinations, potentially revealing optimal configurations for specific tasks.\n- Explore the integration of external knowledge sources during the distillation process to further enhance the reduction of AI model hallucinations.\n- Evaluate the scalability of the proposed knowledge distillation technique across various AI system sizes, ensuring reliability in a broader range of applications.\n- Study long-term effects of reducing hallucinations on user trust and engagement with AI systems, providing insights for application in real-world scenarios.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4396701991",
    "x": 2023.6672390566607,
    "y": 2.8020957955653394,
    "title": "Analyzing and Mitigating Cultural Hallucinations of Commercial Language Models in Turkish",
    "authors": [
      "Yiƒüithan Boztemir",
      "Nil√ºfer √áalƒ±≈ükan"
    ],
    "first_author": "Yiƒüithan Boztemir",
    "first_author_surname": "Boztemir",
    "year": 2024,
    "cited_by_count": 24,
    "venue": "",
    "size": 18.49507936402034,
    "color": "hsl(0, 70%, 60%)",
    "label": "Boztemir ,2024",
    "rag_problem": "Commercial language models exhibit cultural hallucinations when processing Turkish, leading to inaccuracies in cultural representation.",
    "rag_method": "The authors employ a dual approach combining quantitative metrics to evaluate cultural inaccuracies in language models.\n\n**Explanation:** By using quantitative metrics, the authors can systematically identify and measure the extent of cultural hallucinations in commercial language models like Google Gemini 1.5, ChatGPT-4, and Claude 3 Sonet. This approach allows for the precise quantification of where and how these models fail to accurately represent Turkish culture, providing specific insights into their deficiencies. It sets the groundwork for targeted mitigation strategies to improve cultural accuracy.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4398173773",
    "x": 2024.5534574486508,
    "y": 13.205354213068313,
    "title": "Achieving Higher Factual Accuracy in Llama LLM with Weighted Distribution of Retrieval-Augmented Generation",
    "authors": [
      "ÁõñÊåØÂçé",
      "Lianxin Tong",
      "Quan Ge"
    ],
    "first_author": "ÁõñÊåØÂçé",
    "first_author_surname": "ÁõñÊåØÂçé",
    "year": 2024,
    "cited_by_count": 19,
    "venue": "",
    "size": 17.656606832130976,
    "color": "hsl(0, 70%, 60%)",
    "label": "ÁõñÊåØÂçé ,2024",
    "rag_problem": "LlamaÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂú®ÁîüÊàêÊñáÊú¨Êó∂ÁöÑ‰∫ãÂÆûÂáÜÁ°ÆÊÄßÂíå‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÊÄß‰∏çË∂≥„ÄÇ",
    "rag_method": "Â∞ÜÂä†ÊùÉÂàÜÂ∏ÉÁöÑÊ£ÄÁ¥¢Â¢ûÂº∫ÁîüÊàêÔºàRAGÔºâÈõÜÊàêÂà∞LlamaÊ®°Âûã‰∏≠„ÄÇ\n\n**Explanation:** ÈÄöËøáÂºïÂÖ•Âä†ÊùÉRAGÔºåÊ®°ÂûãÂú®ÁîüÊàêÊñáÊú¨Êó∂‰ºòÂÖàËÄÉËôëÈ´òË¥®ÈáèÁöÑ‰ø°ÊÅØÊù•Ê∫ê„ÄÇËøô‰∏ÄÊú∫Âà∂Â∏ÆÂä©Ê®°ÂûãÂú®ÁîüÊàêËøáÁ®ã‰∏≠Êõ¥ÂáÜÁ°ÆÂú∞ËØÜÂà´Âíå‰ΩøÁî®Êù•Ëá™Ê£ÄÁ¥¢Èò∂ÊÆµÁöÑ‰ø°ÊÅØÔºå‰ªéËÄåÊèêÈ´òÁîüÊàêÊñáÊú¨ÁöÑ‰∫ãÂÆûÂáÜÁ°ÆÊÄßÂíå‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÊÄß„ÄÇÂÆûÈ™åÁªìÊûúÊòæÁ§∫ÔºåÈááÁî®Âä†ÊùÉRAGÁöÑÊ®°ÂûãÂú®Â§ö‰∏™ËØÑ‰º∞ÊåáÊ†á‰∏äÂ¶ÇÁ≤æÁ°ÆÂ∫¶„ÄÅÂè¨ÂõûÁéá„ÄÅF1ÂàÜÊï∞ÂíåBLEUÂàÜÊï∞ÊñπÈù¢ÈÉΩÊúâÊòæËëóÊèêÂçáÔºåË°®ÊòéËØ•ÊñπÊ≥ïÊúâÊïàÂú∞Ëß£ÂÜ≥‰∫ÜÂéüÂßãÊ®°ÂûãÂú®‰∫ãÂÆûÊÄßÂíåÁõ∏ÂÖ≥ÊÄß‰∏äÁöÑ‰∏çË∂≥„ÄÇ",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Explore the integration of more diverse retrieval mechanisms in the RAG process to enhance the breadth of information and potentially improve factual accuracy further.\n- Investigate the application of the weighted RAG approach to other language models beyond Llama to assess its generalizability and effectiveness across different architectures.\n- Conduct a deeper analysis into the trade-offs between computational cost and accuracy improvements provided by weighted RAG to optimize performance for large-scale applications.\n- Develop a more granular evaluation framework that includes additional metrics to capture subtleties in contextual relevance and factual accuracy improvements in generated text.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4399395229",
    "x": 2024.0114618257478,
    "y": 20.691854648413685,
    "title": "Dynamic Supplementation of Federated Search Results for Reducing Hallucinations in LLMs",
    "authors": [
      "Jichang Chen",
      "Xinnan Huang",
      "Yongping Li"
    ],
    "first_author": "Jichang Chen",
    "first_author_surname": "Chen",
    "year": 2024,
    "cited_by_count": 14,
    "venue": "",
    "size": 16.57562773035012,
    "color": "hsl(0, 70%, 60%)",
    "label": "Chen ,2024",
    "rag_problem": "LLMs such as the Mistral Large model often produce factually incorrect or misleading outputs, known as hallucinations, due to their reliance on outdated or static data.",
    "rag_method": "The approach dynamically supplements LLM responses with real-time data from federated search engines, integrating information from multiple live sources into the model's outputs.\n\n**Explanation:** By incorporating real-time search engine data into the LLM's responses, the mechanism ensures that the information provided by the model is updated and accurate, effectively reducing the likelihood of hallucinations. The freshness and diversity of the data from multiple sources help verify and correct any potentially erroneous information the model might propose, enhancing the response accuracy.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method depends heavily on the availability and accuracy of real-time federated search engine results, making it less effective if the search engines provide incomplete or biased information.\n- Integrating real-time data from multiple search engines may introduce latency, thus potentially delaying the generation of responses.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4396675319",
    "x": 2023.600238887402,
    "y": 22.947488329854835,
    "title": "Reducing Cultural Hallucination in Non-English Languages Via Prompt Engineering for Large Language Models",
    "authors": [
      "Kanato SATO",
      "Haruto Kaneko",
      "Mei Fujimura"
    ],
    "first_author": "Kanato SATO",
    "first_author_surname": "SATO",
    "year": 2024,
    "cited_by_count": 14,
    "venue": "",
    "size": 16.57562773035012,
    "color": "hsl(0, 70%, 60%)",
    "label": "SATO ,2024",
    "rag_problem": "Large language models exhibit cultural hallucinations when generating content in non-English languages due to a lack of cultural sensitivity and nuanced understanding.",
    "rag_method": "Utilize prompt engineering with tailored prompts that incorporate deep cultural and linguistic insights to mitigate cultural hallucinations in large language models.\n\n**Explanation:** By designing prompts that explicitly incorporate cultural and linguistic context, the language models are guided to generate responses that are culturally aware and sensitive. This strategic formulation helps in aligning the model output with the cultural nuances of various non-English languages, thereby reducing the risk of generating incorrect or culturally insensitive content.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Explore the application of prompt engineering techniques across a wider range of language models to determine their generalizability and impact on reducing cultural hallucinations in non-English languages.\n- Develop a comprehensive framework to incorporate deeper cultural and linguistic insights systematically into prompt engineering, enhancing the cultural sensitivity of language models further.\n- Conduct longitudinal studies to evaluate the long-term effects of tailored prompt engineering on improving the accuracy and reliability of language models in diverse cultural contexts.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W3209721572",
    "x": 2020.7778190438553,
    "y": 17.53733460715317,
    "title": "Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey",
    "authors": [
      "Bonan Min",
      "Hayley Ross",
      "Elior Sulem"
    ],
    "first_author": "Bonan Min",
    "first_author_surname": "Min",
    "year": 2021,
    "cited_by_count": 159,
    "venue": "",
    "size": 25.470207557312186,
    "color": "hsl(21, 70%, 60%)",
    "label": "Min ,2021",
    "rag_problem": "Traditional NLP models require task-specific feature engineering and are limited by the amount of labeled data available, which confines their ability to capture language nuances and achieve high performance across diverse tasks.",
    "rag_method": "The use of large pre-trained transformer-based language models, specifically through the paradigm of pre-train then fine-tune, allows for training on a vast amount of unlabeled data to learn generalized latent feature representations that can be fine-tuned for specific tasks using smaller amounts of labeled data.\n\n**Explanation:** By pre-training on language modeling tasks using vast amounts of naturally occurring text, these models capture comprehensive language representations. These representations can be shared across multiple NLP tasks, mitigating the need for extensive task-specific feature engineering and enabling better performance even with limited labeled data through task-specific fine-tuning.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- There is a preliminary theoretical understanding of the paradigms presented, with insufficient analysis on the generalization across models and languages, making it unclear what precisely contributes to their success.\n- The amount of labeled data required by large pre-trained language models (PLMs) for various NLP tasks remains undetermined, with a lack of rigorous experiments designed to assess this aspect.\n- Integrating implicit semantic information using QA as a supervision signal has not been fully explored, leaving potential benefits unquantified and the impact on different tasks uncertain.",
    "rag_future_work": "- Investigating the complementarity of different pre-trained language models (PLMs) is a future research direction, as combining multiple PLMs may yield further improvements over using a single PLM for various NLP tasks.\n- Exploring the necessity and optimization of meaningful prompts in few-shot learning scenarios could further enhance understanding of how PLMs exploit training data patterns versus interpreting meaningful instructions.\n- Researching the amount of unlabeled data required for effective model training remains an open question, especially in balancing the efficiencies of using less data while retaining key NLP capabilities.\n- Developing methods to produce high-quality, automatically generated data from PLMs as a way to improve NLP task performance could be a promising direction for both research and practical applications.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 57
  },
  {
    "id": "W4392340173",
    "x": 2023.8963197225642,
    "y": 17.68935355814067,
    "title": "Datasets for Large Language Models: A Comprehensive Survey",
    "authors": [
      "Yang Liu",
      "Jiahuan Cao",
      "Chongyu Liu"
    ],
    "first_author": "Yang Liu",
    "first_author_surname": "Liu",
    "year": 2024,
    "cited_by_count": 15,
    "venue": "",
    "size": 16.81813430024161,
    "color": "hsl(0, 70%, 60%)",
    "label": "Liu ,2024",
    "rag_problem": "There is a lack of comprehensive synthesis and standardized assessment across the multitude of datasets associated with Large Language Models (LLMs), making it difficult to understand their state and future trends.",
    "rag_method": "The survey conducted in the paper categorizes and summarizes LLM datasets into five dimensions: pre-training corpora, instruction fine-tuning datasets, preference datasets, evaluation datasets, and traditional NLP datasets.\n\n**Explanation:** By providing a structured overview and analysis of various types of LLM datasets, the paper offers a comprehensive reference point for researchers. This categorization helps in understanding the development trends and challenges of LLM datasets, guiding future research and dataset improvement efforts.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The survey acknowledges the extensive landscape of LLM-related datasets but notes a lack of cohesive synthesis across various types of datasets, which presents a challenge in understanding the current state and future trends.\n- While offering a comprehensive analysis, the survey identifies challenges that persist in the areas of pretraining, fine-tuning instruction, reinforcement learning, and model evaluation, indicating ongoing issues with dataset development in these domains.",
    "rag_future_work": "- Improve pre-training corpora by exploring diverse and inclusive sources to reduce biases and enhance model generalization.\n- Develop more comprehensive fine-tuning instruction datasets that capture a wider array of language tasks and user intents.\n- Create richer preference datasets that incorporate nuanced human feedback to refine model alignment with human values.\n- Enhance evaluation datasets to better measure model performance across varied linguistic and cultural contexts.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 206
  },
  {
    "id": "W4324373918",
    "x": 2023.163265226654,
    "y": -14.707037679040267,
    "title": "Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine",
    "authors": [
      "Stefan Harrer"
    ],
    "first_author": "Stefan Harrer",
    "first_author_surname": "Harrer",
    "year": 2023,
    "cited_by_count": 389,
    "venue": "",
    "size": 28.81808079379605,
    "color": "hsl(7, 70%, 60%)",
    "label": "Harrer ,2023",
    "rag_problem": "Âú®ÂåªÁñóÈ¢ÜÂüü‰∏≠‰ΩøÁî®Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂèØËÉΩÂØºËá¥‰º¶ÁêÜÈóÆÈ¢òÔºå‰æãÂ¶ÇÊÇ£ËÄÖÈöêÁßÅÊ≥ÑÈú≤Âíå‰ø°ÊÅØËØØÂØº„ÄÇ",
    "rag_method": "ËÆæËÆ°Ë¥üË¥£‰ªªÁöÑÊï∞ÊçÆ‰ΩøÁî®ÂçèËÆÆÂíåÈÄèÊòéÁöÑÊ®°ÂûãÂÜ≥Á≠ñÊú∫Âà∂Ôºå‰ª•‰øùÊä§ÊÇ£ËÄÖÈöêÁßÅÂπ∂Á°Æ‰øù‰ø°ÊÅØÁöÑÁúüÂÆûÊÄß„ÄÇ\n\n**Explanation:** ÈÄöËøáÂà∂ÂÆöÊòéÁ°ÆÁöÑÊï∞ÊçÆ‰ΩøÁî®ÂçèËÆÆÔºåÂèØ‰ª•Á°Æ‰øùÊ®°Âûã‰∏ç‰ºöÊª•Áî®ÊïèÊÑüÁöÑÂåªÁñóÊï∞ÊçÆ„ÄÇÂêåÊó∂ÔºåÈÄöËøáÈÄèÊòéÁöÑÊ®°ÂûãÂÜ≥Á≠ñÊú∫Âà∂ÔºåÂèØ‰ª•ËøΩË∏™ÂíåÈ™åËØÅÊ®°ÂûãÁöÑËæìÂá∫Ôºå‰ªéËÄåÂáèÂ∞ë‰ø°ÊÅØËØØÂØºÁöÑÈ£éÈô©„ÄÇËøô‰∫õÊé™ÊñΩÂèØ‰ª•ËÆ©Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®ÂåªÁñóÂ∫îÁî®‰∏≠Êó¢ÊúâÊïàÂèàÁ¨¶Âêà‰º¶ÁêÜËßÑËåÉ„ÄÇ",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4285199616",
    "x": 2022.5236372849824,
    "y": 3.5667514156644797,
    "title": "You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings",
    "authors": [
      "Zeerak Talat",
      "Aur√©lie N√©v√©ol",
      "Stella Biderman"
    ],
    "first_author": "Zeerak Talat",
    "first_author_surname": "Talat",
    "year": 2022,
    "cited_by_count": 56,
    "venue": "",
    "size": 21.591957881526618,
    "color": "hsl(14, 70%, 60%)",
    "label": "Talat ,2022",
    "rag_problem": "Evaluating bias in multilingual large language models (LLMs) is challenging due to ambiguous definitions of bias, cultural differences, and the reliance on Anglo-centric contexts that do not translate well across different languages and cultures.",
    "rag_method": "Develop multilingual bias evaluation frameworks that increase transparency through documentation, expand targets of bias beyond gender, and address cultural differences between languages.\n\n**Explanation:** By documenting the scope and demographic relevance of bias evaluation approaches, researchers can ensure that methods are transparent and applicable across different cultural contexts. Expanding the scope of evaluated biases beyond gender to include context-specific biases, and developing datasets that are culturally aware ensures that the evaluation frameworks are relevant and comprehensive. This approach addresses the issue of Anglo-centric bias by prioritizing inclusivity and cultural sensitivity.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method currently relies heavily on prestige forms of English, which may contribute to an over-emphasis on Western-centric social categories and potentially overlook biases in non-English languages.\n- There is a limited scope in addressing social harms, as our approach does not sufficiently incorporate diverse and non-Anglo-centric definitions of fairness and bias.\n- The lack of specific context within bias evaluation datasets raises concerns about the validity of measures developed for multilingual LLMs, which our method still struggles to address adequately.",
    "rag_future_work": "- Develop methods for multilingual LLMs that include comprehensive documentation of methodologies, speaker demographics, and annotator details to tailor bias evaluation to each language's specific context.\n- Broaden bias evaluation beyond gender, focusing on intersectional issues to account for more nuanced forms of bias in multilingual LLMs.\n- Consider the social and environmental impacts of developing LLMs, especially the potential harm versus benefit balance when creating language models for non-English languages.\n- Address structural inequalities in resources and representation by reducing reliance on English-centric datasets and methods, and incorporating diverse linguistic and cultural contexts in evaluating social biases in LLMs.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 30
  },
  {
    "id": "W4403637392",
    "x": 2023.7818442793484,
    "y": 25.217324826383315,
    "title": "How developments in natural language processing help us in understanding human behaviour",
    "authors": [
      "Rada Mihalcea",
      "Laura Biester",
      "Ryan L. Boyd"
    ],
    "first_author": "Rada Mihalcea",
    "first_author_surname": "Mihalcea",
    "year": 2024,
    "cited_by_count": 14,
    "venue": "",
    "size": 16.57562773035012,
    "color": "hsl(0, 70%, 60%)",
    "label": "Mihalcea ,2024",
    "rag_problem": "No clear research problem description found",
    "rag_method": "No clear method description found",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4402418067",
    "x": 2023.8065054728477,
    "y": 38.30153533457166,
    "title": "Governing with Intelligence: The Impact of Artificial Intelligence on Policy Development",
    "authors": [
      "Muhammad Asfand E Yar",
      "Mahani Hamdan",
      "Muhammad Anshari"
    ],
    "first_author": "Muhammad Asfand E Yar",
    "first_author_surname": "Yar",
    "year": 2024,
    "cited_by_count": 6,
    "venue": "",
    "size": 13.711850151610111,
    "color": "hsl(0, 70%, 60%)",
    "label": "Yar ,2024",
    "rag_problem": "The problem is the need for efficient and effective public policy development, which is often hindered by limited data analysis capabilities and subjective decision-making.",
    "rag_method": "Utilizing artificial intelligence (AI) to enhance data analysis and provide more objective insights for policy formulation.\n\n**Explanation:** AI technologies can process large volumes of data quickly and accurately, offering insights that might not be apparent through traditional analysis methods. By employing AI in policy development, decision-makers can rely on data-driven evidence to support policy choices, reducing reliance on subjective judgment and potentially increasing the efficiency and effectiveness of public policies.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The study relies heavily on secondary data and bibliographic reviews, which may limit the depth of analysis and the ability to obtain insights from real-time policy scenarios.\n- The qualitative nature of the research may introduce subjective interpretations, potentially affecting the objectivity and generalizability of the findings.\n- The impact of AI on policy development is explored primarily through existing literature and case studies, which might not fully capture the dynamic and rapidly evolving nature of AI applications.",
    "rag_future_work": "- Investigate the ethical implications and biases of AI applications in policy development to ensure fair and unbiased decision-making processes.\n- Develop frameworks and standardized guidelines for AI integration in public policy that address transparency, accountability, and public engagement.\n- Conduct longitudinal studies to assess the long-term impacts of AI-driven policy implementations on society and governance structures.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4394828653",
    "x": 2024.4601930349504,
    "y": 40.83772858519048,
    "title": "Artificial Intelligence for the Internal Democracy of Political Parties",
    "authors": [
      "Claudio Novelli",
      "Giuliano Formisano",
      "Prathm Juneja"
    ],
    "first_author": "Claudio Novelli",
    "first_author_surname": "Novelli",
    "year": 2024,
    "cited_by_count": 6,
    "venue": "",
    "size": 13.711850151610111,
    "color": "hsl(0, 70%, 60%)",
    "label": "Novelli ,2024",
    "rag_problem": "Current methodologies for measuring Intra-Party Democracy (IPD) rely on limited data sources, infrequent updates, and subjective interpretations, often leading to incomplete and unreliable data.",
    "rag_method": "The paper proposes leveraging Machine Learning (ML) techniques such as Natural Language Processing (NLP), sentiment analysis, predictive analytics, and anomaly detection to enhance the measurement of IPD.\n\n**Explanation:** ML techniques can extract data from unstructured sources like speeches and social media posts, increasing data availability. Sentiment analysis gauges public perception and internal party dynamics, providing a proxy for IPD measures. Predictive analytics can estimate missing data points using historical information, addressing data incompleteness. Anomaly detection identifies deviations in party practices, ensuring data reliability.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method faces significant challenges due to data availability and party unwillingness to share information, which hampers the effectiveness of machine learning applications in internal party democracy (IPD) analysis.\n- NLP and other machine learning techniques in our approach are susceptible to biases and inaccuracies, such as hallucinations in large language models, requiring specific technical and normative standards to mitigate these issues.",
    "rag_future_work": "- Addressing Data Availablility and Willingness: Future work should focus on overcoming challenges of data availability and incentivizing political parties to share information for better machine learning implementation in measuring IPD.\n- Reducing Biases in ML Techniques: Researchers need to develop specific technical and normative standards to mitigate biases and inaccuracies in NLP and large language models to ensure reliable and fair assessment of IPD.\n- Enhancing Real-Time Monitoring and Decision-Making: Future development should explore the integration of machine learning for real-time monitoring and data-driven decision-making to improve transparency and accuracy of political party operations.\n- Expanding Data Sources and Updates: Enhancing methodologies by diversifying data sources and increasing update frequency could improve the accuracy and reliability in measuring internal democratic processes within political parties.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 19
  },
  {
    "id": "W4393097350",
    "x": 2023.9958082979556,
    "y": 50.532614646980626,
    "title": "Exploring the role of uncertainty, emotions, and scientific discourse during the COVID-19 pandemic",
    "authors": [
      "Antoine Lemor",
      "√âric Montpetit"
    ],
    "first_author": "Antoine Lemor",
    "first_author_surname": "Lemor",
    "year": 2024,
    "cited_by_count": 4,
    "venue": "",
    "size": 12.447539682010172,
    "color": "hsl(0, 70%, 60%)",
    "label": "Lemor ,2024",
    "rag_problem": "Policymakers faced challenges in accurately understanding the role of uncertainty and emotions in shaping effective COVID-19 policies.",
    "rag_method": "The study developed indices using natural language processing (NLP) techniques to quantify sentiments of uncertainty, negative emotions, and the prevalence of scientific discourse.\n\n**Explanation:** By quantifying these elements through NLP, policymakers can better interpret the impact of uncertainty and emotional sentiment on policy decisions, guiding them towards more data-driven strategies that emphasize evidence-based actions. The indices provide a measurable basis to understand and react to these factors, thereby improving policy formulation.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4367397709",
    "x": 2022.519932904138,
    "y": 37.72831307424688,
    "title": "A Study of Ethical Issues in Natural Language Processing with Artificial Intelligence",
    "authors": [
      "Yongfeng Ma"
    ],
    "first_author": "Yongfeng Ma",
    "first_author_surname": "Ma",
    "year": 2023,
    "cited_by_count": 3,
    "venue": "",
    "size": 11.609067150120806,
    "color": "hsl(7, 70%, 60%)",
    "label": "Ma ,2023",
    "rag_problem": "The unpredictability of NLP models due to data uncertainty and model uncertainty.",
    "rag_method": "The TUNA framework, which quantifies uncertainty in NLP tasks by embedding uncertainties into task reliability and uses a probability-based approach to measure model uncertainty.\n\n**Explanation:** The TUNA framework allows NLP models to accurately assess their prediction reliability by quantifying both data and model uncertainties. By embedding uncertainties, it can better predict outcomes and reduce unpredictable performance by allowing users to understand the variabilities in predictions.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles with the unpredictability of NLP models due to the variability and constant change of text data, leading to potential deviations from desired task outcomes.\n- The approach faces challenges arising from biases and noise when training on large-scale datasets, which negatively affect the model's predictive power and accuracy.",
    "rag_future_work": "- Develop methods to ensure the privacy of personal data collected in NLP applications to prevent misuse and data leakage.\n- Create frameworks or guidelines that enhance the ethical practice of data collection in speech recognition and semantic analysis, ensuring compliance with privacy rights.\n- Investigate new techniques for anonymizing sensitive information in datasets used for NLP to protect individual identities and personal information.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 11
  },
  {
    "id": "W4238374879",
    "x": 2011.3236492526905,
    "y": 3.9204780774608246,
    "title": "Congressional Reforms",
    "authors": [
      "E. Scott Adler"
    ],
    "first_author": "E. Scott Adler",
    "first_author_surname": "Adler",
    "year": 2011,
    "cited_by_count": 7,
    "venue": "",
    "size": 14.213600725181209,
    "color": "hsl(92, 70%, 60%)",
    "label": "Adler ,2011",
    "rag_problem": "No clear research problem description found",
    "rag_method": "No clear method description found",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2117002298",
    "x": 2006.1463800946776,
    "y": 0.20165747650922405,
    "title": "Whose Deaths Matter? Mortality, Advocacy, and Attention to Disease in the Mass Media",
    "authors": [
      "Elizabeth Armstrong",
      "Daniel Carpenter",
      "Marie Hojnacki"
    ],
    "first_author": "Elizabeth Armstrong",
    "first_author_surname": "Armstrong",
    "year": 2006,
    "cited_by_count": 75,
    "venue": "",
    "size": 22.672936983307476,
    "color": "hsl(127, 70%, 60%)",
    "label": "Armstrong ,2006",
    "rag_problem": "Â™í‰ΩìÂØπ‰∏çÂêåÁñæÁóÖÁöÑÂÖ≥Ê≥®Á®ãÂ∫¶Â≠òÂú®ÊòéÊòæÂ∑ÆÂºÇÔºåËøôÁßçÂ∑ÆÂºÇÂèØËÉΩ‰∏éÂÆûÈôÖÊ≠ª‰∫°ÁéáÂèäÁñæÁóÖÁöÑÈáçË¶ÅÊÄß‰∏ç‰∏ÄËá¥„ÄÇ",
    "rag_method": "‰ΩøÁî®Êù•Ëá™19Âπ¥Èó¥ÂÖ≥‰∫é‰∏ÉÁßçÁñæÁóÖÁöÑÁã¨ÁâπÊï∞ÊçÆÈõÜÔºåÂàÜÊûêÂ™í‰ΩìÂÖ≥Ê≥®‰∏éÁñæÁóÖÊ≠ª‰∫°ÁéáÂèäÁªÑÁªáÂÆ£‰º†Ê¥ªÂä®‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ\n\n**Explanation:** ÈÄöËøáÂàÜÊûêÊ®™Ë∑®Â§öÁñæÁóÖÂíåÂ§öÊó∂Èó¥ÁÇπÁöÑÊï∞ÊçÆÔºåÁ†îÁ©∂Êè≠Á§∫‰∫ÜÂì™‰∫õÂõ†Á¥†‰øÉËøõÊàñÈòªÁ¢çÂ™í‰ΩìÂØπÁâπÂÆöÁñæÁóÖÁöÑÂÖ≥Ê≥®„ÄÇËøôÁßçÂàÜÊûêÊúâÂä©‰∫éÁêÜËß£Ê≠ª‰∫°Áéá‰∏éÂ™í‰ΩìÂÖ≥Ê≥®ÁöÑÂÖ≥Á≥ªÔºå‰ª•Âèä advocacy ÂíåÁªÑÁªáÊ¥ªÂä®Â¶Ç‰ΩïÂΩ±ÂìçÂ™í‰ΩìÊä•ÈÅìÔºå‰ªéËÄå‰∏∫Ë∞ÉÊï¥Â™í‰ΩìÊä•ÈÅìÁ≠ñÁï•Êèê‰æõÊï∞ÊçÆÊîØÊåÅ„ÄÇ",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2251172991",
    "x": 2013.2417273330097,
    "y": 5.72555645406967,
    "title": "The New Eye of Government: Citizen Sentiment Analysis in Social Media",
    "authors": [
      "R. Arunachalam",
      "Sandipan Sarkar"
    ],
    "first_author": "R. Arunachalam",
    "first_author_surname": "Arunachalam",
    "year": 2013,
    "cited_by_count": 38,
    "venue": "",
    "size": 20.166007536725477,
    "color": "hsl(78, 70%, 60%)",
    "label": "Arunachalam ,2013",
    "rag_problem": "Governments struggle to achieve transparency and engagement with citizens.",
    "rag_method": "Implementing a sentiment analysis approach on social media to monitor and analyze citizen sentiment.\n\n**Explanation:** By using sentiment analysis on social media, governments can systematically understand public emotions and opinions, allowing them to adjust policies and communication strategies effectively. This approach helps bridge the gap between government and citizens by providing a direct line to real-time public sentiment, facilitating greater transparency in understanding citizen needs and promoting engagement via responsive governance.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W1889043906",
    "x": 1980.1122265522004,
    "y": -0.014537119152892686,
    "title": "The Conflict and Peace Data Bank (COPDAB) Project",
    "authors": [
      "Edward E. Azar"
    ],
    "first_author": "Edward E. Azar",
    "first_author_surname": "Azar",
    "year": 1980,
    "cited_by_count": 300,
    "venue": "",
    "size": 27.844738892398624,
    "color": "hsl(240, 70%, 60%)",
    "label": "Azar ,1980",
    "rag_problem": "Lack of systematic observation and analysis of events leading to war and peace.",
    "rag_method": "Development of the Conflict and Peace Data Bank (COPDAB) to systematize observation and improve analysis skills.\n\n**Explanation:** The COPDAB project provides a structured database to record and analyze political events, allowing researchers to identify patterns and relationships that lead to conflict or peace. By centralizing and organizing data, it offers a comprehensive resource that enhances the understanding and forecasting of international relations trends.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Develop improved methodologies for systematizing observations of international events, which would enhance the analysis of factors contributing to war and peace.\n- Expand the scope of data collection to include more variables related to colonialism reduction and quality of life improvement, enabling more comprehensive research on global integration and equitable interdependence.\n- Explore advanced theories that can better predict the outcomes of international tensions, aiming for more precise interventions to promote peace and stability.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2099921486",
    "x": 2012.2394061701223,
    "y": -2.97450199867347,
    "title": "Measuring party positions in Europe",
    "authors": [
      "Ryan Bakker",
      "Catherine E. De Vries",
      "Erica Edwards"
    ],
    "first_author": "Ryan Bakker",
    "first_author_surname": "Bakker",
    "year": 2012,
    "cited_by_count": 828,
    "venue": "",
    "size": 39.564434503845234,
    "color": "hsl(85, 70%, 60%)",
    "label": "Bakker ,2012",
    "rag_problem": "Difficulty in accurately measuring and tracking national party positions on European integration and other policies over time.",
    "rag_method": "The introduction of the CHES trend file, which compiles expert survey data on party positions from 1999 to 2010.\n\n**Explanation:** The CHES trend file provides a standardized and comprehensive dataset that allows researchers to track changes in party positions across European countries over a decade. By using expert surveys, the dataset ensures reliability and captures nuances of party ideologies and policies, thereby facilitating a clearer understanding of political trends within the EU.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Develop methods to improve the reliability and validity of expert judgment in measuring party positions across European countries, ensuring consistency across different surveys and datasets.\n- Investigate the influence of new EU policies on national party positions to determine if emerging political themes are adequately captured in current measurement frameworks.\n- Expand the temporal scope of data collection beyond 1999‚àí2010 to track how party positions have evolved in recent years, providing updated insights into political trends and their implications for European integration.\n- Enhance cross-validation efforts using a broader set of comparative data sources, such as social media analysis and public opinion polls, to corroborate expert surveys and manifesto data.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4388691863",
    "x": 2022.8875449730858,
    "y": 13.789622952485256,
    "title": "Language Model Behavior: A Comprehensive Survey",
    "authors": [
      "Tyler A. Chang",
      "Benjamin Bergen"
    ],
    "first_author": "Tyler A. Chang",
    "first_author_surname": "Chang",
    "year": 2023,
    "cited_by_count": 68,
    "venue": "",
    "size": 22.309857041756352,
    "color": "hsl(7, 70%, 60%)",
    "label": "Chang ,2023",
    "rag_problem": "Large language models often generate unfactual, commonsense errors, memorized text, and socially biased outputs.",
    "rag_method": "Identifying under-generalization and over-generalization in pattern recognition during pre-training as root causes.\n\n**Explanation:** Through text pattern generalization analysis, the authors provide a framework to understand how language models exhibit such weaknesses by learning patterns that either are not sufficiently generalized or by over-generalizing specific patterns. Understanding these errors through the lens of generalization helps in identifying the role of memorization versus effective rule adoption and can direct future model adjustments to reduce these biases and errors.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our survey highlights that language models still remain sensitive to specific inputs and surface features even at large scales, which can lead to incorrect language behavior.\n- The comprehensive study points out that many strengths and weaknesses of language models can be attributed to their ability (or inability) to generalize text patterns accurately.\n- Despite covering extensive research, the survey indicates that there is still a gap in understanding the complete mechanistic analysis of language model internals related to behavioral outcomes.",
    "rag_future_work": "- Investigate methods to reduce sensitivity to specific inputs and surface features in large language models. This could involve developing techniques for better generalization of text patterns.\n- Explore effective regulations and deployment strategies for large language models. Future research could focus on ethical and practical frameworks to ensure responsible use of these models.\n- Conduct in-depth analysis on the generalizations made by language models in different contexts. This would help refine understanding of when models correctly or incorrectly generalize text patterns.\n- Study the scalability and performance of language models beyond hundreds of billions of parameters. Research could aim to uncover relationships between model size and capability while addressing current limitations in text pattern handling.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 43
  },
  {
    "id": "W4310576793",
    "x": 2021.8889992024622,
    "y": 0.9175521619106454,
    "title": "Politics as Usual? Measuring Populism, Nationalism, and Authoritarianism in U.S. Presidential Campaigns (1952‚Äì2020) with Neural Language Models",
    "authors": [
      "Bart Bonikowski",
      "Yuchen Luo",
      "Oscar Stuhler"
    ],
    "first_author": "Bart Bonikowski",
    "first_author_surname": "Bonikowski",
    "year": 2022,
    "cited_by_count": 60,
    "venue": "",
    "size": 21.846804520546883,
    "color": "hsl(14, 70%, 60%)",
    "label": "Bonikowski ,2022",
    "rag_problem": "There is a lack of quantifiable measurement of the usage of radical-right discursive elements like populism, nationalism, and authoritarianism by mainstream U.S. presidential campaigns prior to their visible exploitation in recent times.",
    "rag_method": "The study employs neural language models to analyze historical campaign speeches and documents to quantify the presence of populism, nationalism, and authoritarianism from 1952 to 2020.\n\n**Explanation:** By using advanced neural language models, the study can objectively identify and measure specific linguistic patterns and frames associated with populism, nationalism, and authoritarianism across a wide dataset of presidential campaign materials spanning several decades. This allows for a systematic and data-driven analysis of these discursive elements in mainstream political rhetoric, rather than relying on anecdotal evidence or manual analysis which can be subjective and prone to biases.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Future research could explore the diffusion and impact of radical-right frames on centrist parties beyond the United States to understand their global influence.\n- Further studies might investigate the historical use and evolution of populist, nationalist, and authoritarian rhetoric by mainstream political actors before their adoption by radical-right movements.\n- The application of neural language models to other political campaign data could be expanded to assess their utility in analyzing political discourse across varied contexts and time periods.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4393305539",
    "x": 2023.4782306967415,
    "y": -7.159231547784873,
    "title": "ChatGPT vs SBST: A Comparative Assessment of Unit Test Suite Generation",
    "authors": [
      "Yutian Tang",
      "Zhijie Liu",
      "Zhichao Zhou"
    ],
    "first_author": "Yutian Tang",
    "first_author_surname": "Tang",
    "year": 2024,
    "cited_by_count": 44,
    "venue": "",
    "size": 20.70371577869007,
    "color": "hsl(0, 70%, 60%)",
    "label": "Tang ,2024",
    "rag_problem": "The challenge of generating unit test suites that are effective in bug detection and code coverage, with current methods requiring learning costs for testers and potentially missing complex test cases.",
    "rag_method": "Comparative use of ChatGPT, an LLM model, to assess its effectiveness in generating unit test suites alongside the SBST tool EvoSuite.\n\n**Explanation:** ChatGPT offers a zero learning cost approach by leveraging its ability to understand human-like text and generate code, including test cases. By comparing its output against EvoSuite, insights are obtained on the strengths and limitations. Although ChatGPT performs well in readability and providing initial testing solutions for newcomers, EvoSuite's integration of genetic algorithms usually leads to higher code coverage and bug detection efficacy due to its feedback mechanisms and optimization nature.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The study is constrained by the need for manual querying of ChatGPT, limiting it to the specific queries made for the study without access to the internal model details or training data specifics.\n- The continual updates to ChatGPT mean that the results of the study only reflect the performance of ChatGPT at the time of the experiment, specifically the ChatGPT Jan 30 (2023) version, limiting the study's applicability over time.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 26
  },
  {
    "id": "W4385570444",
    "x": 2023.1969193058803,
    "y": 24.370798329897937,
    "title": "Nonparametric Masked Language Modeling",
    "authors": [
      "Sewon Min",
      "Weijia Shi",
      "Michael Lewis"
    ],
    "first_author": "Sewon Min",
    "first_author_surname": "Min",
    "year": 2023,
    "cited_by_count": 16,
    "venue": "",
    "size": 17.045934206848298,
    "color": "hsl(7, 70%, 60%)",
    "label": "Min ,2023",
    "rag_problem": "Existing language models use a softmax over a finite vocabulary, limiting their ability to predict rare tokens or phrases.",
    "rag_method": "NPM model uses a nonparametric distribution over every phrase in a reference corpus to predict tokens without relying on a fixed vocabulary softmax.\n\n**Explanation:** By eliminating the softmax layer, NPM can directly retrieve phrases from a large reference corpus, allowing it to model and predict rare tokens and phrases more effectively than models that depend on a fixed and finite vocabulary. The NPM utilizes an encoder to map text into a vector space, from which it can then retrieve the most relevant phrase, facilitating predictions of rare patterns and nearly unseen words.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The size of the reference corpus used in our method is smaller than the training data of very large language models, limiting the model's scalability.\n- Our method experiences slower inference due to the search process, though it has potential for speed improvements through better engineering or indexing.",
    "rag_future_work": "- Extend Nonparametric Masked Language Modeling (NPM) to few-shot learning and fine-tuning, as NPM may offer easier fine-tuning compared to larger models, warranting exploration in future studies.\n- Investigate the use of NPM for autoregressive generation since its current design limits it to prediction; methods similar to those in existing literature could be utilized for such extensions.\n- Develop and evaluate multilingual versions of NPM for improved cross-lingual transfer, which can potentially benefit from nonparametric training by reducing the need for extensive multilingual data collection.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 43
  },
  {
    "id": "W2933138175",
    "x": 2019.3854345916507,
    "y": -11.862961451646912,
    "title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
    "authors": [
      "Myle Ott",
      "Sergey Edunov",
      "Alexei Baevski"
    ],
    "first_author": "Myle Ott",
    "first_author_surname": "Ott",
    "year": 2019,
    "cited_by_count": 2464,
    "venue": "",
    "size": 44.68281062209197,
    "color": "hsl(35, 70%, 60%)",
    "label": "Ott ,2019",
    "rag_problem": "Existing sequence modeling toolkits lack integration of essential features like fast execution, extensibility, and support for state-of-the-art methods, which are necessary for both research and production environments.",
    "rag_method": "The development of FAIRSEQ, a toolkit that combines fast execution, extensibility, distributed training, and supports mixed precision training.\n\n**Explanation:** FAIRSEQ addresses the deficiencies of current toolkits by providing a cohesive platform that incorporates a common interface for models, efficient distributed and mixed precision training, and state-of-the-art implementations for various tasks. This enables users to build and extend models quickly and efficiently with support for extensive research applications and production-level deployment. Its compatibility with modern hardware accelerates training processes while maintaining accuracy through advanced techniques like dynamic loss scaling.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Further develop the FAIRSEQ toolkit to enhance its capabilities, allowing it to support a wider range of sequence modeling applications.\n- Incorporate new research advances into the toolkit to keep it at the forefront of sequence modeling technology and methodologies.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 14
  },
  {
    "id": "W2949433733",
    "x": 2014.5875343994278,
    "y": -1.2997936448065945,
    "title": "Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books",
    "authors": [
      "Yukun Zhu",
      "Ryan Kiros",
      "Richard S. Zemel"
    ],
    "first_author": "Yukun Zhu",
    "first_author_surname": "Zhu",
    "year": 2015,
    "cited_by_count": 295,
    "venue": "",
    "size": 27.781796923673223,
    "color": "hsl(64, 70%, 60%)",
    "label": "Zhu ,2015",
    "rag_problem": "Visual content in movies lacks rich, descriptive explanations that go semantically far beyond simplistic captions available in current datasets.",
    "rag_method": "Align books with their movie releases using a neural sentence embedding trained on a large corpus of books and video-text neural embeddings for computing similarities between movie clips and sentences in the book.\n\n**Explanation:** Books provide detailed descriptions about the intentions and mental states of the characters, while movies capture visual aspects of settings. By aligning these sources, the model can infer rich, descriptive explanations for movie scenes based on book paragraphs, bridging the gap between simple visual captions and complex semantic narratives.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to capture the emotional depth and subtleties present in intricate storylines due to complex cinematic and literary elements.\n- There are challenges in accurately aligning visual cues from movies with narrative elements from books, impacting the coherence of the story-like visual explanations.",
    "rag_future_work": "- Investigate improved methods for enhancing sentence and visual embedding techniques to increase alignment accuracy between books and their movie adaptations. This could involve exploring new neural network architectures or training paradigms to better capture the nuances of storytelling in both mediums.\n- Develop a more comprehensive dataset encompassing a broader range of book and movie genres to facilitate the generalization of alignment methods. This would help ensure that the proposed models are robust and applicable across diverse narrative styles and themes.\n- Explore the integration of additional contextual information, like audio cues and emotional tone, into the alignment model to augment the understanding of the narrative's progression and provide richer, story-like visual explanations.\n- Address current limitations in the model's ability to handle highly metaphorical or abstract text by incorporating techniques from natural language processing that are focused on semantic understanding and interpretation of figurative language.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 25
  },
  {
    "id": "W4253067820",
    "x": 2006.4161765557164,
    "y": -0.014179317446060602,
    "title": "The third PASCAL recognizing textual entailment challenge",
    "authors": [
      "Danilo Giampiccolo",
      "Bernardo Magnini",
      "Ido Dagan"
    ],
    "first_author": "Danilo Giampiccolo",
    "first_author_surname": "Giampiccolo",
    "year": 2007,
    "cited_by_count": 357,
    "venue": "",
    "size": 28.496383036511926,
    "color": "hsl(120, 70%, 60%)",
    "label": "Giampiccolo ,2007",
    "rag_problem": "Recognizing textual entailment scenarios are often simplified and may not accurately reflect real-world complexity, limiting the challenge's applicability to practical situations.",
    "rag_method": "The dataset was enhanced with longer texts and more complex scenarios to better mimic realistic use cases, and a pool of shared resources was introduced for participants.\n\n**Explanation:** By incorporating longer texts and more realistic scenarios, the challenge now includes a more accurate representation of the complexity encountered in real-world textual entailment tasks. This prepares systems better for practical applications. The shared resources facilitate a common ground for participant solutions, promoting consistency and comparability across different systems.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method faces challenges in handling longer texts introduced in this year's dataset, which aim to mirror more realistic scenarios.\n- There is a limitation in distinguishing unknown entailments from identified contradictions, as noted in the set up of the pilot task.",
    "rag_future_work": "- Explore the development of improved systems that can handle even more complex and longer texts for textual entailment to enhance the applicability in realistic scenarios.\n- Investigate additional shared resources and tools that could be offered to participants to facilitate standardized comparisons and encourage collaborative growth in textual entailment methodologies.\n- Extend the pilot task focused on differentiating unknown entailments to enhance the understanding of nuanced semantic relationships in challenging scenarios.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4396743843",
    "x": 2024.0773095704606,
    "y": 30.741341916541813,
    "title": "LGBTQ+ and Feminist Digital Activism",
    "authors": [
      "Angela Zottola"
    ],
    "first_author": "Angela Zottola",
    "first_author_surname": "Zottola",
    "year": 2024,
    "cited_by_count": 11,
    "venue": "",
    "size": 15.737155198460755,
    "color": "hsl(0, 70%, 60%)",
    "label": "Zottola ,2024",
    "rag_problem": "The challenges of effectively promoting LGBTQ+ and feminist causes in digital spaces due to lack of structured frameworks.",
    "rag_method": "Introduction of the concept 'LGBTQ+ Digital Activism' alongside 'Feminist Digital Activism' to create a structured framework for linguistic and discursive practices on social media.\n\n**Explanation:** By establishing 'LGBTQ+ Digital Activism' as a recognized term, the paper provides activists a framework to strategize and organize their digital efforts more effectively. This structured approach helps in unifying disparate efforts and enhancing visibility and coherence in promoting gender identity and sexuality issues online.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4392773774",
    "x": 2024.0600034088739,
    "y": 35.93703690190922,
    "title": "Legal Categorization of 'Transgender'",
    "authors": [
      "Kimberly Tao"
    ],
    "first_author": "Kimberly Tao",
    "first_author_surname": "Tao",
    "year": 2024,
    "cited_by_count": 7,
    "venue": "",
    "size": 14.213600725181209,
    "color": "hsl(0, 70%, 60%)",
    "label": "Tao ,2024",
    "rag_problem": "Courts face difficulties in interpreting and categorizing terms such as 'sex', 'man', and 'woman', especially in relation to the legal categorization of transgender individuals. This creates challenges in giving clear binary legal answers.",
    "rag_method": "The paper analyzes the foundational legal reasoning frameworks used by courts to interpret these terms, suggesting an approach that more accurately reflects the complexities of gender identity in legal contexts.\n\n**Explanation:** By examining the foundational frames of legal reasoning, the paper provides a structured method to better interpret the terms 'sex', 'man', and 'woman' in a way that acknowledges transgender identities. This aids judges in making more informed decisions that consider the nuances and realities of gender identity, thereby providing clearer and more appropriate legal outcomes.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Further research could investigate how varying legal contexts might influence the interpretation of terms related to gender identity and how this potentially affects the legal categorization of transgender individuals.\n- Future studies could explore the impact of cultural and regional differences on the legal framing of transgender issues, specifically analyzing how these factors affect judicial decisions.\n- Developing interdisciplinary frameworks that integrate legal, social, and linguistic perspectives might offer more comprehensive solutions to the challenges of legally categorizing transgender identities.\n- There is a need to analyze how emerging legal definitions influence policy-making and the rights of transgender individuals across different jurisdictions.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4406003524",
    "x": 2024.1716428682455,
    "y": 42.86307376875409,
    "title": "Feminism, Corpus-assisted Research and Language Inclusivity",
    "authors": [
      "Federica Formato"
    ],
    "first_author": "Federica Formato",
    "first_author_surname": "Formato",
    "year": 2024,
    "cited_by_count": 6,
    "venue": "",
    "size": 13.711850151610111,
    "color": "hsl(0, 70%, 60%)",
    "label": "Formato ,2024",
    "rag_problem": "Grammatical gender inflections in Italian language reinforce linguistic and social binarism, which can contribute to discrimination against LGBTQIA communities.",
    "rag_method": "The gender inclusive strategy of using schwa is proposed to address these inflections.\n\n**Explanation:** By adopting the schwa in place of traditional feminine and masculine gender markers, language can be made more inclusive and neutral. This reduces the reliance on binary gender distinctions and supports gender inclusivity, counteracting the social and institutional discrimination faced by LGBTQIA communities.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The study utilizes the schwa in Italian and focuses specifically on its application in tweets, potentially limiting generalizability across different mediums or forms of communication.\n- The context of the investigation is confined to Italy, where specific socio-political issues regarding LGBTQIA communities are present, which may not fully represent the global applicability of the findings due to differing cultural contexts.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4406135761",
    "x": 2025.56018289855,
    "y": 5.6986648406436835,
    "title": "Queering Language Revitalisation",
    "authors": [
      "John Walsh",
      "Michael Hornsby",
      "Eva Juarros Dauss√†"
    ],
    "first_author": "John Walsh",
    "first_author_surname": "Walsh",
    "year": 2025,
    "cited_by_count": 6,
    "venue": "",
    "size": 13.711850151610111,
    "color": "hsl(0, 70%, 60%)",
    "label": "Walsh ,2025",
    "rag_problem": "Áº∫‰πèÂÖ≥‰∫éÊñáÂåñ/ËØ≠Ë®ÄË∫´‰ªΩ‰∏éLGBTQ+Ë∫´‰ªΩ‰∫§Ê±áÂ§ÑÁöÑÁ†îÁ©∂ÔºåÁâπÂà´ÊòØÂú®ËØ≠Ë®ÄÂ§çÂÖ¥‰∏≠ÁöÑÂÖ∑‰ΩìÂΩ±Âìç„ÄÇ",
    "rag_method": "ÈááÁî®ÈÖ∑ÂÑøÁêÜËÆ∫ËßÜËßíÊù•ÂàÜÊûêËØ≠Ë®ÄÂ§çÂÖ¥‰∏éLGBTQ+Ë∫´‰ªΩÁöÑÂÖ≥Á≥ªÔºåÂπ∂Êé¢Á¥¢Ê≠§ËßÜËßíÂ¶Ç‰ΩïÊè≠Á§∫Êñ∞ÁöÑË∫´‰ªΩÊûÑÂª∫ÊñπÂºè„ÄÇ\n\n**Explanation:** ÈÖ∑ÂÑøÁêÜËÆ∫Êèê‰æõ‰∫Ü‰∏Ä‰∏™Ê°ÜÊû∂ÔºåËÉΩÂ§üÊâìÁ†¥‰º†Áªü‰∫åÂÖÉÊÄßÂπ∂Ë¥®ËØ¢Â∏∏ËßÑÁöÑË∫´‰ªΩÁ±ªÂà´ÔºåÈÄöËøáËøô‰∏™ËßÜËßíÔºåÂèØ‰ª•Êõ¥Ê∑±ÂÖ•ÁêÜËß£LGBTQ+‰∏™‰ΩìÂú®ËØ≠Ë®ÄÂ§çÂÖ¥‰∏≠ÁöÑÁã¨ÁâπÁªèÂéÜÂíåË¥°ÁåÆ„ÄÇÊ≠§ÁêÜËÆ∫Â∏ÆÂä©ËØÜÂà´Âíå‰∏∞ÂØåËØ≠Ë®Ä‰∏éË∫´‰ªΩÁöÑ‰∫§ÂèâÂä®ÊÄÅÔºåÂ°´Ë°•ÂΩìÂâçÁ†îÁ©∂ÁöÑÁ©∫ÁôΩ„ÄÇ",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2002089036",
    "x": 1995.5660963519076,
    "y": 0.3659874945876884,
    "title": "IS THERE A QUEER PEDAGOGY? OR, STOP READING STRAIGHT",
    "authors": [
      "Deborah P. Britzman"
    ],
    "first_author": "Deborah P. Britzman",
    "first_author_surname": "Britzman",
    "year": 1995,
    "cited_by_count": 755,
    "venue": "",
    "size": 39.13147680843845,
    "color": "hsl(205, 70%, 60%)",
    "label": "Britzman ,1995",
    "rag_problem": "Traditional pedagogies often operate within heteronormative frameworks, which can exclude or marginalize LGBTQ+ perspectives.",
    "rag_method": "Developing 'queer pedagogy', which challenges and reconstructs teaching practices to be inclusive of diverse sexual orientations and gender identities.\n\n**Explanation:** Queer pedagogy aims to dismantle heteronormative assumptions within educational systems and curricula, fostering inclusivity by encouraging educators to engage with and incorporate LGBTQ+ perspectives. This approach can transform learning environments into spaces that validate and represent varied identities, thus addressing the marginalization experienced in traditional settings.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W2060908944",
    "x": 2005.541895217043,
    "y": 4.4462074363123225,
    "title": "Queer (<i>v</i>.) Pedagogy",
    "authors": [
      "G. D. Shlasko"
    ],
    "first_author": "G. D. Shlasko",
    "first_author_surname": "Shlasko",
    "year": 2005,
    "cited_by_count": 75,
    "venue": "",
    "size": 22.672936983307476,
    "color": "hsl(134, 70%, 60%)",
    "label": "Shlasko ,2005",
    "rag_problem": "Traditional pedagogy often fails to adequately address or incorporate diverse sexual identities and orientations into teaching practices, leading to a lack of representation and inclusivity for LGBTQ+ students.",
    "rag_method": "Queer pedagogy, grounded in queer theory, aims to challenge normative structures and practices within education by integrating queer perspectives and embracing diversity in sexual identities.\n\n**Explanation:** Queer pedagogy provides educators with tools to critically analyze and dismantle heteronormative assumptions, enabling a more inclusive environment that values LGBTQ+ experiences. By incorporating queer theory, educators question and reframe traditional curricula to reflect a broader spectrum of identities and experiences, supporting a more comprehensive and inclusive approach to teaching and learning.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Explore the integration of queer pedagogy into diverse educational settings to assess its adaptability and impact on various learning environments.\n- Conduct empirical studies to evaluate the effectiveness of queer pedagogy in enhancing inclusivity and student engagement in classrooms.\n- Develop comprehensive training programs for educators that focus on implementing queer pedagogical techniques and measure their outcomes.\n- Investigate the intersection of queer pedagogy with other pedagogical approaches to understand its role and potential synergies in education.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2036942465",
    "x": 2013.1161059509034,
    "y": 8.600919272216885,
    "title": "Introduction: Gender, language and translation at the crossroads of disciplines",
    "authors": [
      "Olga Castro"
    ],
    "first_author": "Olga Castro",
    "first_author_surname": "Castro",
    "year": 2013,
    "cited_by_count": 27,
    "venue": "",
    "size": 18.920917301730913,
    "color": "hsl(78, 70%, 60%)",
    "label": "Castro ,2013",
    "rag_problem": "Lack of understanding of how gender roles are discursively constructed through language and translation.",
    "rag_method": "Integration of insights from feminist linguistics and feminist translation studies to analyze the role of language and translation in the construction of gender roles.\n\n**Explanation:** By combining feminist linguistics with feminist translation studies, the paper aims to highlight the discursive strategies used in language and translation that contribute to the construction of gender roles. This integrated approach allows for a deeper examination of gender as an omni-relevant category across social practices, providing a comprehensive understanding of its construction via linguistic and translational means.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W49981513",
    "x": 2013.728683160682,
    "y": -2.7834439874813537,
    "title": "A Handbook for Teaching and Learning in Higher Education",
    "authors": [
      "Heather Fry",
      "Steve Ketteridge",
      "Stephanie Marshall"
    ],
    "first_author": "Heather Fry",
    "first_author_surname": "Fry",
    "year": 2014,
    "cited_by_count": 362,
    "venue": "",
    "size": 28.548499651072742,
    "color": "hsl(71, 70%, 60%)",
    "label": "Fry ,2014",
    "rag_problem": "In higher education, students often struggle to effectively reflect on their experiences and integrate new knowledge into logical theories, which impacts their overall learning process.",
    "rag_method": "The Kolb Learning Cycle, particularly the reflection (RO) and conceptualisation (AC) phases, provides a structured method for students to reflect on experiences and integrate new ideas into coherent theories.\n\n**Explanation:** The Kolb Learning Cycle encourages students to actively reflect on their experiences from multiple perspectives (RO) and to develop and process these reflections into sound theories (AC). This cycle provides time and space for reflection, allowing students to take ownership of their learning and improve their ability to connect theory with practice, thereby enhancing the overall effectiveness of the learning process.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W2885484425",
    "x": 2017.458827801373,
    "y": 5.7268073718180315,
    "title": "Queer Theory and Biomedical Practice: The Biomedicalization of Sexuality/The Cultural Politics of Biomedicine",
    "authors": [
      "William J. Spurlin"
    ],
    "first_author": "William J. Spurlin",
    "first_author_surname": "Spurlin",
    "year": 2018,
    "cited_by_count": 42,
    "venue": "",
    "size": 20.532888740788515,
    "color": "hsl(42, 70%, 60%)",
    "label": "Spurlin ,2018",
    "rag_problem": "Biomedical practices historically conflated sexual orientation and gender identity with pathology, leading to homophobic and transphobic assumptions and discrimination against LGBTQI individuals.",
    "rag_method": "Integration of queer theory in biomedical practice to critically analyze and deconstruct the biases in medical discourses and diagnostic categories, while promoting inclusive and human-rights based approaches to gender and sexual health.\n\n**Explanation:** Queer theory provides a framework for understanding how social and cultural biases can infiltrate biomedical practices, enabling a critical reassessment of these practices to ensure they do not perpetuate discrimination. By challenging the normative biases embedded within biomedical knowledge, queer theory helps uncover the socio-cultural influences that lead to stigmatization, allowing for a more inclusive approach that recognizes gender and sexual rights as human rights.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The paper acknowledges the lingering stigmas and biases within healthcare systems and clinical practices that adversely affect the quality of care LGBT patients receive, suggesting that the method may not fully address or overcome these entrenched societal and cultural issues.\n- The approach highlights contradictions between ethical biomedical practices and cultural influences, implying that there is still a need for deeper analysis to bridge these gaps and ensure that good health is not conflated with conformity to gender and sexual norms.\n- There is an emphasis on the historical pathologization and stigmatization of non-conforming genders and sexualities, but the method may not sufficiently provide new frameworks or practical solutions for fostering acceptance and understanding in biomedical contexts.\n- While the paper critiques historical and ongoing biases, it may lack concrete strategies for altering deeply embedded cultural perspectives on gender and sexuality, which are crucial for improving psychosocial and health outcomes for LGBTQI individuals.",
    "rag_future_work": "- Develop new conceptual frameworks to better understand the psychological growth of children who cross-gender identify, incorporating diverse gender expressions without pathologizing nonconformity, and evaluate how these frameworks can be integrated into clinical practice.\n- Investigate the impact of implicit and explicit biases in medical and clinical settings on the quality of care provided to LGBT patients, and develop strategies to reduce discrimination and improve patient-provider communication.\n- Explore the implications of structural competency in medical education, particularly in relation to training healthcare professionals on LGBT health issues, to address the gap in medical school curricula and reduce remnant homophobia.\n- Conduct critical analyses of healthcare policies and practices that link good health with conformity to gender and sexual norms, with a focus on developing ethical biomedical practices that align with human rights principles.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 4
  },
  {
    "id": "W4403233717",
    "x": 2023.9378609063763,
    "y": 58.40033359109521,
    "title": "Efficient Conceptual Knowledge Removal in Large Language Models: Methods and Evaluations",
    "authors": [
      "Miyim Dimitriou",
      "Daniel Rogowski",
      "Michael C. Anderson"
    ],
    "first_author": "Miyim Dimitriou",
    "first_author_surname": "Dimitriou",
    "year": 2024,
    "cited_by_count": 3,
    "venue": "",
    "size": 11.609067150120806,
    "color": "hsl(0, 70%, 60%)",
    "label": "Dimitriou ,2024",
    "rag_problem": "Deep neural networks in large language models retain outdated or biased conceptual knowledge from their training data, which often needs to be selectively removed. Current methods for removing such knowledge are computationally expensive and could degrade model performance.",
    "rag_method": "The paper introduces a scalable framework for conceptual knowledge removal that efficiently erases specific knowledge without requiring costly retraining and while maintaining the overall model performance.\n\n**Explanation:** The proposed framework directly targets and removes undesired conceptual knowledge within the model's learned parameters. By doing so, it avoids the complete retraining of the model which is typically computationally expensive. This approach ensures that the model's overall capabilities, unrelated to the erased knowledge, remain intact and perform efficiently, thereby addressing the core issue of efficiency and precision in knowledge management.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4412664660",
    "x": 2024.3032230835602,
    "y": 61.17429551341611,
    "title": "Large Language Models with Novel Token Processing Architecture: A Study of the Dynamic Sequential Transformer",
    "authors": [
      "Kusi Men",
      "Na Pin",
      "Shaohua Lu"
    ],
    "first_author": "Kusi Men",
    "first_author_surname": "Men",
    "year": 2024,
    "cited_by_count": 3,
    "venue": "",
    "size": 11.609067150120806,
    "color": "hsl(0, 70%, 60%)",
    "label": "Men ,2024",
    "rag_problem": "Large language models face significant challenges related to computational efficiency and inference speed due to increasing complexity and scale.",
    "rag_method": "The introduction of the Dynamic Sequential Transformer (DST) offers a novel architectural enhancement focusing on dynamic token sequencing and adaptive attention recalibration.\n\n**Explanation:** The DST improves computational efficiency and inference speed by processing tokens dynamically, adjusting the sequence based on contextual relevance, and recalibrating attention mechanisms adaptively. This allows the model to focus computational resources on more relevant parts of the input, thus minimizing unnecessary processing and speeding up inference while maintaining efficiency.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The Dynamic Sequential Transformer (DST) still faces challenges with computational efficiency despite its novel approach, which could limit its scalability for very large datasets.\n- There are concerns about inference speed with DST, suggesting that while improvements have been made, it may not achieve optimal performance in real-time applications.\n- The DST method may struggle with generating factually accurate responses consistently, indicating that further refinements are necessary to enhance its reliability.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4403383742",
    "x": 2024.5826220102088,
    "y": 63.748856385993314,
    "title": "Enhancements to Large Language Models: Introducing Dynamic Syntactic Insertion for Improved Model Robustness and Generalization",
    "authors": [
      "Elena Tremaskina",
      "Santiago Deluca",
      "Christopher M. Thompson"
    ],
    "first_author": "Elena Tremaskina",
    "first_author_surname": "Tremaskina",
    "year": 2024,
    "cited_by_count": 2,
    "venue": "",
    "size": 10.528088048339951,
    "color": "hsl(0, 70%, 60%)",
    "label": "Tremaskina ,2024",
    "rag_problem": "Large language models often struggle with robustness and generalization when exposed to diverse and syntactically complex inputs.",
    "rag_method": "Dynamic Syntactic Insertion, a method that dynamically integrates syntactic structures into the input data during training.\n\n**Explanation:** By integrating syntactic structures, the model learns to better recognize and parse complex sentences, improving its ability to handle diverse linguistic patterns and making it more robust to variations in input syntax. This enhanced exposure allows the model to generalize more effectively to new and previously unseen data types by embedding syntactic awareness into the learning process.",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "- Explore the integration of dynamic syntactic insertion with other natural language processing tasks to assess its impact on a broader range of applications and domains for further robustness.\n- Investigate adaptive mechanisms for syntactic insertion that can dynamically adjust according to the context and task requirements to enhance model generalization.\n- Conduct a deeper analysis of how syntactic insertion influences different model architectures and training strategies to identify optimal configurations for diverse language models.\n- Develop methodologies to quantify the improvements in model robustness and generalization achieved through syntax-based interventions, providing a standardized benchmark for future evaluations.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4406458887",
    "x": 2023.5596315082475,
    "y": 65.74704708327197,
    "title": "Model Selection for HERITAGE-AI: Evaluating LLMs for Contextual Data Analysis of Maryland‚Äôs Domestic Traffic Ads (1824‚Äì1864)",
    "authors": [
      "Rajesh Kumar Gnanasekaran",
      "Lori Perine",
      "Mark F. Conrad"
    ],
    "first_author": "Rajesh Kumar Gnanasekaran",
    "first_author_surname": "Gnanasekaran",
    "year": 2024,
    "cited_by_count": 1,
    "venue": "",
    "size": 9.004533575060403,
    "color": "hsl(0, 70%, 60%)",
    "label": "Gnanasekaran ,2024",
    "rag_problem": "No clear research problem description found",
    "rag_method": "No clear method description found",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4412664683",
    "x": 2024.1530374462668,
    "y": 69.20176705944196,
    "title": "Adaptive Prompt Regeneration and Dynamic Response Structuring in Large Language Models Using the Dynamic Query-Response Calibration Protocol",
    "authors": [
      "Charles A. Whitney",
      "Eystein Jansen",
      "Victor Laskowski"
    ],
    "first_author": "Charles A. Whitney",
    "first_author_surname": "Whitney",
    "year": 2024,
    "cited_by_count": 1,
    "venue": "",
    "size": 9.004533575060403,
    "color": "hsl(0, 70%, 60%)",
    "label": "Whitney ,2024",
    "rag_problem": "Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂú®Âä®ÊÄÅÂèòÂåñÁöÑÂØπËØù‰∏ä‰∏ãÊñá‰∏≠Áº∫‰πèÊúâÊïàÁöÑÂìçÂ∫îÈÄÇÂ∫îÊú∫Âà∂ÔºåÂØºËá¥Êó†Ê≥ïÂáÜÁ°ÆÂØπËØùÂπ∂‰øùÊåÅ‰∏ä‰∏ãÊñá‰∏ÄËá¥ÊÄß„ÄÇ",
    "rag_method": "ÂºïÂÖ•‰∫ÜÂä®ÊÄÅÊü•ËØ¢ÂìçÂ∫îÊ†°ÂáÜÔºàDQRCÔºâÂçèËÆÆÔºåÈÄöËøáÈáçÊñ∞Ê†°ÂáÜÊ°ÜÊû∂Â¢ûÂº∫ÂìçÂ∫îÁöÑÈÄÇÂ∫îÊÄßÂíå‰∏éÂÖ∑‰Ωì‰∏ä‰∏ãÊñáÁªÜÂæÆÂ∑ÆÂºÇÁöÑÂØπÈΩê„ÄÇ\n\n**Explanation:** DQRCÂçèËÆÆÈÄöËøáÂØπËæìÂÖ•Êü•ËØ¢ÂíåÁîüÊàêÂìçÂ∫îËøõË°åÂä®ÊÄÅÊ†°ÂáÜÔºå‰ΩøÂæóÊ®°ÂûãËÉΩÂ§üÊõ¥Â•ΩÂú∞ÁêÜËß£ÂíåÈÄÇÂ∫î‰∏çÊñ≠ÂèòÂåñÁöÑÂØπËØù‰∏ä‰∏ãÊñá„ÄÇËøôÁßçËá™ÈÄÇÂ∫îËøáÁ®ã‰øùËØÅ‰∫ÜÂìçÂ∫îÁöÑÂáÜÁ°ÆÊÄßÔºåÂπ∂ËÉΩÂèäÊó∂Ë∞ÉÊï¥‰ª•ÂåπÈÖçÂΩìÂâçÁöÑÂØπËØùÈúÄÊ±ÇÂíå‰∏ä‰∏ãÊñáÁªÜËäÇ„ÄÇÂõ†Ê≠§ÔºåÊ®°ÂûãËÉΩÂ§ü‰∫ßÁîüÊõ¥Âä†‰∏ä‰∏ãÊñáÁõ∏ÂÖ≥ÂíåËøûË¥ØÁöÑËØ≠Ë®ÄËæìÂá∫„ÄÇ",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method still struggles with adaptive response mechanisms in rapidly shifting contexts, potentially limiting its effectiveness in certain dynamic scenarios.\n- Although the Dynamic Query-Response Calibration protocol enhances adaptability, there may still be scenarios where the response alignment with context-specific nuances is not fully optimized.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4389636360",
    "x": 2023.0321856669111,
    "y": -9.23149261483628,
    "title": "Can Large Language Models Transform Computational Social Science?",
    "authors": [
      "Caleb Ziems",
      "William A. Held",
      "Omar Ahmed Shaikh"
    ],
    "first_author": "Caleb Ziems",
    "first_author_surname": "Ziems",
    "year": 2023,
    "cited_by_count": 325,
    "venue": "",
    "size": 28.144542786746907,
    "color": "hsl(7, 70%, 60%)",
    "label": "Ziems ,2023",
    "rag_problem": "Current large language models (LLMs) are capable of zero-shot classification but often struggle to outperform fine-tuned models on taxonomic tasks and lack alignment with expert taxonomies relevant to computational social science (CSS).",
    "rag_method": "The authors propose a blended human-AI scheme where LLMs serve as zero-shot data annotators in conjunction with human annotation teams, utilizing strategies like Design-based Semi-supervised Learning (DSL) to achieve unbiased estimators.\n\n**Explanation:** The blended approach leverages LLM's ability to provide fair levels of agreement with humans on classification tasks by integrating them into human annotation pipelines. This integration can significantly speed up text analysis and improve annotation efficiency while ensuring that the underlying estimators remain unbiased, as DSL can correct for LLM errors and biases by using a mix of pseudo-labels and a small number of gold labels.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The tasks selected for this study do not cover all application domains, particularly excluding sensitive areas like mental health and cultural studies which require expert annotations and community-specific knowledge.\n- The method's scope is limited by available data resources, primarily representing standard dialects from WEIRD populations, which may not capture cross-cultural nuances or applicability effectively.\n- There is a significant concern about data leakage, where test data might have been seen during pre-training of LLMs, potentially leading to artificially inflated performance evaluations.",
    "rag_future_work": "- Investigate the development of open-source LLMs focused on classification tasks to enhance accessibility and adaptability for diverse computational social science applications.\n- Explore the use of larger instruction-tuned models to improve generation quality in terms of faithfulness, relevance, coherence, and fluency, aligning LLM outputs more closely with human expectations and preferences.\n- Study the emergence of new computational social science paradigms that leverage the multipurpose capabilities of LLMs, particularly how these models can transform long-term research methods and outcomes.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 75
  },
  {
    "id": "W4395025834",
    "x": 2024.2631071794428,
    "y": -19.893374272230634,
    "title": "The benefits, risks and bounds of personalizing the alignment of large language models to individuals",
    "authors": [
      "Hannah Rose Kirk",
      "Bertie Vidgen",
      "Paul R√∂ttger"
    ],
    "first_author": "Hannah Rose Kirk",
    "first_author_surname": "Kirk",
    "year": 2024,
    "cited_by_count": 70,
    "venue": "",
    "size": 22.417222850880947,
    "color": "hsl(0, 70%, 60%)",
    "label": "Kirk ,2024",
    "rag_problem": "No clear research problem description found",
    "rag_method": "No clear method description found",
    "rag_limitation": "No clear limitation description found",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 1
  },
  {
    "id": "W4221045317",
    "x": 2022.1783453369972,
    "y": -3.8074513795870066,
    "title": "Towards a standard for identifying and managing bias in artificial intelligence",
    "authors": [
      "Reva Schwartz",
      "Apostol Vassilev",
      "Kristen Greene"
    ],
    "first_author": "Reva Schwartz",
    "first_author_surname": "Schwartz",
    "year": 2022,
    "cited_by_count": 437,
    "venue": "",
    "size": 36.567784388421,
    "color": "hsl(14, 70%, 60%)",
    "label": "Schwartz ,2022",
    "rag_problem": "Bias in AI systems can lead to harmful impacts and reduce public trust in AI technology.",
    "rag_method": "Adopting a socio-technical framework for identifying and managing AI bias.\n\n**Explanation:** A socio-technical framework allows for an expansive view that includes not only computational factors but also human and systemic biases that AI systems may amplify. It emphasizes the importance of recognizing and addressing biases in datasets, algorithms, and human interactions with AI systems. By considering AI within the larger social system and aligning AI system design with societal values, stakeholders can better identify, understand, and mitigate biases across various stages of the AI lifecycle.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The method relies on widespread input from diverse stakeholders, which can be time-consuming and challenging to coordinate.\n- Our initial socio-technical framework for addressing AI bias is foundational but not comprehensive, indicating that further detailed guidance is still necessary.",
    "rag_future_work": "- Develop socio-technical guidance in collaboration with stakeholders impacted by AI bias to improve organizational practices and enhance AI system trustworthiness.\n- Conduct supporting standards development activities, including workshops and public comment periods, to gather broad input and build consensus on AI bias management.\n- Create detailed technical guidance by engaging diverse stakeholders to address the complex challenges of AI bias, ensuring the inclusion of perspectives from communities affected by AI systems.\n- Explore the interaction between computational, statistical factors, and human biases from a socio-technical perspective to better understand and mitigate biased outcomes in AI systems.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 59
  },
  {
    "id": "W4391673229",
    "x": 2023.5681384193063,
    "y": 45.33392581387941,
    "title": "Can Large Language Model Agents Simulate Human Trust Behavior?",
    "authors": [
      "Chengxing Xie",
      "Canyu Chen",
      "Feiran Jia"
    ],
    "first_author": "Chengxing Xie",
    "first_author_surname": "Xie",
    "year": 2024,
    "cited_by_count": 5,
    "venue": "",
    "size": 13.132621623400352,
    "color": "hsl(0, 70%, 60%)",
    "label": "Xie ,2024",
    "rag_problem": "Large language model (LLM) agents are increasingly used to simulate human behavior, yet their ability to accurately replicate human trust behavior is unproven.",
    "rag_method": "Use Trust Games to evaluate and demonstrate LLM agents' ability to exhibit trust behavior akin to humans.\n\n**Explanation:** The Trust Game, a well-established experimental protocol in behavioral economics, involves scenarios where an agent must decide how much to trust another player with their resources. By applying this framework, the study evaluates whether LLM agents exhibit behaviors such as reciprocity anticipation, risk perception, and prosocial preference. Findings show that GPT-4 exhibits high behavioral alignment with human trust behavior, thus suggesting its potential for simulating human trust interactions.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The paper does not explicitly address potential issues related to the privacy and fairness of their approach, which are critical in trust simulations.\n- There is a reliance on future works to address current limitations, suggesting that the method may not fully capture the complexity of human trust behavior as it stands.\n- The authors acknowledge the importance of transparency in discussing limitations, indicating that some aspects may still not be comprehensively covered in the current version of the work.",
    "rag_future_work": "- Explore the trust behavior of large language model agents in complex and dynamic environments to mirror more realistic scenarios beyond simplified trust games.\n- Conduct interdisciplinary studies involving behavioral science, cognitive science, psychology, and sociology to better understand the reasoning processes underpinning LLM agents' trust behavior and its relation to human trust behavior.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 46
  },
  {
    "id": "W4312536817",
    "x": 2021.9291190797826,
    "y": 9.409291020099781,
    "title": "A Generic AI-Based Technique for Assessing Student Performance in Conducting Online Virtual and Remote Controlled Laboratories",
    "authors": [
      "Ahmed M. Abd El‚ÄêHaleem",
      "Mohab Mohammed Eid",
      "Mahmoud M. Elmesalawy"
    ],
    "first_author": "Ahmed M. Abd El‚ÄêHaleem",
    "first_author_surname": "El‚ÄêHaleem",
    "year": 2022,
    "cited_by_count": 16,
    "venue": "",
    "size": 17.045934206848298,
    "color": "hsl(14, 70%, 60%)",
    "label": "El‚ÄêHaleem ,2022",
    "rag_problem": "Assessing student performance in online virtual and remote controlled labs as part of the Laboratory Learning Management System (LLMS).",
    "rag_method": "A generic AI-based technique designed to evaluate student activities and performance within these online lab environments.\n\n**Explanation:** The AI-based assessment technique can analyze various aspects of student interaction and work within virtual and remote labs, providing a comprehensive performance evaluation. This approach leverages AI's ability to handle complex data and generate insights that traditional evaluation methods might miss, thus ensuring that performance metrics reflect the true state of student engagement and understanding.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method is challenged by the need for a specialized Laboratory Learning Management System (LLMS) to effectively support and assess online lab activities, which can complicate its adoption.\n- The approach may struggle with accurately evaluating student performance in complex online virtual and remote controlled laboratories due to potential limitations in its assessment capability.",
    "rag_future_work": "No clear future work description found",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4392573682",
    "x": 2023.6644345636832,
    "y": -30.985375255845955,
    "title": "Integrating AI in education: Opportunities, challenges, and ethical considerations",
    "authors": [
      "Chima Abimbola Eden",
      "Onyebuchi Nneamaka Chisom",
      "Idowu Sulaimon Adeniyi"
    ],
    "first_author": "Chima Abimbola Eden",
    "first_author_surname": "Eden",
    "year": 2024,
    "cited_by_count": 104,
    "venue": "",
    "size": 23.88747788196023,
    "color": "hsl(0, 70%, 60%)",
    "label": "Eden ,2024",
    "rag_problem": "There is a challenge in achieving equitable access to AI-powered educational tools for all students, especially those from marginalized communities and underserved regions, due to disparities in technology and internet connectivity.",
    "rag_method": "Design inclusive AI solutions that prioritize affordability, usability, and accessibility, and provide necessary infrastructure and training to facilitate access.\n\n**Explanation:** By ensuring AI tools are designed to be affordable, user-friendly, and compatible with diverse needs, educators can help bridge the gap created by the digital divide. Training and support ensure that both educators and students can effectively utilize these resources, thus extending the benefits of personalized AI-driven education to all, regardless of socio-economic background.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The integration of AI in education presents challenges related to accessibility, implying that not all educational institutions or students may have equal access to AI technologies.\n- There are concerns about data privacy and security when using AI in educational settings, highlighting the potential vulnerabilities in handling sensitive student information.\n- The digital divide is a limitation, suggesting that disparities in access to technology may exacerbate educational inequalities.\n- Potential biases in AI algorithms represent a risk in educational applications, potentially leading to unfair treatment or discrimination against certain groups of learners.",
    "rag_future_work": "- Develop strategies for responsibly and ethically harnessing AI's potential in education by addressing challenges like accessibility, data privacy, security, and bias in AI algorithms.\n- Explore and implement ways to bridge the digital divide, ensuring equal access to AI-enhanced educational tools and resources for all students, regardless of their socio-economic status.\n- Innovate personalized learning experiences through AI to enhance student engagement and academic outcomes while maintaining a focus on ethical considerations.\n- Collaborate among stakeholders in education to continuously improve teaching and learning practices by integrating AI technologies, focusing on both opportunities and challenges.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 7
  },
  {
    "id": "W4401843656",
    "x": 2023.48698544718,
    "y": 28.063198624221535,
    "title": "Generative Artificial Intelligence and Web Accessibility: Towards an Inclusive and Sustainable Future",
    "authors": [
      "Patricia Acosta-Vargas",
      "Bel√©n Salvador-Acosta",
      "Sylvia Novillo-Villegas"
    ],
    "first_author": "Patricia Acosta-Vargas",
    "first_author_surname": "Acosta-Vargas",
    "year": 2024,
    "cited_by_count": 14,
    "venue": "",
    "size": 16.57562773035012,
    "color": "hsl(0, 70%, 60%)",
    "label": "Acosta-Vargas ,2024",
    "rag_problem": "Generative AI tools present significant accessibility barriers for users with disabilities, as identified using WCAG 2.2 success criteria.",
    "rag_method": "Integrate accessibility considerations into the development of Generative AI applications from the outset and adopt a proactive approach.\n\n**Explanation:** By considering accessibility during the initial stages of development, developers can design applications that inherently accommodate the needs of disabled users, thereby preventing barriers rather than attempting to fix them post-development. A proactive approach ensures ongoing evaluation and adaptation to evolving accessibility standards, promoting continual improvement in user inclusivity.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- Our method may not fully address all accessibility barriers encountered by disabled users in Generative AI tools, as significant issues were still identified during the evaluation.\n- There is a limitation in the scope of our evaluation, as it primarily focuses on applications that adhere to WCAG 2.2 success criteria, potentially leaving out other relevant accessibility standards and guidelines.\n- The study emphasizes the need for a proactive approach, suggesting our current method may have limitations in anticipating all potential accessibility challenges from the outset.",
    "rag_future_work": "- Explore the integration of generative AI techniques with existing web accessibility standards to ensure that AI-generated content is universally accessible. This could involve developing guidelines and tools that help creators utilize AI without compromising accessibility.\n- Investigate the long-term sustainability of using generative AI in web accessibility, focusing on the environmental impact and energy consumption of these technologies, to align with broader sustainability goals.\n- Conduct user studies with diverse populations to evaluate the effectiveness of generative AI tools in improving web accessibility and tailor AI models to better serve underrepresented groups, ensuring an inclusive internet environment.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 2
  },
  {
    "id": "W4388506371",
    "x": 2020.5358119251343,
    "y": 19.96746938587337,
    "title": "Implementing computer-based test for theory exams in tertiary institutions in Nigeria",
    "authors": [
      "Emmanuel Addah",
      "Godwin Ovuworie",
      "Godfrey Omonefe Ariavie"
    ],
    "first_author": "Emmanuel Addah",
    "first_author_surname": "Addah",
    "year": 2021,
    "cited_by_count": 1,
    "venue": "",
    "size": 9.004533575060403,
    "color": "hsl(21, 70%, 60%)",
    "label": "Addah ,2021",
    "rag_problem": "The current online testing systems in Nigerian tertiary institutions primarily handle objective-type questions and lack a comprehensive mechanism for conducting theory-based exams. Additionally, traditional paper-based systems face issues such as high paper costs, storage space requirements, and the risk of losing or damaging scripts.",
    "rag_method": "Implementation of a paperless theory examination system using a client-server architecture and digital tools such as Huion graphics pen tablets and Microsoft OneNote, integrated with the Paperless Exam Ver. 1.0 software.\n\n**Explanation:** The proposed system enables a digital method of conducting theory exams by allowing students to write answers using digital pens on OneNote, which are then submitted digitally to a server. This approach reduces the need for physical paper, minimizing costs associated with printing and storage while ensuring scripts are safely stored in digital formats. Furthermore, digital storage mitigates the risk of damage or loss, utilizes existing computer networks, and enhances ease of script archival and retrieval.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The digital pen, necessary for the system's functionality, took two years to adequately integrate, suggesting potential delays and reliability issues in deployment within institutions.\n- The preferred version of Microsoft OneNote (2013) was chosen due to the absence of bugs found in later versions, indicating dependency on specific software versions and potential issues with future software updates.\n- OneNote 2013 lacks the pen sensitivity feature found in later versions, which can enhance the quality of digital writing, presenting limitations in user experience and potentially affecting the accuracy of theory-based answers.",
    "rag_future_work": "- Implement an automatic grading system by developing an algorithm that includes keywords for specific questions and creates a marking scheme to grade answers without lecturer intervention.\n- Enhance the system's capabilities to improve computer proficiency among students and lecturers, thereby promoting greater digital literacy within the academic environment.\n- Explore the integration of additional features to the computer-based testing system to support various types of assessments beyond theoretical exams, such as practical or interactive evaluations.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 16
  },
  {
    "id": "W4392750151",
    "x": 2023.8691484101578,
    "y": 0.21675511573737372,
    "title": "A review of AI-driven pedagogical strategies for equitable access to science education",
    "authors": [
      "Chima Abimbola Eden",
      "Olabisi Oluwakemi Adeleye",
      "Idowu Sulaimon Adeniyi"
    ],
    "first_author": "Chima Abimbola Eden",
    "first_author_surname": "Eden",
    "year": 2024,
    "cited_by_count": 25,
    "venue": "",
    "size": 18.64245306344593,
    "color": "hsl(0, 70%, 60%)",
    "label": "Eden ,2024",
    "rag_problem": "Disparities in access to quality science education among marginalized and underserved populations.",
    "rag_method": "AI-driven personalized learning platforms that adapt to individual learning styles and pace.\n\n**Explanation:** These AI platforms provide tailored instruction by adapting materials and teaching methods to the specific needs of each student, ensuring inclusivity. By accommodating different learning styles and abilities, personalized learning helps to bridge the gaps caused by socioeconomic disparities and limited resources. This approach enables all students, regardless of background, to have equitable access to science education.",
    "rag_limitation": "**Limitations extracted from paper sections:**\n\n- The implementation of AI-driven pedagogical strategies requires significant investment in technology and professional development for educators, which may be challenging for schools and institutions with limited resources.\n- There are ethical considerations related to data privacy and security, as well as concerns about how student data is collected, stored, and used by AI systems.\n- AI-driven personalized learning and assessment tools could introduce algorithmic bias, impacting students' learning experiences and potentially undermining equity in science education.",
    "rag_future_work": "- Develop AI-powered chatbots and virtual assistants using natural language processing (NLP) to provide on-demand tutoring and support to students, enhancing accessibility and inclusivity.\n- Integrate AI technologies into virtual and augmented reality (VR/AR) environments to create immersive learning experiences, with AI algorithms analyzing student interactions for personalized feedback.\n- Conduct further research on the effectiveness of AI-driven pedagogical strategies in improving learning outcomes, including their impact on student engagement, motivation, and academic achievement, and address potential biases in AI.\n- Provide training and professional development for educators on how to integrate AI technologies into their teaching practices, focusing on data interpretation and ethical considerations.",
    "analysis_method": "deep_paper_2.0",
    "sections_extracted": 12
  }
];
                const edgesData = [
  {
    "from": "W4382246105",
    "to": "W4385574029",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4382246105",
    "to": "W3106340866",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4382246105",
    "to": "W2970925270",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4382246105",
    "to": "W3133702157",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4382246105",
    "to": "W2117130368",
    "type": "Alternative",
    "color": "#E67E22",
    "original_color": "#E67E22",
    "width": 2.0,
    "dash": "dot",
    "description": "Âè¶ËæüËπäÂæÑ - BÁî®ÂÆåÂÖ®‰∏çÂêåÁöÑËåÉÂºèËß£ÂÜ≥ÈóÆÈ¢òÔºàÈ¢†Ë¶ÜÂàõÊñ∞Ôºâ"
  },
  {
    "from": "W3046375318",
    "to": "W2963716420",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W3046375318",
    "to": "W2145383760",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3046375318",
    "to": "W2736047977",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3046375318",
    "to": "W2174775663",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3046375318",
    "to": "W2970771982",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W3046375318",
    "to": "W2034269086",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3046375318",
    "to": "W2911489562",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W3046375318",
    "to": "W2971258845",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W3046375318",
    "to": "W2250539671",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3046375318",
    "to": "W2981852735",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3046375318",
    "to": "W2955483668",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4205802268",
    "to": "W2070205520",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4205802268",
    "to": "W2614620964",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4205802268",
    "to": "W1562955078",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4205802268",
    "to": "W3104723404",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4205802268",
    "to": "W2147152072",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4205802268",
    "to": "W2130942839",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4205802268",
    "to": "W2155870214",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4205802268",
    "to": "W2012976256",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2593831809",
    "to": "W4382246105",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2593831809",
    "to": "W2970183140",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2593831809",
    "to": "W2950601686",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2593831809",
    "to": "W3175225269",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2593831809",
    "to": "W3106340866",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399528455",
    "to": "W4382246105",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399528455",
    "to": "W3184144760",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399528455",
    "to": "W4287855127",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4399528455",
    "to": "W4287887510",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399528455",
    "to": "W4385571817",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399528455",
    "to": "W3133702157",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4394994587",
    "to": "W4382246105",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4394994587",
    "to": "W3042856524",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4394994587",
    "to": "W4367046619",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384561707",
    "to": "W3046375318",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384561707",
    "to": "W2905810301",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384561707",
    "to": "W3150212014",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384561707",
    "to": "W2747680751",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384561707",
    "to": "W4367310920",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384561707",
    "to": "W3133702157",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384561707",
    "to": "W2911489562",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384071683",
    "to": "W3046375318",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4384071683",
    "to": "W4224308101",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384071683",
    "to": "W4307079201",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384071683",
    "to": "W3162922479",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384071683",
    "to": "W2972522091",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384071683",
    "to": "W3083410900",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3156333129",
    "to": "W3046375318",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3156333129",
    "to": "W2147152072",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3156333129",
    "to": "W3034850762",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3156333129",
    "to": "W2963143606",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3156333129",
    "to": "W2612228435",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3156333129",
    "to": "W2250539671",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4360845368",
    "to": "W4205802268",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4360845368",
    "to": "W2083078026",
    "type": "Realizes",
    "color": "#9B59B6",
    "original_color": "#9B59B6",
    "width": 2.5,
    "dash": "solid",
    "description": "ÂÆûÁé∞ÊÑøÊôØ - BÂÆûÁé∞‰∫ÜAÁöÑÊú™Êù•Â∑•‰ΩúÂª∫ËÆÆÔºàÁßëÁ†î‰º†ÊâøÔºâ"
  },
  {
    "from": "W4360845368",
    "to": "W3133702157",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4360845368",
    "to": "W3183428091",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4360845368",
    "to": "W4244669226",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4360845368",
    "to": "W4241903662",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4317823603",
    "to": "W4205802268",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4317823603",
    "to": "W4238374879",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4317823603",
    "to": "W2117002298",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4317823603",
    "to": "W2251172991",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4317823603",
    "to": "W1889043906",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4317823603",
    "to": "W2099921486",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4389155560",
    "to": "W4205802268",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4389155560",
    "to": "W4389155560",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4389155560",
    "to": "W2002089036",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4389155560",
    "to": "W2060908944",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4389155560",
    "to": "W2036942465",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4389155560",
    "to": "W49981513",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4389155560",
    "to": "W2885484425",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4387425757",
    "to": "W4385292983",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4387425757",
    "to": "W4383374753",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4387425757",
    "to": "W4323655724",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4387425757",
    "to": "W4362655923",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4405910844",
    "to": "W4385292983",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4405910844",
    "to": "W4383374753",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4405910844",
    "to": "W4385988359",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4405910844",
    "to": "W4391759824",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4405910844",
    "to": "W4405660551",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4405910844",
    "to": "W2747680751",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4405660551",
    "to": "W4385292983",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4405660551",
    "to": "W4383374753",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4405660551",
    "to": "W4385988359",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4405660551",
    "to": "W4384484700",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4393318216",
    "to": "W4390602553",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4393318216",
    "to": "W3209721572",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4393318216",
    "to": "W4392340173",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4393318216",
    "to": "W4324373918",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4393318216",
    "to": "W4285199616",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399496842",
    "to": "W4390602553",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399496842",
    "to": "W4389636360",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399496842",
    "to": "W4393318216",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399496842",
    "to": "W4395025834",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399496842",
    "to": "W4221045317",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4399496842",
    "to": "W4391673229",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399496842",
    "to": "W4396701991",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4399496842",
    "to": "W4396675319",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399496842",
    "to": "W4391143839",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4409894900",
    "to": "W4390602553",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4409894900",
    "to": "W4312536817",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4409894900",
    "to": "W4392573682",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4409894900",
    "to": "W4401843656",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4409894900",
    "to": "W4388506371",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4409894900",
    "to": "W4392750151",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4385574029",
    "to": "W2933138175",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4385574029",
    "to": "W2949433733",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4385574029",
    "to": "W4224308101",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4385574029",
    "to": "W4253067820",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3106340866",
    "to": "W2834342720",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3106340866",
    "to": "W3034328552",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3106340866",
    "to": "W3035044482",
    "type": "Alternative",
    "color": "#E67E22",
    "original_color": "#E67E22",
    "width": 2.0,
    "dash": "dot",
    "description": "Âè¶ËæüËπäÂæÑ - BÁî®ÂÆåÂÖ®‰∏çÂêåÁöÑËåÉÂºèËß£ÂÜ≥ÈóÆÈ¢òÔºàÈ¢†Ë¶ÜÂàõÊñ∞Ôºâ"
  },
  {
    "from": "W3106340866",
    "to": "W2130942839",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2970925270",
    "to": "W2962863357",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2970925270",
    "to": "W2095705004",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2970925270",
    "to": "W2919290281",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W2970925270",
    "to": "W2130942839",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2963716420",
    "to": "W2955483668",
    "type": "Extends",
    "color": "#2ECC71",
    "original_color": "#2ECC71",
    "width": 2.0,
    "dash": "solid",
    "description": "ÊñπÊ≥ïÊâ©Â±ï - BÂú®AÁöÑÊñπÊ≥ïÂü∫Á°Ä‰∏äÂÅöÂ¢ûÈáèÊîπËøõÔºàÂæÆÂàõÊñ∞Ôºâ"
  },
  {
    "from": "W2963716420",
    "to": "W2250539671",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2963716420",
    "to": "W2190333735",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2963716420",
    "to": "W2911489562",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W2145383760",
    "to": "W2040610660",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W2145383760",
    "to": "W2100751507",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2145383760",
    "to": "W2101727078",
    "type": "Extends",
    "color": "#2ECC71",
    "original_color": "#2ECC71",
    "width": 2.0,
    "dash": "solid",
    "description": "ÊñπÊ≥ïÊâ©Â±ï - BÂú®AÁöÑÊñπÊ≥ïÂü∫Á°Ä‰∏äÂÅöÂ¢ûÈáèÊîπËøõÔºàÂæÆÂàõÊñ∞Ôºâ"
  },
  {
    "from": "W2145383760",
    "to": "W2126276057",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W2145383760",
    "to": "W2168905447",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2736047977",
    "to": "W2174775663",
    "type": "Extends",
    "color": "#2ECC71",
    "original_color": "#2ECC71",
    "width": 2.0,
    "dash": "solid",
    "description": "ÊñπÊ≥ïÊâ©Â±ï - BÂú®AÁöÑÊñπÊ≥ïÂü∫Á°Ä‰∏äÂÅöÂ¢ûÈáèÊîπËøõÔºàÂæÆÂàõÊñ∞Ôºâ"
  },
  {
    "from": "W2736047977",
    "to": "W2262329766",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2736047977",
    "to": "W2034269086",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2736047977",
    "to": "W2117692326",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W2736047977",
    "to": "W2594411204",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2736047977",
    "to": "W2032069669",
    "type": "Extends",
    "color": "#2ECC71",
    "original_color": "#2ECC71",
    "width": 2.0,
    "dash": "solid",
    "description": "ÊñπÊ≥ïÊâ©Â±ï - BÂú®AÁöÑÊñπÊ≥ïÂü∫Á°Ä‰∏äÂÅöÂ¢ûÈáèÊîπËøõÔºàÂæÆÂàõÊñ∞Ôºâ"
  },
  {
    "from": "W2070205520",
    "to": "W2070205520",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2070205520",
    "to": "W2131243403",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2070205520",
    "to": "W2155870214",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W2070205520",
    "to": "W2345398358",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2070205520",
    "to": "W1604834089",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2070205520",
    "to": "W2044688197",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W1562955078",
    "to": "W2140406733",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W1562955078",
    "to": "W2950577311",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W1562955078",
    "to": "W2117130368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W1562955078",
    "to": "W2146502635",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W1562955078",
    "to": "W1662133657",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4383374753",
    "to": "W1966084915",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4383374753",
    "to": "W2039154778",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4383374753",
    "to": "W2168015293",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4383374753",
    "to": "W2044823787",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4392353733",
    "to": "W4384561707",
    "type": "Realizes",
    "color": "#9B59B6",
    "original_color": "#9B59B6",
    "width": 2.5,
    "dash": "solid",
    "description": "ÂÆûÁé∞ÊÑøÊôØ - BÂÆûÁé∞‰∫ÜAÁöÑÊú™Êù•Â∑•‰ΩúÂª∫ËÆÆÔºàÁßëÁ†î‰º†ÊâøÔºâ"
  },
  {
    "from": "W4391855109",
    "to": "W4384561707",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4391855109",
    "to": "W4394994587",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4391855109",
    "to": "W4391136507",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4391855109",
    "to": "W4323655724",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4392193048",
    "to": "W4384561707",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4392193048",
    "to": "W4384071683",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4392193048",
    "to": "W4307079201",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4392193048",
    "to": "W4391136507",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4400324908",
    "to": "W4384561707",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4400324908",
    "to": "W4392193048",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4400324908",
    "to": "W4384071683",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4400324908",
    "to": "W3162922479",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4400324908",
    "to": "W3083410900",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4390919701",
    "to": "W4384561707",
    "type": "Extends",
    "color": "#2ECC71",
    "original_color": "#2ECC71",
    "width": 2.0,
    "dash": "solid",
    "description": "ÊñπÊ≥ïÊâ©Â±ï - BÂú®AÁöÑÊñπÊ≥ïÂü∫Á°Ä‰∏äÂÅöÂ¢ûÈáèÊîπËøõÔºàÂæÆÂàõÊñ∞Ôºâ"
  },
  {
    "from": "W4390919701",
    "to": "W4384071683",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4390919701",
    "to": "W4367310920",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W3150212014",
    "to": "W2905810301",
    "type": "Alternative",
    "color": "#E67E22",
    "original_color": "#E67E22",
    "width": 2.0,
    "dash": "dot",
    "description": "Âè¶ËæüËπäÂæÑ - BÁî®ÂÆåÂÖ®‰∏çÂêåÁöÑËåÉÂºèËß£ÂÜ≥ÈóÆÈ¢òÔºàÈ¢†Ë¶ÜÂàõÊñ∞Ôºâ"
  },
  {
    "from": "W2747680751",
    "to": "W2117130368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4367310920",
    "to": "W4384071683",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4391136507",
    "to": "W4384071683",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4387500346",
    "to": "W4384071683",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4387500346",
    "to": "W2970771982",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4387500346",
    "to": "W2911489562",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4387500346",
    "to": "W4367310920",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4386697749",
    "to": "W4384071683",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4366989525",
    "to": "W4384071683",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4366989525",
    "to": "W2911489562",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3162922479",
    "to": "W2911489562",
    "type": "Realizes",
    "color": "#9B59B6",
    "original_color": "#9B59B6",
    "width": 2.5,
    "dash": "solid",
    "description": "ÂÆûÁé∞ÊÑøÊôØ - BÂÆûÁé∞‰∫ÜAÁöÑÊú™Êù•Â∑•‰ΩúÂª∫ËÆÆÔºàÁßëÁ†î‰º†ÊâøÔºâ"
  },
  {
    "from": "W3162922479",
    "to": "W2963716420",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2972522091",
    "to": "W2911489562",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2012976256",
    "to": "W2070205520",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2012976256",
    "to": "W2044688197",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2791507392",
    "to": "W2070205520",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2067130497",
    "to": "W2070205520",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3089323922",
    "to": "W2070205520",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2911489562",
    "to": "W2963716420",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2970771982",
    "to": "W2963716420",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2971258845",
    "to": "W2963716420",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3160137267",
    "to": "W2963716420",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W3122890974",
    "to": "W3156333129",
    "type": "Realizes",
    "color": "#9B59B6",
    "original_color": "#9B59B6",
    "width": 2.5,
    "dash": "solid",
    "description": "ÂÆûÁé∞ÊÑøÊôØ - BÂÆûÁé∞‰∫ÜAÁöÑÊú™Êù•Â∑•‰ΩúÂª∫ËÆÆÔºàÁßëÁ†î‰º†ÊâøÔºâ"
  },
  {
    "from": "W3180181113",
    "to": "W3156333129",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3195038684",
    "to": "W3156333129",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4382516982",
    "to": "W3156333129",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3133966466",
    "to": "W3156333129",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2979462702",
    "to": "W2614620964",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2725817383",
    "to": "W2614620964",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W1977132984",
    "to": "W2614620964",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4214933144",
    "to": "W2614620964",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2045236476",
    "to": "W2614620964",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4309674289",
    "to": "W2593831809",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4402665833",
    "to": "W2593831809",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2347127863",
    "to": "W2593831809",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4389265550",
    "to": "W2593831809",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2014583745",
    "to": "W2593831809",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2981852735",
    "to": "W2970925270",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3093871477",
    "to": "W2970925270",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3017454464",
    "to": "W2970925270",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W3176023514",
    "to": "W2970925270",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4406302454",
    "to": "W4399528455",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4394630908",
    "to": "W4399528455",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4409283601",
    "to": "W4399528455",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4403203873",
    "to": "W4399528455",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4406658975",
    "to": "W4399528455",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4401857375",
    "to": "W4394994587",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4401834466",
    "to": "W4394994587",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4400118952",
    "to": "W4394994587",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4404239715",
    "to": "W4394994587",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4392394652",
    "to": "W4387425757",
    "type": "Realizes",
    "color": "#9B59B6",
    "original_color": "#9B59B6",
    "width": 2.5,
    "dash": "solid",
    "description": "ÂÆûÁé∞ÊÑøÊôØ - BÂÆûÁé∞‰∫ÜAÁöÑÊú™Êù•Â∑•‰ΩúÂª∫ËÆÆÔºàÁßëÁ†î‰º†ÊâøÔºâ"
  },
  {
    "from": "W4392938070",
    "to": "W4387425757",
    "type": "Realizes",
    "color": "#9B59B6",
    "original_color": "#9B59B6",
    "width": 2.5,
    "dash": "solid",
    "description": "ÂÆûÁé∞ÊÑøÊôØ - BÂÆûÁé∞‰∫ÜAÁöÑÊú™Êù•Â∑•‰ΩúÂª∫ËÆÆÔºàÁßëÁ†î‰º†ÊâøÔºâ"
  },
  {
    "from": "W4392976364",
    "to": "W4387425757",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399365040",
    "to": "W4387425757",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399788390",
    "to": "W4387425757",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4362655923",
    "to": "W4383374753",
    "type": "Realizes",
    "color": "#9B59B6",
    "original_color": "#9B59B6",
    "width": 2.5,
    "dash": "solid",
    "description": "ÂÆûÁé∞ÊÑøÊôØ - BÂÆûÁé∞‰∫ÜAÁöÑÊú™Êù•Â∑•‰ΩúÂª∫ËÆÆÔºàÁßëÁ†î‰º†ÊâøÔºâ"
  },
  {
    "from": "W2941187122",
    "to": "W2736047977",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2984946521",
    "to": "W2736047977",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4389042124",
    "to": "W2736047977",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2896674942",
    "to": "W2736047977",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2003516452",
    "to": "W2145383760",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2230749025",
    "to": "W2145383760",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2090247362",
    "to": "W2145383760",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2032069669",
    "to": "W2145383760",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4385988359",
    "to": "W4383374753",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4390023570",
    "to": "W4383374753",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4384484700",
    "to": "W4383374753",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2942091739",
    "to": "W1562955078",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3104723404",
    "to": "W1562955078",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2294774419",
    "to": "W1562955078",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2786464815",
    "to": "W1562955078",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W2952190837",
    "to": "W1562955078",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4402916447",
    "to": "W4405910844",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4407837005",
    "to": "W4405910844",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4407852113",
    "to": "W4405910844",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4409385489",
    "to": "W4405910844",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4408757728",
    "to": "W4405910844",
    "type": "Extends",
    "color": "#2ECC71",
    "original_color": "#2ECC71",
    "width": 2.0,
    "dash": "solid",
    "description": "ÊñπÊ≥ïÊâ©Â±ï - BÂú®AÁöÑÊñπÊ≥ïÂü∫Á°Ä‰∏äÂÅöÂ¢ûÈáèÊîπËøõÔºàÂæÆÂàõÊñ∞Ôºâ"
  },
  {
    "from": "W4285294723",
    "to": "W3106340866",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W3202729335",
    "to": "W3106340866",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W3176690085",
    "to": "W3106340866",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4405586902",
    "to": "W4405660551",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4391143839",
    "to": "W4405660551",
    "type": "Overcomes",
    "color": "#E74C3C",
    "original_color": "#E74C3C",
    "width": 3.0,
    "dash": "solid",
    "description": "ÊîªÂÖã/‰ºòÂåñ - BËß£ÂÜ≥‰∫ÜAÁöÑÂ±ÄÈôêÊÄßÔºàÁ∫µÂêëÊ∑±ÂåñÔºâ"
  },
  {
    "from": "W4411183121",
    "to": "W4405660551",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4408417845",
    "to": "W4405660551",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4381332452",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4390583680",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4389456104",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399213274",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4394684629",
    "to": "W4360845368",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399009725",
    "to": "W4393318216",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4396701991",
    "to": "W4393318216",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4398173773",
    "to": "W4393318216",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4399395229",
    "to": "W4393318216",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4396675319",
    "to": "W4393318216",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4403637392",
    "to": "W4317823603",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4402418067",
    "to": "W4317823603",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4394828653",
    "to": "W4317823603",
    "type": "Adapts_to",
    "color": "#3498DB",
    "original_color": "#3498DB",
    "width": 2.0,
    "dash": "dash",
    "description": "ËøÅÁßª/Â∫îÁî® - BÂ∞ÜAÁöÑÊñπÊ≥ïÂ∫îÁî®Âà∞Êñ∞È¢ÜÂüüÔºàÊ®™ÂêëÊâ©Êï£Ôºâ"
  },
  {
    "from": "W4393097350",
    "to": "W4317823603",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4367397709",
    "to": "W4317823603",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4388691863",
    "to": "W4385574029",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4310576793",
    "to": "W4385574029",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4393305539",
    "to": "W4385574029",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4385570444",
    "to": "W4385574029",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4396743843",
    "to": "W4389155560",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4392773774",
    "to": "W4389155560",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4406003524",
    "to": "W4389155560",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4406135761",
    "to": "W4389155560",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4403233717",
    "to": "W4399496842",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4412664660",
    "to": "W4399496842",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4403383742",
    "to": "W4399496842",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4406458887",
    "to": "W4399496842",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  },
  {
    "from": "W4412664683",
    "to": "W4399496842",
    "type": "Baselines",
    "color": "#95A5A6",
    "original_color": "#95A5A6",
    "width": 1.0,
    "dash": "solid",
    "description": "Âü∫Á∫øÂØπÊØî - B‰ªÖÊääA‰Ωú‰∏∫ÂØπÊØîÂØπË±°ÔºàÊó†Áõ¥Êé•ÁªßÊâøÔºâ"
  }
];

                // Deep SurveyÊï∞ÊçÆ
                const deepSurveyData = {
  "topic": "Natural Language Processing",
  "timestamp": "2025-12-21T21:36:46.885920",
  "pruning_stats": {
    "original_papers": 226,
    "pruned_papers": 69,
    "removed_papers": 157,
    "pruning_mode": "comprehensive",
    "strong_components_count": 18,
    "components_with_seed": 0,
    "largest_component_size": 9,
    "seed_papers": 0,
    "original_edges": 282,
    "strong_edges": 58,
    "weak_edges_removed": 30,
    "retention_rate": 0.3053097345132743,
    "relation_type_distribution": {
      "Baselines": 30,
      "Alternative": 2,
      "Overcomes": 21,
      "Adapts_to": 23,
      "Extends": 6,
      "Realizes": 6
    }
  },
  "evolutionary_paths": [
    {
      "thread_type": "convergence",
      "pattern_type": "The Convergence (Ê±áËÅöÊ®°Âºè)",
      "title": "Â§öÊäÄÊúØË∑ØÁ∫øÊ±áËÅöÂà∞ The Cancer Hallmarks Analytics Tool (CHAT) utilizes text mining algorithms to sy",
      "narrative": "**ËÉåÊôØ**  \nÂú®ÁôåÁóáÁ†îÁ©∂È¢ÜÂüüÔºåÈöèÁùÄÁßëÂ≠¶ÊñáÁåÆÁöÑÂø´ÈÄüÂ¢ûÈïøÔºåÂ¶Ç‰ΩïÊúâÊïàÁªÑÁªáÂíåÂàÜÊûêËøô‰∫õÊñáÁåÆÊàê‰∏∫‰∏ÄÈ°πÈáçË¶ÅÊåëÊàò„ÄÇ2011Âπ¥ÁöÑÁªèÂÖ∏ËÆ∫Êñá„ÄäHallmarks of Cancer: The Next Generation„ÄãÊõ¥Êñ∞Âπ∂Êâ©Â±ï‰∫ÜÁôåÁóáÁîüÁâ©Â≠¶ÁâπÂæÅÁöÑÊ°ÜÊû∂Ôºå‰∏∫ÁôåÁóáÁ†îÁ©∂Êèê‰æõ‰∫ÜÁªü‰∏ÄÁöÑÁêÜËÆ∫Âü∫Á°Ä„ÄÇÁÑ∂ËÄåÔºåÂ∞ΩÁÆ°Ëøô‰∏ÄÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÊåáÂØºÊÑè‰πâÔºåÂ¶Ç‰ΩïÂ∞ÜÂÖ∂Â∫îÁî®‰∫éÊµ∑ÈáèÁöÑÁßëÂ≠¶ÊñáÁåÆ‰∏≠‰ªçÁÑ∂Áº∫‰πèÂÖ∑‰ΩìÁöÑÊäÄÊúØÊâãÊÆµ„ÄÇÈöèÂêéÔºå2012Âπ¥ÁöÑÁ†îÁ©∂ÊèêÂá∫‰∫ÜÁîüÁâ©ÂåªÂ≠¶ÊñáÊú¨ÊåñÊéòÊäÄÊúØÔºåÂ±ïÁ§∫‰∫ÜËá™Âä®Â§ÑÁêÜÁôåÁóáÁõ∏ÂÖ≥ÊñáÁåÆÁöÑÊΩúÂäõÔºåËÄå2015Âπ¥ÁöÑÁ†îÁ©∂Ëøõ‰∏ÄÊ≠•ÂºÄÂèë‰∫Ü‰∏ÄÁßçËØ≠‰πâÂàÜÁ±ªÁ≥ªÁªüÔºåËÉΩÂ§üÊ†πÊçÆÁôåÁóáÁâπÂæÅÂØπÊñáÁåÆËøõË°åËá™Âä®ÂåñÂàÜÁ±ª„ÄÇËøô‰∫õÁ†îÁ©∂ÂàÜÂà´‰ªéÁêÜËÆ∫Ê°ÜÊû∂„ÄÅÊäÄÊúØÂ∑•ÂÖ∑ÂíåÂ∫îÁî®ÂÆûË∑µÁ≠â‰∏çÂêåÊñπÂêëÊé®Ëøõ‰∫ÜÈ¢ÜÂüüÁöÑÂèëÂ±ïÔºå‰ΩÜÂΩºÊ≠§‰πãÈó¥Áº∫‰πèÊúâÊú∫Êï¥Âêà„ÄÇ\n\n**Ê±áËÅö**  \n2017Âπ¥ÁöÑ‰∏≠ÂøÉËÆ∫Êñá„ÄäCancer Hallmarks Analytics Tool (CHAT)„ÄãÂú®Ê≠§ËÉåÊôØ‰∏ãÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàõÊñ∞ÊÄßÁöÑÊï¥ÂêàÊñπÊ≥ï„ÄÇCHATÂ∑•ÂÖ∑ÁªìÂêà‰∫ÜÁôåÁóáÁîüÁâ©Â≠¶ÁâπÂæÅÁöÑÁêÜËÆ∫Ê°ÜÊû∂‰∏éÂÖàËøõÁöÑÊñáÊú¨ÊåñÊéòÊäÄÊúØÔºåÂºÄÂèëÂá∫‰∏Ä‰∏™ËÉΩÂ§üËá™Âä®ÂàÜÁ±ªÂíåËØÑ‰º∞ÁôåÁóáÊñáÁåÆÁöÑÁ≥ªÁªü„ÄÇÈÄöËøáÊï¥Âêà2011Âπ¥ÊèêÂá∫ÁöÑÁôåÁóáÁâπÂæÅÊ°ÜÊû∂„ÄÅ2012Âπ¥ÁöÑÊñáÊú¨ÊåñÊéòÊäÄÊúØ‰ª•Âèä2015Âπ¥ÁöÑËØ≠‰πâÂàÜÁ±ªÊñπÊ≥ïÔºåCHATÂÆûÁé∞‰∫Ü‰ªéÁêÜËÆ∫Âà∞ÊäÄÊúØÂÜçÂà∞Â∫îÁî®ÁöÑÂÖ®ÈìæÊù°Êï¥ÂêàÔºå‰∏∫Á†îÁ©∂ËÄÖÊèê‰æõ‰∫Ü‰∏ÄÁßçÈ´òÊïàÁªÑÁªáÂíåÂàÜÊûêÁôåÁóáÊñáÁåÆÁöÑÊñ∞ÈÄîÂæÑ„ÄÇ\n\n**ÊÑè‰πâ**  \nËøôÁßçÊï¥Âêà‰∏ç‰ªÖÊòæËëóÊèêÂçá‰∫ÜÁôåÁóáÊñáÁåÆÂàÜÊûêÁöÑÊïàÁéáÔºåËøòÂú®ÁêÜËÆ∫ÂíåÂÆûË∑µÂ±ÇÈù¢‰∫ßÁîü‰∫ÜÊ∑±ËøúÂΩ±Âìç„ÄÇCHATÂ∑•ÂÖ∑ÁöÑÂºÄÂèë‰ΩøÂæóÁ†îÁ©∂ËÄÖËÉΩÂ§üÊõ¥Á≥ªÁªüÂú∞ÁêÜËß£ÁôåÁóáÁâπÂæÅ‰∏éÁõ∏ÂÖ≥ÊñáÁåÆ‰πãÈó¥ÁöÑÂÖ≥ËÅîÔºåÊé®Âä®‰∫ÜÁôåÁóáÁ†îÁ©∂‰ªéÂàÜÊï£ÁöÑ‰∏™‰ΩìÊé¢Á¥¢ÂêëÁ≥ªÁªüÂåñ„ÄÅÊï∞ÊçÆÈ©±Âä®ÁöÑÊñπÂêëËΩ¨Âèò„ÄÇÊ≠§Â§ñÔºåËøô‰∏ÄÊï¥ÂêàÊ°ÜÊû∂‰∏∫ÂÖ∂‰ªñÈ¢ÜÂüüÁöÑÊñáÁåÆÂàÜÊûêÊèê‰æõ‰∫ÜÂèÇËÄÉËåÉÂºèÔºåÂ±ïÁé∞‰∫ÜË∑®Â≠¶ÁßëÊñπÊ≥ïÂú®Ëß£ÂÜ≥Â§çÊùÇÁßëÂ≠¶ÈóÆÈ¢ò‰∏≠ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ",
      "center_paper": "Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer",
      "routes_count": 3,
      "routes": [
        {
          "relation_type": "Extends",
          "papers": [
            {
              "paper_id": "W2174775663",
              "title": "Automatic semantic classification of scientific literature according to the hallmarks of cancer",
              "year": 2015,
              "cited_by_count": 108
            }
          ]
        },
        {
          "relation_type": "Adapts_to",
          "papers": [
            {
              "paper_id": "W2117692326",
              "title": "Hallmarks of Cancer: The Next Generation",
              "year": 2011,
              "cited_by_count": 64282
            }
          ]
        },
        {
          "relation_type": "Extends",
          "papers": [
            {
              "paper_id": "W2032069669",
              "title": "Biomedical text mining and its applications in cancer research",
              "year": 2012,
              "cited_by_count": 245
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W2736047977",
          "title": "Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer",
          "year": 2017,
          "cited_by_count": 114,
          "role": "center"
        },
        {
          "paper_id": "W2174775663",
          "title": "Automatic semantic classification of scientific literature according to the hallmarks of cancer",
          "year": 2015,
          "cited_by_count": 108
        },
        {
          "paper_id": "W2117692326",
          "title": "Hallmarks of Cancer: The Next Generation",
          "year": 2011,
          "cited_by_count": 64282
        },
        {
          "paper_id": "W2032069669",
          "title": "Biomedical text mining and its applications in cancer research",
          "year": 2012,
          "cited_by_count": 245
        }
      ],
      "total_citations": 64749,
      "visual_structure": "3 Routes -> Center",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2174775663",
            "title": "Automatic semantic classification of scientific literature according to the hallmarks of cancer",
            "year": 2015
          },
          "to_paper": {
            "id": "W2736047977",
            "title": "Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer",
            "year": 2017
          },
          "relation_type": "Extends",
          "narrative_relation": "Ë¢´Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancerÊï¥Âêà",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2117692326",
            "title": "Hallmarks of Cancer: The Next Generation",
            "year": 2011
          },
          "to_paper": {
            "id": "W2736047977",
            "title": "Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer",
            "year": 2017
          },
          "relation_type": "Adapts_to",
          "narrative_relation": "Ë¢´Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancerÊï¥Âêà",
          "route_id": 2,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2032069669",
            "title": "Biomedical text mining and its applications in cancer research",
            "year": 2012
          },
          "to_paper": {
            "id": "W2736047977",
            "title": "Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer",
            "year": 2017
          },
          "relation_type": "Extends",
          "narrative_relation": "Ë¢´Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancerÊï¥Âêà",
          "route_id": 3,
          "direction": "chronological"
        }
      ],
      "component_id": 133298908837824
    },
    {
      "thread_type": "convergence",
      "pattern_type": "The Convergence (Ê±áËÅöÊ®°Âºè)",
      "title": "Â§öÊäÄÊúØË∑ØÁ∫øÊ±áËÅöÂà∞ Pretraining language models solely on domain-specific in-domain biomedical text ",
      "narrative": "**ËÉåÊôØ**  \nÂú®ÁîüÁâ©ÂåªÂ≠¶Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÔºåÂ§ö‰∏™Áã¨Á´ãÊñπÂêëÁöÑÁ†îÁ©∂Â∞ùËØïËß£ÂÜ≥È¢ÜÂüüÂÜÖ‰ªªÂä°ÁöÑË¥üËøÅÁßªÈóÆÈ¢ò„ÄÇËøô‰∫õÁ†îÁ©∂ÂåÖÊã¨ÂºÄÂèëÈ¢ÜÂüüÁâπÂÆöÁöÑÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÔºåÂ¶ÇBioBERTÂíåSciBERTÔºåÂÆÉ‰ª¨ÂàÜÂà´ÈÄöËøáÂú®ÁîüÁâ©ÂåªÂ≠¶ÊñáÊú¨ÂíåÁßëÂ≠¶ÊñáÊú¨‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÊù•ÊèêÂçáÈ¢ÜÂüüÂÜÖ‰ªªÂä°ÁöÑË°®Áé∞„ÄÇÊ≠§Â§ñÔºåÈíàÂØπ‰∏¥Â∫äÊñáÊú¨ÁöÑÊ®°ÂûãÔºåÂ¶ÇÂü∫‰∫éMIMICÊï∞ÊçÆÈõÜÁöÑÂµåÂÖ•ÊñπÊ≥ïÔºå‰ª•ÂèäBLUEÂü∫ÂáÜËØÑ‰º∞Ê°ÜÊû∂ÁöÑÊèêÂá∫Ôºå‰πü‰∏∫È¢ÜÂüüÂÜÖÊ®°ÂûãÊÄßËÉΩÁöÑËØÑ‰º∞Êèê‰æõ‰∫ÜÈáçË¶ÅÂèÇËÄÉ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÁ†îÁ©∂ËôΩÂêÑËá™ÂèñÂæóËøõÂ±ïÔºå‰ΩÜÁî±‰∫éÈ¢ÑËÆ≠ÁªÉËØ≠ÊñôÁöÑÂºÇË¥®ÊÄßÂíåÊñπÊ≥ïÁöÑÂàÜÊï£ÊÄßÔºå‰ªçÂ≠òÂú®Ê®°ÂûãËøÅÁßªÊïàÊûú‰∏ç‰Ω≥ÁöÑÈóÆÈ¢ò„ÄÇ\n\n**Ê±áËÅö**  \n‰∏≠ÂøÉËÆ∫Êñá„ÄäDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing„ÄãÊï¥Âêà‰∫Ü‰∏äËø∞Á†îÁ©∂ÊñπÂêëÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏ìÊ≥®‰∫éÈ¢ÜÂüüÁâπÂÆöÈ¢ÑËÆ≠ÁªÉÁöÑÁªü‰∏ÄÊ°ÜÊû∂„ÄÇËÆ∫ÊñáÈÄöËøá‰ªÖ‰ΩøÁî®ÁîüÁâ©ÂåªÂ≠¶È¢ÜÂüüÂÜÖÁöÑÊñáÊú¨ËøõË°åËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉÔºåÈÅøÂÖç‰∫ÜË∑®È¢ÜÂüüËØ≠ÊñôÂ∏¶Êù•ÁöÑË¥üËøÅÁßªÈóÆÈ¢ò„ÄÇËøôÁßçÊñπÊ≥ï‰∏ç‰ªÖÂÄüÈâ¥‰∫ÜBioBERTÂíåSciBERTÂú®È¢ÜÂüüÁâπÂÆöÈ¢ÑËÆ≠ÁªÉ‰∏≠ÁöÑÊàêÂäüÁªèÈ™åÔºåËøòÂê∏Êî∂‰∫Ü‰∏¥Â∫äÂµåÂÖ•ÊäÄÊúØÂíåBLUEÂü∫ÂáÜËØÑ‰º∞ÁöÑÊàêÊûúÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÁßçÊõ¥ÂÖ∑ÈíàÂØπÊÄßÂíåÈÄÇÂ∫îÊÄßÁöÑÊ®°ÂûãÂºÄÂèëÁ≠ñÁï•„ÄÇ\n\n**ÊÑè‰πâ**  \nËøôÁßçÊï¥Âêà‰∏∫ÁîüÁâ©ÂåªÂ≠¶Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÁöÑÂèëÂ±ïÂ∏¶Êù•‰∫ÜÈáçË¶ÅÁöÑÁêÜËÆ∫ÂíåÂÆûË∑µ‰ª∑ÂÄº„ÄÇÈÄöËøá‰∏ìÊ≥®‰∫éÈ¢ÜÂüüÁâπÂÆöËØ≠ÊñôÁöÑÈ¢ÑËÆ≠ÁªÉÔºå‰∏≠ÂøÉËÆ∫ÊñáÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂú®ÁîüÁâ©ÂåªÂ≠¶‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜË¥üËøÅÁßªÁöÑÈ£éÈô©„ÄÇËøôÁßçÁªü‰∏ÄÊ°ÜÊû∂‰∏ç‰ªÖ‰∏∫ÂêéÁª≠Á†îÁ©∂Êèê‰æõ‰∫ÜÊòéÁ°ÆÁöÑÊñπÂêëÔºåËøòÊé®Âä®‰∫ÜÈ¢ÜÂüüÁâπÂÆöËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ†áÂáÜÂåñÂíå‰ºòÂåñÔºå‰∏∫ÁîüÁâ©ÂåªÂ≠¶ÊñáÊú¨ÊåñÊéòÂíå‰∏¥Â∫ä‰ø°ÊÅØÂ§ÑÁêÜÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ïÂ•†ÂÆö‰∫ÜÂùöÂÆûÂü∫Á°Ä„ÄÇ",
      "center_paper": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
      "routes_count": 5,
      "routes": [
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W2963716420",
              "title": "Publicly Available Clinical",
              "year": 2019,
              "cited_by_count": 1422
            },
            {
              "paper_id": "W2911489562",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019,
              "cited_by_count": 6148
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W2970771982",
              "title": "SciBERT: A Pretrained Language Model for Scientific Text",
              "year": 2019,
              "cited_by_count": 2777
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W2911489562",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019,
              "cited_by_count": 6148
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W2971258845",
              "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
              "year": 2019,
              "cited_by_count": 797
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W2955483668",
              "title": "Enhancing clinical concept extraction with contextual embeddings",
              "year": 2019,
              "cited_by_count": 314
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W3046375318",
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
          "year": 2021,
          "cited_by_count": 1737,
          "role": "center"
        },
        {
          "paper_id": "W2963716420",
          "title": "Publicly Available Clinical",
          "year": 2019,
          "cited_by_count": 1422
        },
        {
          "paper_id": "W2911489562",
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "year": 2019,
          "cited_by_count": 6148
        },
        {
          "paper_id": "W2970771982",
          "title": "SciBERT: A Pretrained Language Model for Scientific Text",
          "year": 2019,
          "cited_by_count": 2777
        },
        {
          "paper_id": "W2911489562",
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "year": 2019,
          "cited_by_count": 6148
        },
        {
          "paper_id": "W2971258845",
          "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
          "year": 2019,
          "cited_by_count": 797
        },
        {
          "paper_id": "W2955483668",
          "title": "Enhancing clinical concept extraction with contextual embeddings",
          "year": 2019,
          "cited_by_count": 314
        }
      ],
      "total_citations": 19343,
      "visual_structure": "5 Routes -> Center",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2963716420",
            "title": "Publicly Available Clinical",
            "year": 2019
          },
          "to_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Ë¢´Domain-Specific Language Model Pretraining for Biomedical Natural Language ProcessingÊï¥Âêà",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2970771982",
            "title": "SciBERT: A Pretrained Language Model for Scientific Text",
            "year": 2019
          },
          "to_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Ë¢´Domain-Specific Language Model Pretraining for Biomedical Natural Language ProcessingÊï¥Âêà",
          "route_id": 2,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019
          },
          "to_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Ë¢´Domain-Specific Language Model Pretraining for Biomedical Natural Language ProcessingÊï¥Âêà",
          "route_id": 3,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2971258845",
            "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
            "year": 2019
          },
          "to_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Ë¢´Domain-Specific Language Model Pretraining for Biomedical Natural Language ProcessingÊï¥Âêà",
          "route_id": 4,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2955483668",
            "title": "Enhancing clinical concept extraction with contextual embeddings",
            "year": 2019
          },
          "to_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Ë¢´Domain-Specific Language Model Pretraining for Biomedical Natural Language ProcessingÊï¥Âêà",
          "route_id": 5,
          "direction": "chronological"
        }
      ],
      "component_id": 133298910465728
    },
    {
      "thread_type": "convergence",
      "pattern_type": "The Convergence (Ê±áËÅöÊ®°Âºè)",
      "title": "Â§öÊäÄÊúØË∑ØÁ∫øÊ±áËÅöÂà∞ The authors propose a method for learning multilingual distributed representatio",
      "narrative": "**ËÉåÊôØ**  \nÂú®Â§öËØ≠Ë®ÄÂµåÂÖ•Á†îÁ©∂È¢ÜÂüüÔºåÊó©ÊúüÁöÑÂ∑•‰Ωú‰∏ªË¶ÅÈõÜ‰∏≠‰∫éËß£ÂÜ≥Ë∑®ËØ≠Ë®ÄËØ≠‰πâÂØπÈΩêÁöÑÈóÆÈ¢ò„ÄÇ2008Âπ¥ÁöÑÁ†îÁ©∂„ÄäLearning Bilingual Lexicons from Monolingual Corpora„ÄãÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ªéÂçïËØ≠ËØ≠Êñô‰∏≠Â≠¶‰π†ÂèåËØ≠ËØçÊ±áË°®ÁöÑÊñπÊ≥ïÔºåËØïÂõæÂú®Êó†ÈúÄÊòéÁ°ÆÂØπÈΩêÁöÑÊÉÖÂÜµ‰∏ãÊûÑÂª∫Ë∑®ËØ≠Ë®ÄËÅîÁ≥ª„ÄÇÊ≠§Â§ñÔºå2010Âπ¥ÁöÑ„ÄäFrom Frequency to Meaning: Vector Space Models of Semantics„ÄãËøõ‰∏ÄÊ≠•ÂèëÂ±ï‰∫ÜÂü∫‰∫éÂêëÈáèÁ©∫Èó¥Ê®°ÂûãÔºàVSMÔºâÁöÑËØ≠‰πâË°®Á§∫ÊñπÊ≥ïÔºåÈÄöËøáÊï∞Â≠¶ÁªìÊûÑÊçïÊçâËØçÊ±áÈó¥ÁöÑËØ≠‰πâÂÖ≥Á≥ª„ÄÇ‰∏éÊ≠§ÂêåÊó∂Ôºå2013Âπ¥ÁöÑ„ÄäEfficient Estimation of Word Representations in Vector Space„ÄãÂºïÂÖ•‰∫ÜËøûÁª≠ËØçË¢ãÔºàCBoWÔºâÁ≠âÈ´òÊïàÊ®°ÂûãÊû∂ÊûÑÔºå‰∏∫ÂçïËØ≠ËØ≠Êñô‰∏≠ÁöÑËØçË°®Á§∫Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊäÄÊúØË∑ØÂæÑ„ÄÇËøô‰∫õÁ†îÁ©∂ËôΩÁÑ∂ÂêÑËá™Áã¨Á´ãÔºå‰ΩÜÈÉΩ‰∏∫Ë∑®ËØ≠Ë®ÄÂµåÂÖ•ÁöÑÊûÑÂª∫Êèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÁêÜËÆ∫ÂíåÊäÄÊúØÂü∫Á°Ä„ÄÇ\n\n**Ê±áËÅö**  \n„ÄäMultilingual Distributed Representations without Word Alignment„ÄãÂú®‰∏äËø∞Á†îÁ©∂ÁöÑÂü∫Á°Ä‰∏äÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÈúÄËØçÂØπÈΩêÁöÑÂ§öËØ≠Ë®ÄÂàÜÂ∏ÉÂºèË°®Á§∫Â≠¶‰π†ÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïÊï¥Âêà‰∫Ü‰ªéÂçïËØ≠ËØ≠Êñô‰∏≠Â≠¶‰π†ÂèåËØ≠ËØçÊ±áÂÖ≥Á≥ªÁöÑÁêÜÂøµÔºåÂêåÊó∂ÂÄüÈâ¥‰∫ÜÂêëÈáèÁ©∫Èó¥Ê®°ÂûãÁöÑËØ≠‰πâË°®Á§∫ËÉΩÂäõÔºåÂπ∂ÈÄÇÈÖç‰∫ÜËøûÁª≠ËØçË¢ãÁ≠âÈ´òÊïàÊû∂ÊûÑ„ÄÇÈÄöËøáËøôÁßçÊï¥ÂêàÔºå‰∏≠ÂøÉËÆ∫ÊñáÂÆûÁé∞‰∫ÜÂú®‰∏ç‰æùËµñÊòéÁ°ÆËØçÂØπÈΩêÁöÑÊÉÖÂÜµ‰∏ãÔºåÊûÑÂª∫Â§öËØ≠Ë®ÄÂµåÂÖ•ÁöÑÁªü‰∏ÄÊ°ÜÊû∂„ÄÇËøôÁßçÊñπÊ≥ï‰∏ç‰ªÖÂÖãÊúç‰∫Ü‰º†ÁªüÂ§öËØ≠Ë®ÄÂµåÂÖ•ÂØπËØçÂØπÈΩêÁöÑ‰æùËµñÔºåËøòÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÈÄÇÁî®ÊÄßÂíåÊâ©Â±ïÊÄß„ÄÇ\n\n**ÊÑè‰πâ**  \nËØ•Á†îÁ©∂ÁöÑÊï¥ÂêàÊÄßË¥°ÁåÆ‰∏∫Â§öËØ≠Ë®ÄÂµåÂÖ•È¢ÜÂüüÂ∏¶Êù•‰∫ÜÊ∑±ËøúÂΩ±Âìç„ÄÇ‰∏ÄÊñπÈù¢ÔºåÂÆÉÁ™ÅÁ†¥‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÊäÄÊúØÁì∂È¢àÔºå‰ΩøÂæóË∑®ËØ≠Ë®ÄÂµåÂÖ•ËÉΩÂ§üÂú®Êõ¥ÂπøÊ≥õÁöÑËØ≠ÊñôÂíåËØ≠Ë®ÄÁéØÂ¢É‰∏≠Â∫îÁî®ÔºõÂè¶‰∏ÄÊñπÈù¢ÔºåÂÆÉ‰∏∫Â§öËØ≠Ë®ÄËØ≠‰πâË°®Á§∫Êèê‰æõ‰∫ÜÊñ∞ÁöÑÁêÜËÆ∫ËßÜËßíÔºåÊé®Âä®‰∫ÜË∑®ËØ≠Ë®ÄËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°ÁöÑÂèëÂ±ï„ÄÇËøôÁßç‰ªéÂàÜÊï£Âà∞Êï¥ÂêàÁöÑÊºîËøõÔºå‰∏ç‰ªÖÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÁêÜËÆ∫ÂÆåÂ§áÊÄßÔºåËøò‰∏∫ÂêéÁª≠Á†îÁ©∂Êèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÊäÄÊúØÂü∫Áü≥ÔºåËøõ‰∏ÄÊ≠•‰øÉËøõ‰∫ÜËØ≠Ë®ÄÊäÄÊúØÁöÑÂÖ®ÁêÉÂåñÂ∫îÁî®‰∏éÂàõÊñ∞„ÄÇ",
      "center_paper": "Multilingual Distributed Representations without Word Alignment",
      "routes_count": 3,
      "routes": [
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W2140406733",
              "title": "Learning Bilingual Lexicons from Monolingual Corpora",
              "year": 2008,
              "cited_by_count": 313
            }
          ]
        },
        {
          "relation_type": "Adapts_to",
          "papers": [
            {
              "paper_id": "W2950577311",
              "title": "Efficient Estimation of Word Representations in Vector Space",
              "year": 2013,
              "cited_by_count": 11710
            }
          ]
        },
        {
          "relation_type": "Adapts_to",
          "papers": [
            {
              "paper_id": "W1662133657",
              "title": "From Frequency to Meaning: Vector Space Models of Semantics",
              "year": 2010,
              "cited_by_count": 2827
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W1562955078",
          "title": "Multilingual Distributed Representations without Word Alignment",
          "year": 2013,
          "cited_by_count": 67,
          "role": "center"
        },
        {
          "paper_id": "W2140406733",
          "title": "Learning Bilingual Lexicons from Monolingual Corpora",
          "year": 2008,
          "cited_by_count": 313
        },
        {
          "paper_id": "W2950577311",
          "title": "Efficient Estimation of Word Representations in Vector Space",
          "year": 2013,
          "cited_by_count": 11710
        },
        {
          "paper_id": "W1662133657",
          "title": "From Frequency to Meaning: Vector Space Models of Semantics",
          "year": 2010,
          "cited_by_count": 2827
        }
      ],
      "total_citations": 14917,
      "visual_structure": "3 Routes -> Center",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2140406733",
            "title": "Learning Bilingual Lexicons from Monolingual Corpora",
            "year": 2008
          },
          "to_paper": {
            "id": "W1562955078",
            "title": "Multilingual Distributed Representations without Word Alignment",
            "year": 2013
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Ë¢´Multilingual Distributed Representations without Word AlignmentÊï¥Âêà",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2950577311",
            "title": "Efficient Estimation of Word Representations in Vector Space",
            "year": 2013
          },
          "to_paper": {
            "id": "W1562955078",
            "title": "Multilingual Distributed Representations without Word Alignment",
            "year": 2013
          },
          "relation_type": "Adapts_to",
          "narrative_relation": "Ë¢´Multilingual Distributed Representations without Word AlignmentÊï¥Âêà",
          "route_id": 2,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W1662133657",
            "title": "From Frequency to Meaning: Vector Space Models of Semantics",
            "year": 2010
          },
          "to_paper": {
            "id": "W1562955078",
            "title": "Multilingual Distributed Representations without Word Alignment",
            "year": 2013
          },
          "relation_type": "Adapts_to",
          "narrative_relation": "Ë¢´Multilingual Distributed Representations without Word AlignmentÊï¥Âêà",
          "route_id": 3,
          "direction": "chronological"
        }
      ],
      "component_id": 133298908838592
    },
    {
      "thread_type": "divergence",
      "pattern_type": "The Divergence",
      "title": "ÈíàÂØπ The vast and dynamic nature of medical data, along with intricate domain-specifi ÁöÑÂ§öÊäÄÊúØË∑ØÁ∫øÂçöÂºà",
      "narrative": "### Focus  \n\"Large Language Models in Medicine (2023)\" serves as a cornerstone in the study of large language models (LLMs) in healthcare, being the first to systematically examine the challenges posed by the complexity and dynamism of medical data for LLM applications. The paper points out that, although LLMs have demonstrated potential in handling complex medical tasks, they still exhibit significant limitations regarding data privacy, domain adaptability, and performance reliability. Addressing these issues is not only crucial for the practical deployment of LLMs in medicine but also provides clear directions for future research.\n\n### Divergence  \nIn response to the challenges raised in the central paper, different research pathways have adopted distinct technical approaches. **Pathway 1** focuses on the safety and privacy of LLMs, enhancing model robustness through optimization methods such as adversarial training; **Pathway 2** concentrates on domain adaptability, proposing techniques like in-context learning to enable LLMs to surpass human experts in clinical text summarization tasks; **Pathway 3** aims to overcome the limitations of LLMs in clinical decision-making by developing an evaluation and refinement framework to improve model reliability; **Pathway 4** expands the application scenarios of LLMs, exploring their utility in radiology and proposing concrete implementation schemes based on models such as ChatGPT. These research directions collectively illustrate the diverse potential of LLMs in the medical field.\n\n### Comparison  \nThe aforementioned research pathways differ in their technical objectives and innovation emphases, yet also share certain intersections. **Pathway 1** and **Pathway 3** both address LLM limitations, but the former emphasizes privacy protection and security, while the latter focuses more on model performance evaluation and improvement; **Pathway 2** and **Pathway 4** place greater emphasis on the practical applicability of LLMs‚Äî the former enhances model performance in specific tasks through technical adaptation, while the latter explores broader clinical and research contexts. Overall, these studies jointly advance the technological progress of LLMs in medicine and offer multidimensional solutions to the unresolved issues highlighted in the central paper.",
      "center_paper": "Large language models in medicine",
      "routes_count": 4,
      "routes": [
        {
          "relation_type": "Realizes",
          "papers": [
            {
              "paper_id": "W4392353733",
              "title": "A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly",
              "year": 2024,
              "cited_by_count": 596
            }
          ]
        },
        {
          "relation_type": "Adapts_to",
          "papers": [
            {
              "paper_id": "W4392193048",
              "title": "Adapted large language models can outperform medical experts in clinical text summarization",
              "year": 2024,
              "cited_by_count": 443
            }
          ]
        },
        {
          "relation_type": "Overcomes",
          "papers": [
            {
              "paper_id": "W4400324908",
              "title": "Evaluation and mitigation of the limitations of large language models in clinical decision-making",
              "year": 2024,
              "cited_by_count": 329
            }
          ]
        },
        {
          "relation_type": "Extends",
          "papers": [
            {
              "paper_id": "W4390919701",
              "title": "Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications",
              "year": 2024,
              "cited_by_count": 227
            }
          ]
        }
      ],
      "papers": [
        {
          "paper_id": "W4384561707",
          "title": "Large language models in medicine",
          "year": 2023,
          "cited_by_count": 2502,
          "role": "center"
        },
        {
          "paper_id": "W4392353733",
          "title": "A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly",
          "year": 2024,
          "cited_by_count": 596
        },
        {
          "paper_id": "W4392193048",
          "title": "Adapted large language models can outperform medical experts in clinical text summarization",
          "year": 2024,
          "cited_by_count": 443
        },
        {
          "paper_id": "W4400324908",
          "title": "Evaluation and mitigation of the limitations of large language models in clinical decision-making",
          "year": 2024,
          "cited_by_count": 329
        },
        {
          "paper_id": "W4390919701",
          "title": "Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications",
          "year": 2024,
          "cited_by_count": 227
        }
      ],
      "total_citations": 4097,
      "visual_structure": "Center -> 4 Routes",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W4384561707",
            "title": "Large language models in medicine",
            "year": 2023
          },
          "to_paper": {
            "id": "W4392353733",
            "title": "A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly",
            "year": 2024
          },
          "relation_type": "Realizes",
          "narrative_relation": "Inspired",
          "route_id": 1,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W4384561707",
            "title": "Large language models in medicine",
            "year": 2023
          },
          "to_paper": {
            "id": "W4392193048",
            "title": "Adapted large language models can outperform medical experts in clinical text summarization",
            "year": 2024
          },
          "relation_type": "Adapts_to",
          "narrative_relation": "Was_Adapted_By",
          "route_id": 2,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W4384561707",
            "title": "Large language models in medicine",
            "year": 2023
          },
          "to_paper": {
            "id": "W4400324908",
            "title": "Evaluation and mitigation of the limitations of large language models in clinical decision-making",
            "year": 2024
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "route_id": 3,
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W4384561707",
            "title": "Large language models in medicine",
            "year": 2023
          },
          "to_paper": {
            "id": "W4390919701",
            "title": "Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications",
            "year": 2024
          },
          "relation_type": "Extends",
          "narrative_relation": "Was_Extended_By",
          "route_id": 4,
          "direction": "chronological"
        }
      ],
      "component_id": 133298910464832
    },
    {
      "thread_type": "chain",
      "pattern_type": "The Chain (Á∫øÊÄßÈìæÊù°)",
      "title": "‰ªé Creation and public release of domain-specific BERT models trained on clinical t Âà∞ Introduction of MultiMedQA benchmark, which combines multiple medical question-a ÁöÑÊºîËøõ‰πãË∑Ø",
      "narrative": "**Ëµ∑Ê∫ê**  \n2019Âπ¥ÔºåÁ¨¨‰∏ÄÁØáËÆ∫Êñá„ÄäPublicly Available Clinical„ÄãÂºÄÂàõ‰∫ÜÈ¢ÜÂüüÁâπÂÆöËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏¥Â∫äÊñáÊú¨Â§ÑÁêÜ‰∏≠ÁöÑÂ∫îÁî®ÊñπÂêë„ÄÇÁ†îÁ©∂ËÄÖËØÜÂà´Âà∞ÈÄöÁî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºàÂ¶ÇBERTÔºâÂú®‰∏¥Â∫äÈ¢ÜÂüüË°®Áé∞‰∏ç‰Ω≥ÁöÑÈóÆÈ¢òÔºåÊèêÂá∫Âπ∂ÂÖ¨ÂºÄ‰∫Ü‰∏ìÈó®ÈíàÂØπ‰∏¥Â∫äÊñáÊú¨ËÆ≠ÁªÉÁöÑBERTÊ®°Âûã„ÄÇÁÑ∂ËÄåÔºåËØ•ÊñπÊ≥ïÂú®ÂéªËØÜÂà´Âåñ‰ªªÂä°ÔºàÂ¶Çi2b2 2006Âíåi2b2 2014Ôºâ‰∏≠Ë°®Áé∞‰∏çÁêÜÊÉ≥ÔºåÂêåÊó∂Êú™ËÉΩÂÖÖÂàÜÊé¢Á¥¢ÂêàÊàêPHIÔºà‰∏™‰∫∫ÂÅ•Â∫∑‰ø°ÊÅØÔºâÊé©Á†ÅÂØπ‰∏ä‰∏ãÊñáÂµåÂÖ•Ê®°ÂûãÁöÑÂΩ±Âìç„ÄÇËøô‰∫õÂ±ÄÈôêÊÄß‰∏∫ÂêéÁª≠Á†îÁ©∂Êèê‰æõ‰∫ÜÊîπËøõÁ©∫Èó¥„ÄÇ\n\n**ÊºîËøõ**  \nÂêåÂπ¥ÔºåÁ¨¨‰∫åÁØáËÆ∫Êñá„ÄäBioBERT: a pre-trained biomedical language representation model for biomedical text mining„ÄãÈíàÂØπÈÄöÁî®ËØ≠Ë®ÄÊ®°ÂûãÂú®ÁîüÁâ©ÂåªÂ≠¶ÊñáÊú¨ÊåñÊéò‰∏≠ÁöÑË°®Áé∞‰∏çË∂≥ÈóÆÈ¢òÔºåÂºÄÂèë‰∫ÜBioBERTÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂú®ÁîüÁâ©ÂåªÂ≠¶È¢ÜÂüüÊñáÊú¨‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÊòæËëóÊèêÂçá‰∫Ü‰ªªÂä°ÊÄßËÉΩÔºåÂÖãÊúç‰∫ÜÈÄöÁî®Ê®°ÂûãÁöÑÂ±ÄÈôêÊÄß„ÄÇÁÑ∂ËÄåÔºåBioBERTÁöÑÈ¢ÑËÆ≠ÁªÉËøáÁ®ãÈúÄË¶ÅÂ§ßÈáèËÆ°ÁÆóËµÑÊ∫êÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂ÂπøÊ≥õÂ∫îÁî®„ÄÇ2021Âπ¥ÔºåÁ¨¨‰∏âÁØáËÆ∫Êñá„ÄäDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing„ÄãËøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫ÜÈ¢ÜÂüüÁâπÂÆöÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊèêÂá∫‰ªÖÂú®ÁîüÁâ©ÂåªÂ≠¶È¢ÜÂüüÊñáÊú¨‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºå‰ª•ÂáèÂ∞ëË¥üËøÅÁßªÈóÆÈ¢ò„ÄÇËøô‰∏ÄÊñπÊ≥ïÂÖãÊúç‰∫ÜBioBERTÁöÑÈÉ®ÂàÜÂ±ÄÈôêÊÄßÔºå‰ΩÜ‰ªçÁº∫‰πèÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó†Ê≥ïÂÖÖÂàÜÈ™åËØÅÂÖ∂ÂØπÂ§öÊ†∑Âåñ‰ªªÂä°ÁöÑÈÄÇÁî®ÊÄß„ÄÇ\n\n**ÊúÄÊñ∞ËøõÂ±ï**  \n2023Âπ¥ÔºåÁ¨¨ÂõõÁØáËÆ∫Êñá„ÄäThe future landscape of large language models in medicine„ÄãÊé¢ËÆ®‰∫ÜÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂåªÂ≠¶È¢ÜÂüüÁöÑÊΩúÂäõÔºåÂêåÊó∂ÊåáÂá∫ÂÖ∂Âú®ÂáÜÁ°ÆÊÄßÂíåÂÅèÂ∑ÆÊéßÂà∂ÊñπÈù¢ÁöÑÊåëÊàò„ÄÇÁ†îÁ©∂ËÄÖÈÄöËøáÂºïÂÖ•‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†ÂíåÊåÅÁª≠ÂæÆË∞ÉÊú∫Âà∂Ôºå‰∏∫ÂêéÁª≠Á†îÁ©∂Â•†ÂÆö‰∫ÜÊäÄÊúØÂü∫ÂáÜ„ÄÇÂêåÂπ¥ÔºåÁ¨¨‰∫îÁØáËÆ∫Êñá„ÄäLarge language models encode clinical knowledge„ÄãÂú®Ê≠§Âü∫Á°Ä‰∏äÂèñÂæó‰∫ÜÁ™ÅÁ†¥ÔºåÊèêÂá∫‰∫ÜÁªºÂêàÊÄßÁöÑMultiMedQAÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞LLMsÁöÑ‰∏¥Â∫äÁü•ËØÜË°®Áé∞„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏ç‰ªÖÂÖãÊúç‰∫ÜÂâç‰∫∫Á†îÁ©∂‰∏≠ËØÑ‰º∞ËåÉÂõ¥ÊúâÈôêÁöÑÈóÆÈ¢òÔºåËøòÈÄöËøáË∑®Â≠¶ÁßëÂêà‰ΩúÊé®Âä®‰∫ÜAIÂú®ÂåªÁñóÈ¢ÜÂüüÁöÑË¥üË¥£‰ªªÂ∫îÁî®„ÄÇÁÑ∂ËÄåÔºåÂ¶Ç‰ΩïÁ°Æ‰øùÊ®°ÂûãÂú®ÂÆâÂÖ®ÂÖ≥ÈîÆ‰ªªÂä°‰∏≠ÁöÑÂõûÁ≠îË¥®Èáè‰ªçÊòØÊú™Êù•Á†îÁ©∂ÁöÑÈáçË¶ÅÊñπÂêë„ÄÇ",
      "papers": [
        {
          "paper_id": "W2963716420",
          "title": "Publicly Available Clinical",
          "year": 2019,
          "cited_by_count": 1422
        },
        {
          "paper_id": "W2911489562",
          "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
          "year": 2019,
          "cited_by_count": 6148
        },
        {
          "paper_id": "W3046375318",
          "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
          "year": 2021,
          "cited_by_count": 1737
        },
        {
          "paper_id": "W4387500346",
          "title": "The future landscape of large language models in medicine",
          "year": 2023,
          "cited_by_count": 732
        },
        {
          "paper_id": "W4384071683",
          "title": "Large language models encode clinical knowledge",
          "year": 2023,
          "cited_by_count": 2248
        }
      ],
      "total_citations": 12287,
      "visual_structure": "Paper_1 -> Paper_2 -> Paper_3 -> Paper_4 -> Paper_5",
      "relation_chain": [
        {
          "from_paper": {
            "id": "W2963716420",
            "title": "Publicly Available Clinical",
            "year": 2019
          },
          "to_paper": {
            "id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Overcomes",
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019
          },
          "to_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Was_Overcome_By",
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021
          },
          "to_paper": {
            "id": "W4387500346",
            "title": "The future landscape of large language models in medicine",
            "year": 2023
          },
          "relation_type": "Temporal_Evolution",
          "narrative_relation": "Temporal_Evolution",
          "direction": "chronological"
        },
        {
          "from_paper": {
            "id": "W4387500346",
            "title": "The future landscape of large language models in medicine",
            "year": 2023
          },
          "to_paper": {
            "id": "W4384071683",
            "title": "Large language models encode clinical knowledge",
            "year": 2023
          },
          "relation_type": "Overcomes",
          "narrative_relation": "Overcomes",
          "direction": "chronological"
        }
      ],
      "component_id": 133298910465728
    }
  ],
  "survey_report": {
    "title": "Deep Survey: Natural Language Processing",
    "abstract": "Êú¨ÁªºËø∞Âü∫‰∫éÁü•ËØÜÂõæË∞±ÂàÜÊûê‰∫Ü Natural Language Processing È¢ÜÂüüÁöÑÊºîËøõÂéÜÁ®ã„ÄÇÈÄöËøáÂÖ≥Á≥ªÂâ™ÊûùÔºåÊàë‰ª¨‰ªéÂéüÂßãÂõæË∞±‰∏≠Á≠õÈÄâÂá∫ 69 ÁØáÈ´òË¥®ÈáèËÆ∫ÊñáÔºåÂπ∂ËØÜÂà´Âá∫ 5 Êù°ÂÖ≥ÈîÆÊºîÂåñË∑ØÂæÑ„ÄÇÂÖ∂‰∏≠ÂåÖÊã¨ 1 Êù°Á∫øÊÄßÊäÄÊúØÈìæÊù°„ÄÅ1 ‰∏™ÂàÜÂåñÁªìÊûÑÂíå 3 ‰∏™Ê±áËÅöÁªìÊûÑÔºåÂÆåÊï¥ÂëàÁé∞‰∫ÜËØ•È¢ÜÂüüÁöÑÊäÄÊúØÊºîËøõËÑâÁªú„ÄÅÂàÜÂåñË∂ãÂäøÂíåÊï¥ÂêàÊ®°Âºè„ÄÇ",
    "threads": [
      {
        "thread_id": 1,
        "thread_name": "Thread 1: The Convergence (Ê±áËÅöÊ®°Âºè)",
        "title": "Â§öÊäÄÊúØË∑ØÁ∫øÊ±áËÅöÂà∞ The Cancer Hallmarks Analytics Tool (CHAT) utilizes text mining algorithms to sy",
        "pattern_type": "The Convergence (Ê±áËÅöÊ®°Âºè)",
        "thread_type": "convergence",
        "narrative": "**ËÉåÊôØ**  \nÂú®ÁôåÁóáÁ†îÁ©∂È¢ÜÂüüÔºåÈöèÁùÄÁßëÂ≠¶ÊñáÁåÆÁöÑÂø´ÈÄüÂ¢ûÈïøÔºåÂ¶Ç‰ΩïÊúâÊïàÁªÑÁªáÂíåÂàÜÊûêËøô‰∫õÊñáÁåÆÊàê‰∏∫‰∏ÄÈ°πÈáçË¶ÅÊåëÊàò„ÄÇ2011Âπ¥ÁöÑÁªèÂÖ∏ËÆ∫Êñá„ÄäHallmarks of Cancer: The Next Generation„ÄãÊõ¥Êñ∞Âπ∂Êâ©Â±ï‰∫ÜÁôåÁóáÁîüÁâ©Â≠¶ÁâπÂæÅÁöÑÊ°ÜÊû∂Ôºå‰∏∫ÁôåÁóáÁ†îÁ©∂Êèê‰æõ‰∫ÜÁªü‰∏ÄÁöÑÁêÜËÆ∫Âü∫Á°Ä„ÄÇÁÑ∂ËÄåÔºåÂ∞ΩÁÆ°Ëøô‰∏ÄÊ°ÜÊû∂ÂÖ∑ÊúâÂπøÊ≥õÁöÑÊåáÂØºÊÑè‰πâÔºåÂ¶Ç‰ΩïÂ∞ÜÂÖ∂Â∫îÁî®‰∫éÊµ∑ÈáèÁöÑÁßëÂ≠¶ÊñáÁåÆ‰∏≠‰ªçÁÑ∂Áº∫‰πèÂÖ∑‰ΩìÁöÑÊäÄÊúØÊâãÊÆµ„ÄÇÈöèÂêéÔºå2012Âπ¥ÁöÑÁ†îÁ©∂ÊèêÂá∫‰∫ÜÁîüÁâ©ÂåªÂ≠¶ÊñáÊú¨ÊåñÊéòÊäÄÊúØÔºåÂ±ïÁ§∫‰∫ÜËá™Âä®Â§ÑÁêÜÁôåÁóáÁõ∏ÂÖ≥ÊñáÁåÆÁöÑÊΩúÂäõÔºåËÄå2015Âπ¥ÁöÑÁ†îÁ©∂Ëøõ‰∏ÄÊ≠•ÂºÄÂèë‰∫Ü‰∏ÄÁßçËØ≠‰πâÂàÜÁ±ªÁ≥ªÁªüÔºåËÉΩÂ§üÊ†πÊçÆÁôåÁóáÁâπÂæÅÂØπÊñáÁåÆËøõË°åËá™Âä®ÂåñÂàÜÁ±ª„ÄÇËøô‰∫õÁ†îÁ©∂ÂàÜÂà´‰ªéÁêÜËÆ∫Ê°ÜÊû∂„ÄÅÊäÄÊúØÂ∑•ÂÖ∑ÂíåÂ∫îÁî®ÂÆûË∑µÁ≠â‰∏çÂêåÊñπÂêëÊé®Ëøõ‰∫ÜÈ¢ÜÂüüÁöÑÂèëÂ±ïÔºå‰ΩÜÂΩºÊ≠§‰πãÈó¥Áº∫‰πèÊúâÊú∫Êï¥Âêà„ÄÇ\n\n**Ê±áËÅö**  \n2017Âπ¥ÁöÑ‰∏≠ÂøÉËÆ∫Êñá„ÄäCancer Hallmarks Analytics Tool (CHAT)„ÄãÂú®Ê≠§ËÉåÊôØ‰∏ãÊèêÂá∫‰∫Ü‰∏ÄÁßçÂàõÊñ∞ÊÄßÁöÑÊï¥ÂêàÊñπÊ≥ï„ÄÇCHATÂ∑•ÂÖ∑ÁªìÂêà‰∫ÜÁôåÁóáÁîüÁâ©Â≠¶ÁâπÂæÅÁöÑÁêÜËÆ∫Ê°ÜÊû∂‰∏éÂÖàËøõÁöÑÊñáÊú¨ÊåñÊéòÊäÄÊúØÔºåÂºÄÂèëÂá∫‰∏Ä‰∏™ËÉΩÂ§üËá™Âä®ÂàÜÁ±ªÂíåËØÑ‰º∞ÁôåÁóáÊñáÁåÆÁöÑÁ≥ªÁªü„ÄÇÈÄöËøáÊï¥Âêà2011Âπ¥ÊèêÂá∫ÁöÑÁôåÁóáÁâπÂæÅÊ°ÜÊû∂„ÄÅ2012Âπ¥ÁöÑÊñáÊú¨ÊåñÊéòÊäÄÊúØ‰ª•Âèä2015Âπ¥ÁöÑËØ≠‰πâÂàÜÁ±ªÊñπÊ≥ïÔºåCHATÂÆûÁé∞‰∫Ü‰ªéÁêÜËÆ∫Âà∞ÊäÄÊúØÂÜçÂà∞Â∫îÁî®ÁöÑÂÖ®ÈìæÊù°Êï¥ÂêàÔºå‰∏∫Á†îÁ©∂ËÄÖÊèê‰æõ‰∫Ü‰∏ÄÁßçÈ´òÊïàÁªÑÁªáÂíåÂàÜÊûêÁôåÁóáÊñáÁåÆÁöÑÊñ∞ÈÄîÂæÑ„ÄÇ\n\n**ÊÑè‰πâ**  \nËøôÁßçÊï¥Âêà‰∏ç‰ªÖÊòæËëóÊèêÂçá‰∫ÜÁôåÁóáÊñáÁåÆÂàÜÊûêÁöÑÊïàÁéáÔºåËøòÂú®ÁêÜËÆ∫ÂíåÂÆûË∑µÂ±ÇÈù¢‰∫ßÁîü‰∫ÜÊ∑±ËøúÂΩ±Âìç„ÄÇCHATÂ∑•ÂÖ∑ÁöÑÂºÄÂèë‰ΩøÂæóÁ†îÁ©∂ËÄÖËÉΩÂ§üÊõ¥Á≥ªÁªüÂú∞ÁêÜËß£ÁôåÁóáÁâπÂæÅ‰∏éÁõ∏ÂÖ≥ÊñáÁåÆ‰πãÈó¥ÁöÑÂÖ≥ËÅîÔºåÊé®Âä®‰∫ÜÁôåÁóáÁ†îÁ©∂‰ªéÂàÜÊï£ÁöÑ‰∏™‰ΩìÊé¢Á¥¢ÂêëÁ≥ªÁªüÂåñ„ÄÅÊï∞ÊçÆÈ©±Âä®ÁöÑÊñπÂêëËΩ¨Âèò„ÄÇÊ≠§Â§ñÔºåËøô‰∏ÄÊï¥ÂêàÊ°ÜÊû∂‰∏∫ÂÖ∂‰ªñÈ¢ÜÂüüÁöÑÊñáÁåÆÂàÜÊûêÊèê‰æõ‰∫ÜÂèÇËÄÉËåÉÂºèÔºåÂ±ïÁé∞‰∫ÜË∑®Â≠¶ÁßëÊñπÊ≥ïÂú®Ëß£ÂÜ≥Â§çÊùÇÁßëÂ≠¶ÈóÆÈ¢ò‰∏≠ÁöÑÂ∑®Â§ßÊΩúÂäõ„ÄÇ",
        "papers": [
          {
            "paper_id": "W2736047977",
            "title": "Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer",
            "year": 2017,
            "cited_by_count": 114,
            "role": "center"
          },
          {
            "paper_id": "W2174775663",
            "title": "Automatic semantic classification of scientific literature according to the hallmarks of cancer",
            "year": 2015,
            "cited_by_count": 108
          },
          {
            "paper_id": "W2117692326",
            "title": "Hallmarks of Cancer: The Next Generation",
            "year": 2011,
            "cited_by_count": 64282
          },
          {
            "paper_id": "W2032069669",
            "title": "Biomedical text mining and its applications in cancer research",
            "year": 2012,
            "cited_by_count": 245
          }
        ],
        "total_citations": 64749,
        "visual_structure": "3 Routes -> Center",
        "relation_stats": {
          "total_relations": 3,
          "relation_distribution": {
            "Extends": 2,
            "Adapts_to": 1
          },
          "dominant_relation": "Extends"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2174775663",
              "title": "Automatic semantic classification of scientific literature according to the hallmarks of cancer",
              "year": 2015
            },
            "to_paper": {
              "id": "W2736047977",
              "title": "Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer",
              "year": 2017
            },
            "relation_type": "Extends",
            "narrative_relation": "Ë¢´Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancerÊï¥Âêà",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2117692326",
              "title": "Hallmarks of Cancer: The Next Generation",
              "year": 2011
            },
            "to_paper": {
              "id": "W2736047977",
              "title": "Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer",
              "year": 2017
            },
            "relation_type": "Adapts_to",
            "narrative_relation": "Ë¢´Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancerÊï¥Âêà",
            "route_id": 2,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2032069669",
              "title": "Biomedical text mining and its applications in cancer research",
              "year": 2012
            },
            "to_paper": {
              "id": "W2736047977",
              "title": "Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer",
              "year": 2017
            },
            "relation_type": "Extends",
            "narrative_relation": "Ë¢´Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancerÊï¥Âêà",
            "route_id": 3,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2736047977",
              "label": "Center",
              "title": "Cancer Hallmarks Analytics Tool (CHAT): a text mining approach to organize and evaluate scientific literature on cancer",
              "year": 2017,
              "citations": 114,
              "role": "center"
            },
            {
              "id": "W2174775663",
              "label": "Automatic semantic classificat...",
              "title": "Automatic semantic classification of scientific literature according to the hallmarks of cancer",
              "year": 2015,
              "citations": 108
            },
            {
              "id": "W2117692326",
              "label": "Hallmarks of Cancer: The Next ...",
              "title": "Hallmarks of Cancer: The Next Generation",
              "year": 2011,
              "citations": 64282
            },
            {
              "id": "W2032069669",
              "label": "Biomedical text mining and its...",
              "title": "Biomedical text mining and its applications in cancer research",
              "year": 2012,
              "citations": 245
            }
          ],
          "edges": [
            {
              "source": "W2174775663",
              "target": "W2736047977",
              "type": "Extends",
              "direction": "forward"
            },
            {
              "source": "W2117692326",
              "target": "W2736047977",
              "type": "Adapts_to",
              "direction": "forward"
            },
            {
              "source": "W2032069669",
              "target": "W2736047977",
              "type": "Extends",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "ÁÆ≠Â§¥ÊñπÂêëË°®Á§∫Êó∂Èó¥ÊºîËøõÊñπÂêëÔºàÊó©Âπ¥‰ªΩ ‚Üí ÊôöÂπ¥‰ªΩÔºâ",
          "pattern_note": "divergence=‰∏≠ÂøÉÊâ©Êï£, convergence=Â§öÊ∫êÊ±áËÅö"
        }
      },
      {
        "thread_id": 2,
        "thread_name": "Thread 2: The Convergence (Ê±áËÅöÊ®°Âºè)",
        "title": "Â§öÊäÄÊúØË∑ØÁ∫øÊ±áËÅöÂà∞ Pretraining language models solely on domain-specific in-domain biomedical text ",
        "pattern_type": "The Convergence (Ê±áËÅöÊ®°Âºè)",
        "thread_type": "convergence",
        "narrative": "**ËÉåÊôØ**  \nÂú®ÁîüÁâ©ÂåªÂ≠¶Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÔºåÂ§ö‰∏™Áã¨Á´ãÊñπÂêëÁöÑÁ†îÁ©∂Â∞ùËØïËß£ÂÜ≥È¢ÜÂüüÂÜÖ‰ªªÂä°ÁöÑË¥üËøÅÁßªÈóÆÈ¢ò„ÄÇËøô‰∫õÁ†îÁ©∂ÂåÖÊã¨ÂºÄÂèëÈ¢ÜÂüüÁâπÂÆöÁöÑÈ¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÔºåÂ¶ÇBioBERTÂíåSciBERTÔºåÂÆÉ‰ª¨ÂàÜÂà´ÈÄöËøáÂú®ÁîüÁâ©ÂåªÂ≠¶ÊñáÊú¨ÂíåÁßëÂ≠¶ÊñáÊú¨‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÊù•ÊèêÂçáÈ¢ÜÂüüÂÜÖ‰ªªÂä°ÁöÑË°®Áé∞„ÄÇÊ≠§Â§ñÔºåÈíàÂØπ‰∏¥Â∫äÊñáÊú¨ÁöÑÊ®°ÂûãÔºåÂ¶ÇÂü∫‰∫éMIMICÊï∞ÊçÆÈõÜÁöÑÂµåÂÖ•ÊñπÊ≥ïÔºå‰ª•ÂèäBLUEÂü∫ÂáÜËØÑ‰º∞Ê°ÜÊû∂ÁöÑÊèêÂá∫Ôºå‰πü‰∏∫È¢ÜÂüüÂÜÖÊ®°ÂûãÊÄßËÉΩÁöÑËØÑ‰º∞Êèê‰æõ‰∫ÜÈáçË¶ÅÂèÇËÄÉ„ÄÇÁÑ∂ËÄåÔºåËøô‰∫õÁ†îÁ©∂ËôΩÂêÑËá™ÂèñÂæóËøõÂ±ïÔºå‰ΩÜÁî±‰∫éÈ¢ÑËÆ≠ÁªÉËØ≠ÊñôÁöÑÂºÇË¥®ÊÄßÂíåÊñπÊ≥ïÁöÑÂàÜÊï£ÊÄßÔºå‰ªçÂ≠òÂú®Ê®°ÂûãËøÅÁßªÊïàÊûú‰∏ç‰Ω≥ÁöÑÈóÆÈ¢ò„ÄÇ\n\n**Ê±áËÅö**  \n‰∏≠ÂøÉËÆ∫Êñá„ÄäDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing„ÄãÊï¥Âêà‰∫Ü‰∏äËø∞Á†îÁ©∂ÊñπÂêëÁöÑÊ†∏ÂøÉÊÄùÊÉ≥ÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßç‰∏ìÊ≥®‰∫éÈ¢ÜÂüüÁâπÂÆöÈ¢ÑËÆ≠ÁªÉÁöÑÁªü‰∏ÄÊ°ÜÊû∂„ÄÇËÆ∫ÊñáÈÄöËøá‰ªÖ‰ΩøÁî®ÁîüÁâ©ÂåªÂ≠¶È¢ÜÂüüÂÜÖÁöÑÊñáÊú¨ËøõË°åËØ≠Ë®ÄÊ®°ÂûãÈ¢ÑËÆ≠ÁªÉÔºåÈÅøÂÖç‰∫ÜË∑®È¢ÜÂüüËØ≠ÊñôÂ∏¶Êù•ÁöÑË¥üËøÅÁßªÈóÆÈ¢ò„ÄÇËøôÁßçÊñπÊ≥ï‰∏ç‰ªÖÂÄüÈâ¥‰∫ÜBioBERTÂíåSciBERTÂú®È¢ÜÂüüÁâπÂÆöÈ¢ÑËÆ≠ÁªÉ‰∏≠ÁöÑÊàêÂäüÁªèÈ™åÔºåËøòÂê∏Êî∂‰∫Ü‰∏¥Â∫äÂµåÂÖ•ÊäÄÊúØÂíåBLUEÂü∫ÂáÜËØÑ‰º∞ÁöÑÊàêÊûúÔºåÂΩ¢Êàê‰∫Ü‰∏ÄÁßçÊõ¥ÂÖ∑ÈíàÂØπÊÄßÂíåÈÄÇÂ∫îÊÄßÁöÑÊ®°ÂûãÂºÄÂèëÁ≠ñÁï•„ÄÇ\n\n**ÊÑè‰πâ**  \nËøôÁßçÊï¥Âêà‰∏∫ÁîüÁâ©ÂåªÂ≠¶Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÈ¢ÜÂüüÁöÑÂèëÂ±ïÂ∏¶Êù•‰∫ÜÈáçË¶ÅÁöÑÁêÜËÆ∫ÂíåÂÆûË∑µ‰ª∑ÂÄº„ÄÇÈÄöËøá‰∏ìÊ≥®‰∫éÈ¢ÜÂüüÁâπÂÆöËØ≠ÊñôÁöÑÈ¢ÑËÆ≠ÁªÉÔºå‰∏≠ÂøÉËÆ∫ÊñáÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÂú®ÁîüÁâ©ÂåªÂ≠¶‰ªªÂä°‰∏≠ÁöÑË°®Áé∞ÔºåÂêåÊó∂ÂáèÂ∞ë‰∫ÜË¥üËøÅÁßªÁöÑÈ£éÈô©„ÄÇËøôÁßçÁªü‰∏ÄÊ°ÜÊû∂‰∏ç‰ªÖ‰∏∫ÂêéÁª≠Á†îÁ©∂Êèê‰æõ‰∫ÜÊòéÁ°ÆÁöÑÊñπÂêëÔºåËøòÊé®Âä®‰∫ÜÈ¢ÜÂüüÁâπÂÆöËØ≠Ë®ÄÊ®°ÂûãÁöÑÊ†áÂáÜÂåñÂíå‰ºòÂåñÔºå‰∏∫ÁîüÁâ©ÂåªÂ≠¶ÊñáÊú¨ÊåñÊéòÂíå‰∏¥Â∫ä‰ø°ÊÅØÂ§ÑÁêÜÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ïÂ•†ÂÆö‰∫ÜÂùöÂÆûÂü∫Á°Ä„ÄÇ",
        "papers": [
          {
            "paper_id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021,
            "cited_by_count": 1737,
            "role": "center"
          },
          {
            "paper_id": "W2963716420",
            "title": "Publicly Available Clinical",
            "year": 2019,
            "cited_by_count": 1422
          },
          {
            "paper_id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019,
            "cited_by_count": 6148
          },
          {
            "paper_id": "W2970771982",
            "title": "SciBERT: A Pretrained Language Model for Scientific Text",
            "year": 2019,
            "cited_by_count": 2777
          },
          {
            "paper_id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019,
            "cited_by_count": 6148
          },
          {
            "paper_id": "W2971258845",
            "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
            "year": 2019,
            "cited_by_count": 797
          },
          {
            "paper_id": "W2955483668",
            "title": "Enhancing clinical concept extraction with contextual embeddings",
            "year": 2019,
            "cited_by_count": 314
          }
        ],
        "total_citations": 19343,
        "visual_structure": "5 Routes -> Center",
        "relation_stats": {
          "total_relations": 5,
          "relation_distribution": {
            "Overcomes": 5
          },
          "dominant_relation": "Overcomes"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2963716420",
              "title": "Publicly Available Clinical",
              "year": 2019
            },
            "to_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Ë¢´Domain-Specific Language Model Pretraining for Biomedical Natural Language ProcessingÊï¥Âêà",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2970771982",
              "title": "SciBERT: A Pretrained Language Model for Scientific Text",
              "year": 2019
            },
            "to_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Ë¢´Domain-Specific Language Model Pretraining for Biomedical Natural Language ProcessingÊï¥Âêà",
            "route_id": 2,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2911489562",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019
            },
            "to_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Ë¢´Domain-Specific Language Model Pretraining for Biomedical Natural Language ProcessingÊï¥Âêà",
            "route_id": 3,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2971258845",
              "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
              "year": 2019
            },
            "to_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Ë¢´Domain-Specific Language Model Pretraining for Biomedical Natural Language ProcessingÊï¥Âêà",
            "route_id": 4,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2955483668",
              "title": "Enhancing clinical concept extraction with contextual embeddings",
              "year": 2019
            },
            "to_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Ë¢´Domain-Specific Language Model Pretraining for Biomedical Natural Language ProcessingÊï¥Âêà",
            "route_id": 5,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W3046375318",
              "label": "Center",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021,
              "citations": 1737,
              "role": "center"
            },
            {
              "id": "W2963716420",
              "label": "Publicly Available Clinical...",
              "title": "Publicly Available Clinical",
              "year": 2019,
              "citations": 1422
            },
            {
              "id": "W2911489562",
              "label": "BioBERT: a pre-trained biomedi...",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019,
              "citations": 6148
            },
            {
              "id": "W2970771982",
              "label": "SciBERT: A Pretrained Language...",
              "title": "SciBERT: A Pretrained Language Model for Scientific Text",
              "year": 2019,
              "citations": 2777
            },
            {
              "id": "W2911489562",
              "label": "BioBERT: a pre-trained biomedi...",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019,
              "citations": 6148
            },
            {
              "id": "W2971258845",
              "label": "Transfer Learning in Biomedica...",
              "title": "Transfer Learning in Biomedical Natural Language Processing: An Evaluation of BERT and ELMo on Ten Benchmarking Datasets",
              "year": 2019,
              "citations": 797
            },
            {
              "id": "W2955483668",
              "label": "Enhancing clinical concept ext...",
              "title": "Enhancing clinical concept extraction with contextual embeddings",
              "year": 2019,
              "citations": 314
            }
          ],
          "edges": [
            {
              "source": "W2963716420",
              "target": "W3046375318",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W2970771982",
              "target": "W3046375318",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W2911489562",
              "target": "W3046375318",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W2971258845",
              "target": "W3046375318",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W2955483668",
              "target": "W3046375318",
              "type": "Overcomes",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "ÁÆ≠Â§¥ÊñπÂêëË°®Á§∫Êó∂Èó¥ÊºîËøõÊñπÂêëÔºàÊó©Âπ¥‰ªΩ ‚Üí ÊôöÂπ¥‰ªΩÔºâ",
          "pattern_note": "divergence=‰∏≠ÂøÉÊâ©Êï£, convergence=Â§öÊ∫êÊ±áËÅö"
        }
      },
      {
        "thread_id": 3,
        "thread_name": "Thread 3: The Convergence (Ê±áËÅöÊ®°Âºè)",
        "title": "Â§öÊäÄÊúØË∑ØÁ∫øÊ±áËÅöÂà∞ The authors propose a method for learning multilingual distributed representatio",
        "pattern_type": "The Convergence (Ê±áËÅöÊ®°Âºè)",
        "thread_type": "convergence",
        "narrative": "**ËÉåÊôØ**  \nÂú®Â§öËØ≠Ë®ÄÂµåÂÖ•Á†îÁ©∂È¢ÜÂüüÔºåÊó©ÊúüÁöÑÂ∑•‰Ωú‰∏ªË¶ÅÈõÜ‰∏≠‰∫éËß£ÂÜ≥Ë∑®ËØ≠Ë®ÄËØ≠‰πâÂØπÈΩêÁöÑÈóÆÈ¢ò„ÄÇ2008Âπ¥ÁöÑÁ†îÁ©∂„ÄäLearning Bilingual Lexicons from Monolingual Corpora„ÄãÊèêÂá∫‰∫Ü‰∏ÄÁßç‰ªéÂçïËØ≠ËØ≠Êñô‰∏≠Â≠¶‰π†ÂèåËØ≠ËØçÊ±áË°®ÁöÑÊñπÊ≥ïÔºåËØïÂõæÂú®Êó†ÈúÄÊòéÁ°ÆÂØπÈΩêÁöÑÊÉÖÂÜµ‰∏ãÊûÑÂª∫Ë∑®ËØ≠Ë®ÄËÅîÁ≥ª„ÄÇÊ≠§Â§ñÔºå2010Âπ¥ÁöÑ„ÄäFrom Frequency to Meaning: Vector Space Models of Semantics„ÄãËøõ‰∏ÄÊ≠•ÂèëÂ±ï‰∫ÜÂü∫‰∫éÂêëÈáèÁ©∫Èó¥Ê®°ÂûãÔºàVSMÔºâÁöÑËØ≠‰πâË°®Á§∫ÊñπÊ≥ïÔºåÈÄöËøáÊï∞Â≠¶ÁªìÊûÑÊçïÊçâËØçÊ±áÈó¥ÁöÑËØ≠‰πâÂÖ≥Á≥ª„ÄÇ‰∏éÊ≠§ÂêåÊó∂Ôºå2013Âπ¥ÁöÑ„ÄäEfficient Estimation of Word Representations in Vector Space„ÄãÂºïÂÖ•‰∫ÜËøûÁª≠ËØçË¢ãÔºàCBoWÔºâÁ≠âÈ´òÊïàÊ®°ÂûãÊû∂ÊûÑÔºå‰∏∫ÂçïËØ≠ËØ≠Êñô‰∏≠ÁöÑËØçË°®Á§∫Êèê‰æõ‰∫ÜÊñ∞ÁöÑÊäÄÊúØË∑ØÂæÑ„ÄÇËøô‰∫õÁ†îÁ©∂ËôΩÁÑ∂ÂêÑËá™Áã¨Á´ãÔºå‰ΩÜÈÉΩ‰∏∫Ë∑®ËØ≠Ë®ÄÂµåÂÖ•ÁöÑÊûÑÂª∫Êèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÁêÜËÆ∫ÂíåÊäÄÊúØÂü∫Á°Ä„ÄÇ\n\n**Ê±áËÅö**  \n„ÄäMultilingual Distributed Representations without Word Alignment„ÄãÂú®‰∏äËø∞Á†îÁ©∂ÁöÑÂü∫Á°Ä‰∏äÔºåÊèêÂá∫‰∫Ü‰∏ÄÁßçÊó†ÈúÄËØçÂØπÈΩêÁöÑÂ§öËØ≠Ë®ÄÂàÜÂ∏ÉÂºèË°®Á§∫Â≠¶‰π†ÊñπÊ≥ï„ÄÇËØ•ÊñπÊ≥ïÊï¥Âêà‰∫Ü‰ªéÂçïËØ≠ËØ≠Êñô‰∏≠Â≠¶‰π†ÂèåËØ≠ËØçÊ±áÂÖ≥Á≥ªÁöÑÁêÜÂøµÔºåÂêåÊó∂ÂÄüÈâ¥‰∫ÜÂêëÈáèÁ©∫Èó¥Ê®°ÂûãÁöÑËØ≠‰πâË°®Á§∫ËÉΩÂäõÔºåÂπ∂ÈÄÇÈÖç‰∫ÜËøûÁª≠ËØçË¢ãÁ≠âÈ´òÊïàÊû∂ÊûÑ„ÄÇÈÄöËøáËøôÁßçÊï¥ÂêàÔºå‰∏≠ÂøÉËÆ∫ÊñáÂÆûÁé∞‰∫ÜÂú®‰∏ç‰æùËµñÊòéÁ°ÆËØçÂØπÈΩêÁöÑÊÉÖÂÜµ‰∏ãÔºåÊûÑÂª∫Â§öËØ≠Ë®ÄÂµåÂÖ•ÁöÑÁªü‰∏ÄÊ°ÜÊû∂„ÄÇËøôÁßçÊñπÊ≥ï‰∏ç‰ªÖÂÖãÊúç‰∫Ü‰º†ÁªüÂ§öËØ≠Ë®ÄÂµåÂÖ•ÂØπËØçÂØπÈΩêÁöÑ‰æùËµñÔºåËøòÊòæËëóÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÈÄÇÁî®ÊÄßÂíåÊâ©Â±ïÊÄß„ÄÇ\n\n**ÊÑè‰πâ**  \nËØ•Á†îÁ©∂ÁöÑÊï¥ÂêàÊÄßË¥°ÁåÆ‰∏∫Â§öËØ≠Ë®ÄÂµåÂÖ•È¢ÜÂüüÂ∏¶Êù•‰∫ÜÊ∑±ËøúÂΩ±Âìç„ÄÇ‰∏ÄÊñπÈù¢ÔºåÂÆÉÁ™ÅÁ†¥‰∫Ü‰º†ÁªüÊñπÊ≥ïÁöÑÊäÄÊúØÁì∂È¢àÔºå‰ΩøÂæóË∑®ËØ≠Ë®ÄÂµåÂÖ•ËÉΩÂ§üÂú®Êõ¥ÂπøÊ≥õÁöÑËØ≠ÊñôÂíåËØ≠Ë®ÄÁéØÂ¢É‰∏≠Â∫îÁî®ÔºõÂè¶‰∏ÄÊñπÈù¢ÔºåÂÆÉ‰∏∫Â§öËØ≠Ë®ÄËØ≠‰πâË°®Á§∫Êèê‰æõ‰∫ÜÊñ∞ÁöÑÁêÜËÆ∫ËßÜËßíÔºåÊé®Âä®‰∫ÜË∑®ËØ≠Ë®ÄËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°ÁöÑÂèëÂ±ï„ÄÇËøôÁßç‰ªéÂàÜÊï£Âà∞Êï¥ÂêàÁöÑÊºîËøõÔºå‰∏ç‰ªÖÊèêÂçá‰∫ÜÊ®°ÂûãÁöÑÁêÜËÆ∫ÂÆåÂ§áÊÄßÔºåËøò‰∏∫ÂêéÁª≠Á†îÁ©∂Êèê‰æõ‰∫ÜÈáçË¶ÅÁöÑÊäÄÊúØÂü∫Áü≥ÔºåËøõ‰∏ÄÊ≠•‰øÉËøõ‰∫ÜËØ≠Ë®ÄÊäÄÊúØÁöÑÂÖ®ÁêÉÂåñÂ∫îÁî®‰∏éÂàõÊñ∞„ÄÇ",
        "papers": [
          {
            "paper_id": "W1562955078",
            "title": "Multilingual Distributed Representations without Word Alignment",
            "year": 2013,
            "cited_by_count": 67,
            "role": "center"
          },
          {
            "paper_id": "W2140406733",
            "title": "Learning Bilingual Lexicons from Monolingual Corpora",
            "year": 2008,
            "cited_by_count": 313
          },
          {
            "paper_id": "W2950577311",
            "title": "Efficient Estimation of Word Representations in Vector Space",
            "year": 2013,
            "cited_by_count": 11710
          },
          {
            "paper_id": "W1662133657",
            "title": "From Frequency to Meaning: Vector Space Models of Semantics",
            "year": 2010,
            "cited_by_count": 2827
          }
        ],
        "total_citations": 14917,
        "visual_structure": "3 Routes -> Center",
        "relation_stats": {
          "total_relations": 3,
          "relation_distribution": {
            "Overcomes": 1,
            "Adapts_to": 2
          },
          "dominant_relation": "Adapts_to"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2140406733",
              "title": "Learning Bilingual Lexicons from Monolingual Corpora",
              "year": 2008
            },
            "to_paper": {
              "id": "W1562955078",
              "title": "Multilingual Distributed Representations without Word Alignment",
              "year": 2013
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Ë¢´Multilingual Distributed Representations without Word AlignmentÊï¥Âêà",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2950577311",
              "title": "Efficient Estimation of Word Representations in Vector Space",
              "year": 2013
            },
            "to_paper": {
              "id": "W1562955078",
              "title": "Multilingual Distributed Representations without Word Alignment",
              "year": 2013
            },
            "relation_type": "Adapts_to",
            "narrative_relation": "Ë¢´Multilingual Distributed Representations without Word AlignmentÊï¥Âêà",
            "route_id": 2,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W1662133657",
              "title": "From Frequency to Meaning: Vector Space Models of Semantics",
              "year": 2010
            },
            "to_paper": {
              "id": "W1562955078",
              "title": "Multilingual Distributed Representations without Word Alignment",
              "year": 2013
            },
            "relation_type": "Adapts_to",
            "narrative_relation": "Ë¢´Multilingual Distributed Representations without Word AlignmentÊï¥Âêà",
            "route_id": 3,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W1562955078",
              "label": "Center",
              "title": "Multilingual Distributed Representations without Word Alignment",
              "year": 2013,
              "citations": 67,
              "role": "center"
            },
            {
              "id": "W2140406733",
              "label": "Learning Bilingual Lexicons fr...",
              "title": "Learning Bilingual Lexicons from Monolingual Corpora",
              "year": 2008,
              "citations": 313
            },
            {
              "id": "W2950577311",
              "label": "Efficient Estimation of Word R...",
              "title": "Efficient Estimation of Word Representations in Vector Space",
              "year": 2013,
              "citations": 11710
            },
            {
              "id": "W1662133657",
              "label": "From Frequency to Meaning: Vec...",
              "title": "From Frequency to Meaning: Vector Space Models of Semantics",
              "year": 2010,
              "citations": 2827
            }
          ],
          "edges": [
            {
              "source": "W2140406733",
              "target": "W1562955078",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W2950577311",
              "target": "W1562955078",
              "type": "Adapts_to",
              "direction": "forward"
            },
            {
              "source": "W1662133657",
              "target": "W1562955078",
              "type": "Adapts_to",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "ÁÆ≠Â§¥ÊñπÂêëË°®Á§∫Êó∂Èó¥ÊºîËøõÊñπÂêëÔºàÊó©Âπ¥‰ªΩ ‚Üí ÊôöÂπ¥‰ªΩÔºâ",
          "pattern_note": "divergence=‰∏≠ÂøÉÊâ©Êï£, convergence=Â§öÊ∫êÊ±áËÅö"
        }
      },
      {
        "thread_id": 4,
        "thread_name": "Thread 4: The Divergence",
        "title": "ÈíàÂØπ The vast and dynamic nature of medical data, along with intricate domain-specifi ÁöÑÂ§öÊäÄÊúØË∑ØÁ∫øÂçöÂºà",
        "pattern_type": "The Divergence",
        "thread_type": "divergence",
        "narrative": "### Focus  \n\"Large Language Models in Medicine (2023)\" serves as a cornerstone in the study of large language models (LLMs) in healthcare, being the first to systematically examine the challenges posed by the complexity and dynamism of medical data for LLM applications. The paper points out that, although LLMs have demonstrated potential in handling complex medical tasks, they still exhibit significant limitations regarding data privacy, domain adaptability, and performance reliability. Addressing these issues is not only crucial for the practical deployment of LLMs in medicine but also provides clear directions for future research.\n\n### Divergence  \nIn response to the challenges raised in the central paper, different research pathways have adopted distinct technical approaches. **Pathway 1** focuses on the safety and privacy of LLMs, enhancing model robustness through optimization methods such as adversarial training; **Pathway 2** concentrates on domain adaptability, proposing techniques like in-context learning to enable LLMs to surpass human experts in clinical text summarization tasks; **Pathway 3** aims to overcome the limitations of LLMs in clinical decision-making by developing an evaluation and refinement framework to improve model reliability; **Pathway 4** expands the application scenarios of LLMs, exploring their utility in radiology and proposing concrete implementation schemes based on models such as ChatGPT. These research directions collectively illustrate the diverse potential of LLMs in the medical field.\n\n### Comparison  \nThe aforementioned research pathways differ in their technical objectives and innovation emphases, yet also share certain intersections. **Pathway 1** and **Pathway 3** both address LLM limitations, but the former emphasizes privacy protection and security, while the latter focuses more on model performance evaluation and improvement; **Pathway 2** and **Pathway 4** place greater emphasis on the practical applicability of LLMs‚Äî the former enhances model performance in specific tasks through technical adaptation, while the latter explores broader clinical and research contexts. Overall, these studies jointly advance the technological progress of LLMs in medicine and offer multidimensional solutions to the unresolved issues highlighted in the central paper.",
        "papers": [
          {
            "paper_id": "W4384561707",
            "title": "Large language models in medicine",
            "year": 2023,
            "cited_by_count": 2502,
            "role": "center"
          },
          {
            "paper_id": "W4392353733",
            "title": "A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly",
            "year": 2024,
            "cited_by_count": 596
          },
          {
            "paper_id": "W4392193048",
            "title": "Adapted large language models can outperform medical experts in clinical text summarization",
            "year": 2024,
            "cited_by_count": 443
          },
          {
            "paper_id": "W4400324908",
            "title": "Evaluation and mitigation of the limitations of large language models in clinical decision-making",
            "year": 2024,
            "cited_by_count": 329
          },
          {
            "paper_id": "W4390919701",
            "title": "Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications",
            "year": 2024,
            "cited_by_count": 227
          }
        ],
        "total_citations": 4097,
        "visual_structure": "Center -> 4 Routes",
        "relation_stats": {
          "total_relations": 4,
          "relation_distribution": {
            "Realizes": 1,
            "Adapts_to": 1,
            "Overcomes": 1,
            "Extends": 1
          },
          "dominant_relation": "Realizes"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W4384561707",
              "title": "Large language models in medicine",
              "year": 2023
            },
            "to_paper": {
              "id": "W4392353733",
              "title": "A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly",
              "year": 2024
            },
            "relation_type": "Realizes",
            "narrative_relation": "Inspired",
            "route_id": 1,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W4384561707",
              "title": "Large language models in medicine",
              "year": 2023
            },
            "to_paper": {
              "id": "W4392193048",
              "title": "Adapted large language models can outperform medical experts in clinical text summarization",
              "year": 2024
            },
            "relation_type": "Adapts_to",
            "narrative_relation": "Was_Adapted_By",
            "route_id": 2,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W4384561707",
              "title": "Large language models in medicine",
              "year": 2023
            },
            "to_paper": {
              "id": "W4400324908",
              "title": "Evaluation and mitigation of the limitations of large language models in clinical decision-making",
              "year": 2024
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "route_id": 3,
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W4384561707",
              "title": "Large language models in medicine",
              "year": 2023
            },
            "to_paper": {
              "id": "W4390919701",
              "title": "Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications",
              "year": 2024
            },
            "relation_type": "Extends",
            "narrative_relation": "Was_Extended_By",
            "route_id": 4,
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W4384561707",
              "label": "Center",
              "title": "Large language models in medicine",
              "year": 2023,
              "citations": 2502,
              "role": "center"
            },
            {
              "id": "W4392353733",
              "label": "A survey on large language mod...",
              "title": "A survey on large language model (LLM) security and privacy: The Good, The Bad, and The Ugly",
              "year": 2024,
              "citations": 596
            },
            {
              "id": "W4392193048",
              "label": "Adapted large language models ...",
              "title": "Adapted large language models can outperform medical experts in clinical text summarization",
              "year": 2024,
              "citations": 443
            },
            {
              "id": "W4400324908",
              "label": "Evaluation and mitigation of t...",
              "title": "Evaluation and mitigation of the limitations of large language models in clinical decision-making",
              "year": 2024,
              "citations": 329
            },
            {
              "id": "W4390919701",
              "label": "Chatbots and Large Language Mo...",
              "title": "Chatbots and Large Language Models in Radiology: A Practical Primer for Clinical and Research Applications",
              "year": 2024,
              "citations": 227
            }
          ],
          "edges": [
            {
              "source": "W4384561707",
              "target": "W4392353733",
              "type": "Realizes",
              "direction": "forward"
            },
            {
              "source": "W4384561707",
              "target": "W4392193048",
              "type": "Adapts_to",
              "direction": "forward"
            },
            {
              "source": "W4384561707",
              "target": "W4400324908",
              "type": "Overcomes",
              "direction": "forward"
            },
            {
              "source": "W4384561707",
              "target": "W4390919701",
              "type": "Extends",
              "direction": "forward"
            }
          ],
          "layout": "radial",
          "direction_note": "ÁÆ≠Â§¥ÊñπÂêëË°®Á§∫Êó∂Èó¥ÊºîËøõÊñπÂêëÔºàÊó©Âπ¥‰ªΩ ‚Üí ÊôöÂπ¥‰ªΩÔºâ",
          "pattern_note": "divergence=‰∏≠ÂøÉÊâ©Êï£, convergence=Â§öÊ∫êÊ±áËÅö"
        }
      },
      {
        "thread_id": 5,
        "thread_name": "Thread 5: The Chain (Á∫øÊÄßÈìæÊù°)",
        "title": "‰ªé Creation and public release of domain-specific BERT models trained on clinical t Âà∞ Introduction of MultiMedQA benchmark, which combines multiple medical question-a ÁöÑÊºîËøõ‰πãË∑Ø",
        "pattern_type": "The Chain (Á∫øÊÄßÈìæÊù°)",
        "thread_type": "chain",
        "narrative": "**Ëµ∑Ê∫ê**  \n2019Âπ¥ÔºåÁ¨¨‰∏ÄÁØáËÆ∫Êñá„ÄäPublicly Available Clinical„ÄãÂºÄÂàõ‰∫ÜÈ¢ÜÂüüÁâπÂÆöËØ≠Ë®ÄÊ®°ÂûãÂú®‰∏¥Â∫äÊñáÊú¨Â§ÑÁêÜ‰∏≠ÁöÑÂ∫îÁî®ÊñπÂêë„ÄÇÁ†îÁ©∂ËÄÖËØÜÂà´Âà∞ÈÄöÁî®È¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºàÂ¶ÇBERTÔºâÂú®‰∏¥Â∫äÈ¢ÜÂüüË°®Áé∞‰∏ç‰Ω≥ÁöÑÈóÆÈ¢òÔºåÊèêÂá∫Âπ∂ÂÖ¨ÂºÄ‰∫Ü‰∏ìÈó®ÈíàÂØπ‰∏¥Â∫äÊñáÊú¨ËÆ≠ÁªÉÁöÑBERTÊ®°Âûã„ÄÇÁÑ∂ËÄåÔºåËØ•ÊñπÊ≥ïÂú®ÂéªËØÜÂà´Âåñ‰ªªÂä°ÔºàÂ¶Çi2b2 2006Âíåi2b2 2014Ôºâ‰∏≠Ë°®Áé∞‰∏çÁêÜÊÉ≥ÔºåÂêåÊó∂Êú™ËÉΩÂÖÖÂàÜÊé¢Á¥¢ÂêàÊàêPHIÔºà‰∏™‰∫∫ÂÅ•Â∫∑‰ø°ÊÅØÔºâÊé©Á†ÅÂØπ‰∏ä‰∏ãÊñáÂµåÂÖ•Ê®°ÂûãÁöÑÂΩ±Âìç„ÄÇËøô‰∫õÂ±ÄÈôêÊÄß‰∏∫ÂêéÁª≠Á†îÁ©∂Êèê‰æõ‰∫ÜÊîπËøõÁ©∫Èó¥„ÄÇ\n\n**ÊºîËøõ**  \nÂêåÂπ¥ÔºåÁ¨¨‰∫åÁØáËÆ∫Êñá„ÄäBioBERT: a pre-trained biomedical language representation model for biomedical text mining„ÄãÈíàÂØπÈÄöÁî®ËØ≠Ë®ÄÊ®°ÂûãÂú®ÁîüÁâ©ÂåªÂ≠¶ÊñáÊú¨ÊåñÊéò‰∏≠ÁöÑË°®Áé∞‰∏çË∂≥ÈóÆÈ¢òÔºåÂºÄÂèë‰∫ÜBioBERTÊ®°Âûã„ÄÇËØ•Ê®°ÂûãÈÄöËøáÂú®ÁîüÁâ©ÂåªÂ≠¶È¢ÜÂüüÊñáÊú¨‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÔºåÊòæËëóÊèêÂçá‰∫Ü‰ªªÂä°ÊÄßËÉΩÔºåÂÖãÊúç‰∫ÜÈÄöÁî®Ê®°ÂûãÁöÑÂ±ÄÈôêÊÄß„ÄÇÁÑ∂ËÄåÔºåBioBERTÁöÑÈ¢ÑËÆ≠ÁªÉËøáÁ®ãÈúÄË¶ÅÂ§ßÈáèËÆ°ÁÆóËµÑÊ∫êÔºåËøôÈôêÂà∂‰∫ÜÂÖ∂ÂπøÊ≥õÂ∫îÁî®„ÄÇ2021Âπ¥ÔºåÁ¨¨‰∏âÁØáËÆ∫Êñá„ÄäDomain-Specific Language Model Pretraining for Biomedical Natural Language Processing„ÄãËøõ‰∏ÄÊ≠•‰ºòÂåñ‰∫ÜÈ¢ÜÂüüÁâπÂÆöÈ¢ÑËÆ≠ÁªÉÁ≠ñÁï•ÔºåÊèêÂá∫‰ªÖÂú®ÁîüÁâ©ÂåªÂ≠¶È¢ÜÂüüÊñáÊú¨‰∏äËøõË°åÈ¢ÑËÆ≠ÁªÉÁöÑÊñπÊ≥ïÔºå‰ª•ÂáèÂ∞ëË¥üËøÅÁßªÈóÆÈ¢ò„ÄÇËøô‰∏ÄÊñπÊ≥ïÂÖãÊúç‰∫ÜBioBERTÁöÑÈÉ®ÂàÜÂ±ÄÈôêÊÄßÔºå‰ΩÜ‰ªçÁº∫‰πèÂÖ®Èù¢ÁöÑÂü∫ÂáÜÊµãËØïÔºåÊó†Ê≥ïÂÖÖÂàÜÈ™åËØÅÂÖ∂ÂØπÂ§öÊ†∑Âåñ‰ªªÂä°ÁöÑÈÄÇÁî®ÊÄß„ÄÇ\n\n**ÊúÄÊñ∞ËøõÂ±ï**  \n2023Âπ¥ÔºåÁ¨¨ÂõõÁØáËÆ∫Êñá„ÄäThe future landscape of large language models in medicine„ÄãÊé¢ËÆ®‰∫ÜÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂåªÂ≠¶È¢ÜÂüüÁöÑÊΩúÂäõÔºåÂêåÊó∂ÊåáÂá∫ÂÖ∂Âú®ÂáÜÁ°ÆÊÄßÂíåÂÅèÂ∑ÆÊéßÂà∂ÊñπÈù¢ÁöÑÊåëÊàò„ÄÇÁ†îÁ©∂ËÄÖÈÄöËøáÂºïÂÖ•‰∫∫Á±ªÂèçÈ¶àÂº∫ÂåñÂ≠¶‰π†ÂíåÊåÅÁª≠ÂæÆË∞ÉÊú∫Âà∂Ôºå‰∏∫ÂêéÁª≠Á†îÁ©∂Â•†ÂÆö‰∫ÜÊäÄÊúØÂü∫ÂáÜ„ÄÇÂêåÂπ¥ÔºåÁ¨¨‰∫îÁØáËÆ∫Êñá„ÄäLarge language models encode clinical knowledge„ÄãÂú®Ê≠§Âü∫Á°Ä‰∏äÂèñÂæó‰∫ÜÁ™ÅÁ†¥ÔºåÊèêÂá∫‰∫ÜÁªºÂêàÊÄßÁöÑMultiMedQAÂü∫ÂáÜÔºåÁî®‰∫éËØÑ‰º∞LLMsÁöÑ‰∏¥Â∫äÁü•ËØÜË°®Áé∞„ÄÇËøô‰∏ÄÊñπÊ≥ï‰∏ç‰ªÖÂÖãÊúç‰∫ÜÂâç‰∫∫Á†îÁ©∂‰∏≠ËØÑ‰º∞ËåÉÂõ¥ÊúâÈôêÁöÑÈóÆÈ¢òÔºåËøòÈÄöËøáË∑®Â≠¶ÁßëÂêà‰ΩúÊé®Âä®‰∫ÜAIÂú®ÂåªÁñóÈ¢ÜÂüüÁöÑË¥üË¥£‰ªªÂ∫îÁî®„ÄÇÁÑ∂ËÄåÔºåÂ¶Ç‰ΩïÁ°Æ‰øùÊ®°ÂûãÂú®ÂÆâÂÖ®ÂÖ≥ÈîÆ‰ªªÂä°‰∏≠ÁöÑÂõûÁ≠îË¥®Èáè‰ªçÊòØÊú™Êù•Á†îÁ©∂ÁöÑÈáçË¶ÅÊñπÂêë„ÄÇ",
        "papers": [
          {
            "paper_id": "W2963716420",
            "title": "Publicly Available Clinical",
            "year": 2019,
            "cited_by_count": 1422
          },
          {
            "paper_id": "W2911489562",
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "year": 2019,
            "cited_by_count": 6148
          },
          {
            "paper_id": "W3046375318",
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "year": 2021,
            "cited_by_count": 1737
          },
          {
            "paper_id": "W4387500346",
            "title": "The future landscape of large language models in medicine",
            "year": 2023,
            "cited_by_count": 732
          },
          {
            "paper_id": "W4384071683",
            "title": "Large language models encode clinical knowledge",
            "year": 2023,
            "cited_by_count": 2248
          }
        ],
        "total_citations": 12287,
        "visual_structure": "Paper_1 -> Paper_2 -> Paper_3 -> Paper_4 -> Paper_5",
        "relation_stats": {
          "total_relations": 2,
          "relation_distribution": {
            "Overcomes": 2
          },
          "dominant_relation": "Overcomes"
        },
        "relation_chain": [
          {
            "from_paper": {
              "id": "W2963716420",
              "title": "Publicly Available Clinical",
              "year": 2019
            },
            "to_paper": {
              "id": "W2911489562",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Overcomes",
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W2911489562",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019
            },
            "to_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Was_Overcome_By",
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W3046375318",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021
            },
            "to_paper": {
              "id": "W4387500346",
              "title": "The future landscape of large language models in medicine",
              "year": 2023
            },
            "relation_type": "Temporal_Evolution",
            "narrative_relation": "Temporal_Evolution",
            "direction": "chronological"
          },
          {
            "from_paper": {
              "id": "W4387500346",
              "title": "The future landscape of large language models in medicine",
              "year": 2023
            },
            "to_paper": {
              "id": "W4384071683",
              "title": "Large language models encode clinical knowledge",
              "year": 2023
            },
            "relation_type": "Overcomes",
            "narrative_relation": "Overcomes",
            "direction": "chronological"
          }
        ],
        "visualization_data": {
          "nodes": [
            {
              "id": "W2963716420",
              "label": "Paper 1",
              "title": "Publicly Available Clinical",
              "year": 2019,
              "citations": 1422
            },
            {
              "id": "W2911489562",
              "label": "Paper 2",
              "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
              "year": 2019,
              "citations": 6148
            },
            {
              "id": "W3046375318",
              "label": "Paper 3",
              "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
              "year": 2021,
              "citations": 1737
            },
            {
              "id": "W4387500346",
              "label": "Paper 4",
              "title": "The future landscape of large language models in medicine",
              "year": 2023,
              "citations": 732
            },
            {
              "id": "W4384071683",
              "label": "Paper 5",
              "title": "Large language models encode clinical knowledge",
              "year": 2023,
              "citations": 2248
            }
          ],
          "edges": [
            {
              "source": "W2963716420",
              "target": "W2911489562",
              "type": "chronological_evolution",
              "label": "2019 ‚Üí 2019"
            },
            {
              "source": "W2911489562",
              "target": "W3046375318",
              "type": "chronological_evolution",
              "label": "2019 ‚Üí 2021"
            },
            {
              "source": "W3046375318",
              "target": "W4387500346",
              "type": "chronological_evolution",
              "label": "2021 ‚Üí 2023"
            },
            {
              "source": "W4387500346",
              "target": "W4384071683",
              "type": "chronological_evolution",
              "label": "2023 ‚Üí 2023"
            }
          ],
          "layout": "hierarchical",
          "direction_note": "ÁÆ≠Â§¥ÊñπÂêëË°®Á§∫Êó∂Èó¥ÊºîËøõÊñπÂêëÔºàÊó©Âπ¥‰ªΩ ‚Üí ÊôöÂπ¥‰ªΩÔºâ",
          "pattern_note": ""
        }
      }
    ],
    "metadata": {
      "total_papers_analyzed": 226,
      "papers_after_pruning": 69,
      "total_threads": 5,
      "generation_date": "2025-12-21T21:36:46.885863"
    }
  },
  "summary": {
    "original_papers": 226,
    "pruned_papers": 69,
    "total_threads": 5
  }
};

                // Research IdeasÊï∞ÊçÆ
                const researchIdeasData = {
  "topic": "Natural Language Processing",
  "total_ideas": 20,
  "successful_ideas": 20,
  "ideas": [
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "Adopting a socio-technical framework for identifying and managing AI bias.\n\n**Explanation:** A socio-technical framework allows for an expansive view that includes not only computational factors but also human and systemic biases that AI systems may amplify. It emphasizes the importance of recognizing and addressing biases in datasets, algorithms, and human interactions with AI systems. By considering AI within the larger social system and aligning AI system design with societal values, stakeholders can better identify, understand, and mitigate biases across various stages of the AI lifecycle.",
      "status": "SUCCESS",
      "title": "Integrating Socio-Technical Frameworks to Address Intersectional Bias in Language Revitalization for LGBTQ+ Communities",
      "abstract": "Background: Current research in sociolinguistics often fails to adequately address the intersection of cultural/linguistic identities and LGBTQ+ experiences, particularly in the context of language revitalization efforts. This gap highlights the need for more comprehensive frameworks that can incorporate queer identities into language revitalization initiatives. Gap: Existing socio-technical frameworks for identifying and managing AI bias focus primarily on computational and systemic biases but have not been adapted to address intersectional sociolinguistic challenges, especially those involving LGBTQ+ communities. Proposed Method: We propose adapting the socio-technical framework for AI bias to the sociolinguistic domain by incorporating intersectional identity variables, community-driven data collection, and participatory design principles. This adaptation will enable the identification and mitigation of biases in language revitalization tools and practices, ensuring inclusivity for LGBTQ+ identities. Expected Result: The proposed framework will provide a novel, interdisciplinary approach to addressing intersectional biases in sociolinguistics, fostering more inclusive and effective language revitalization efforts.",
      "modification": "Incorporate intersectional identity variables, community-driven data collection, and participatory design principles into the socio-technical framework to address biases in language revitalization for LGBTQ+ communities.",
      "reasoning": "Step 1: Analyze Compatibility - The socio-technical framework for AI bias is designed to address systemic and computational biases by considering both technical and social factors. This aligns with the need to address biases in sociolinguistic contexts, as both involve complex interactions between systems and human factors. However, the current framework does not explicitly account for intersectional identity variables or the specific challenges of language revitalization for LGBTQ+ communities. Thus, while the framework is broadly compatible, it requires adaptation to this specific domain. Step 2: Identify the Gap - The key gap is the lack of mechanisms within the socio-technical framework to address intersectional biases related to cultural/linguistic identities and LGBTQ+ experiences. The bridging variable is the integration of intersectional identity variables, community-driven data collection, and participatory design principles. These elements will enable the framework to address the unique sociolinguistic challenges of language revitalization for LGBTQ+ communities. Step 3: Draft the Idea - The title and abstract were crafted to reflect the interdisciplinary nature of the proposed adaptation and its potential to address the identified gap. The core innovation is the integration of intersectional identity variables and participatory design principles into the socio-technical framework, enabling it to address biases in sociolinguistic contexts effectively.",
      "rationale": "Step 1 Analysis: The socio-technical framework for AI bias is inherently interdisciplinary, combining computational, systemic, and human-centered approaches to identify and mitigate biases. This makes it a promising candidate for addressing the sociolinguistic challenges of language revitalization for LGBTQ+ communities, which also involve complex interactions between systemic and human factors. However, the framework's current focus on AI systems means it lacks specific mechanisms for addressing intersectional biases or the unique challenges of sociolinguistic contexts. Step 2 Analysis: To bridge this gap, the framework must be adapted to include intersectional identity variables, which capture the complex interplay between cultural/linguistic identities and LGBTQ+ experiences. Additionally, community-driven data collection and participatory design principles are essential to ensure that the framework is inclusive and responsive to the needs of LGBTQ+ communities. These adaptations will enable the framework to identify and mitigate biases in language revitalization tools and practices effectively. Step 3 Analysis: The proposed title and abstract were designed to highlight the interdisciplinary nature of the adaptation and its potential to address the identified gap. The core innovation is the integration of intersectional identity variables, community-driven data collection, and participatory design principles into the socio-technical framework. This approach draws on lessons from the evolutionary paths, where domain-specific adaptations and interdisciplinary approaches were used to overcome limitations in related fields. Decision Chain: Limitation ‚Üí Compatibility Analysis ‚Üí Gap Identification ‚Üí Proposed Adaptation ‚Üí Expected Outcome. Evidence: The evolutionary paths demonstrate the effectiveness of domain-specific adaptations and interdisciplinary approaches in overcoming limitations. By applying similar logic, the proposed adaptation of the socio-technical framework has the potential to address the identified gap effectively."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "Queer pedagogy, grounded in queer theory, aims to challenge normative structures and practices within education by integrating queer perspectives and embracing diversity in sexual identities.\n\n**Explanation:** Queer pedagogy provides educators with tools to critically analyze and dismantle heteronormative assumptions, enabling a more inclusive environment that values LGBTQ+ experiences. By incorporating queer theory, educators question and reframe traditional curricula to reflect a broader spectrum of identities and experiences, supporting a more comprehensive and inclusive approach to teaching and learning.",
      "status": "SUCCESS",
      "title": "Integrating Queer Pedagogy into Sociolinguistic Frameworks: A Model for Incorporating LGBTQ+ and Cultural Identities in Language Revitalization",
      "abstract": "Background: Current sociolinguistic methods struggle to address the intersection of cultural/linguistic identities and LGBTQ+ experiences, particularly in the context of language revitalization. This gap highlights the need for comprehensive frameworks that integrate queer identities into revitalization efforts. Gap: While queer pedagogy offers tools to challenge normative structures and embrace LGBTQ+ diversity, it has not been systematically adapted to sociolinguistic contexts, particularly in language revitalization. Proposed Method: We propose a novel framework that integrates queer pedagogy into sociolinguistic research, focusing on the intersection of cultural/linguistic identities and LGBTQ+ experiences. By adapting queer pedagogical principles to language revitalization, this model aims to create inclusive strategies that reflect diverse identities. Expected Result: This approach is expected to provide a more inclusive and comprehensive framework for sociolinguistic research, enabling effective incorporation of queer identities into language revitalization efforts and addressing the current research bottleneck.",
      "modification": "Adapt queer pedagogy principles to sociolinguistic frameworks, particularly focusing on language revitalization efforts. This involves creating a theoretical and practical model that bridges the gap between queer theory and sociolinguistic methodologies.",
      "reasoning": "Step 1: Analyze Compatibility - Queer pedagogy is grounded in queer theory and focuses on challenging normative structures and embracing diversity in sexual identities. Its theoretical foundation aligns with the need to address LGBTQ+ experiences in sociolinguistic research. However, queer pedagogy is primarily applied in educational contexts and lacks direct application to sociolinguistic frameworks, particularly in language revitalization. Despite this, its adaptability and focus on inclusivity make it a suitable candidate for extension into this domain. Step 2: Identify the Gap - The primary gap lies in the lack of a systematic adaptation of queer pedagogy to sociolinguistic research, particularly in the context of language revitalization. The bridging variable is the development of a theoretical and practical model that integrates queer pedagogical principles into sociolinguistic methodologies. This requires reinterpreting queer pedagogy to address the intersection of cultural/linguistic identities and LGBTQ+ experiences. Step 3: Draft the Idea - The proposed idea builds on the compatibility of queer pedagogy with the identified limitation, addressing the gap through a novel integration of queer theory into sociolinguistic frameworks. The core innovation lies in adapting queer pedagogy principles to create inclusive strategies for language revitalization, reflecting diverse identities.",
      "rationale": "Step 1 Analysis: Queer pedagogy is theoretically compatible with the limitation as it emphasizes inclusivity and challenges normative structures, aligning with the need to incorporate LGBTQ+ experiences into sociolinguistic research. However, its primary application in educational contexts means it requires adaptation to be effective in sociolinguistic frameworks, particularly in language revitalization. The computational complexity and assumptions of queer pedagogy are not directly relevant, as this is a theoretical and methodological adaptation rather than a computational one. Step 2 Analysis: The gap lies in the lack of a framework that integrates queer pedagogy into sociolinguistic research. The bridging variable is the adaptation of queer pedagogical principles to address the intersection of cultural/linguistic identities and LGBTQ+ experiences in language revitalization. This involves developing a theoretical and practical model that reinterprets queer pedagogy for sociolinguistic contexts. Step 3 Analysis: The proposed idea builds on the compatibility of queer pedagogy with the limitation, addressing the gap through a novel integration of queer theory into sociolinguistic frameworks. The title and abstract reflect the focus on adapting queer pedagogy to create inclusive strategies for language revitalization. The core innovation lies in reinterpreting queer pedagogy to address the intersection of cultural/linguistic identities and LGBTQ+ experiences. Decision Chain: Limitation ‚Üí Compatibility of queer pedagogy ‚Üí Gap in adaptation to sociolinguistic frameworks ‚Üí Bridging variable (adaptation of queer pedagogy principles) ‚Üí Proposed solution. Evidence: The evolutionary paths highlight the importance of adapting existing methods to new domains and contexts. Similar to how domain-specific adaptations of BERT addressed limitations in biomedical NLP, adapting queer pedagogy to sociolinguistic frameworks can address the identified research bottleneck."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "Developing 'queer pedagogy', which challenges and reconstructs teaching practices to be inclusive of diverse sexual orientations and gender identities.\n\n**Explanation:** Queer pedagogy aims to dismantle heteronormative assumptions within educational systems and curricula, fostering inclusivity by encouraging educators to engage with and incorporate LGBTQ+ perspectives. This approach can transform learning environments into spaces that validate and represent varied identities, thus addressing the marginalization experienced in traditional settings.",
      "status": "SUCCESS",
      "title": "Integrating Queer Pedagogy into Sociolinguistic Frameworks: A Comprehensive Approach to Language Revitalisation and LGBTQ+ Representation",
      "abstract": "Background: Current sociolinguistic methods struggle to address the intersection of cultural/linguistic identities and LGBTQ+ experiences, particularly in the context of language revitalisation. This gap highlights the need for more inclusive frameworks that integrate queer identities effectively. Gap: While queer pedagogy offers a transformative approach to dismantling heteronormative assumptions in educational systems, its application to sociolinguistic research and language revitalisation efforts remains underexplored. Proposed Method: We propose a novel framework that adapts queer pedagogy principles to sociolinguistic methodologies, focusing on the co-creation of language revitalisation strategies that validate and incorporate LGBTQ+ identities. This involves rethinking linguistic identity as fluid and intersectional, and embedding these principles into community-driven language practices. Expected Result: This approach is expected to create more inclusive and representative language revitalisation efforts, fostering environments where diverse identities are acknowledged and celebrated.",
      "modification": "Adapt queer pedagogy principles to sociolinguistic research by developing frameworks that treat linguistic identity as fluid and intersectional, and embed these principles into community-driven language revitalisation practices.",
      "reasoning": "The reasoning process involved analyzing the compatibility of queer pedagogy with the limitations in sociolinguistic research, identifying the gap in its application to language revitalisation, and drafting a solution that bridges this gap. The evolutionary context provided insights into how domain-specific adaptations and cross-domain innovations have successfully addressed limitations in other fields, which informed the proposed adaptation of queer pedagogy principles to sociolinguistics.",
      "rationale": "Step 1: Analyze Compatibility. Queer pedagogy is fundamentally compatible with the limitation as it challenges heteronormative assumptions and fosters inclusivity, aligning with the need to address the intersection of cultural/linguistic identities and LGBTQ+ experiences. Its theoretical basis in deconstructing norms and embracing diversity makes it suitable for sociolinguistic applications. However, its current focus on educational systems means it requires adaptation to address language revitalisation specifically. Step 2: Identify the Gap. The gap lies in the lack of frameworks that integrate queer pedagogy principles into sociolinguistic research, particularly for language revitalisation. The bridging variable is the adaptation of queer pedagogy to treat linguistic identity as fluid and intersectional, embedding these principles into community-driven language practices. This requires rethinking traditional sociolinguistic methodologies to include LGBTQ+ perspectives explicitly. Step 3: Draft the Idea. The proposed idea is titled 'Integrating Queer Pedagogy into Sociolinguistic Frameworks: A Comprehensive Approach to Language Revitalisation and LGBTQ+ Representation.' The abstract outlines the background limitation, the identified gap, the proposed method of adapting queer pedagogy principles, and the expected result of creating more inclusive language revitalisation efforts. The core innovation is the adaptation of queer pedagogy principles to treat linguistic identity as fluid and intersectional, embedding these principles into sociolinguistic research. Decision Chain: The limitation of addressing LGBTQ+ experiences in sociolinguistics ‚Üí Compatibility of queer pedagogy with inclusivity goals ‚Üí Gap in its application to language revitalisation ‚Üí Proposed adaptation of queer pedagogy principles. Evidence: The evolutionary context demonstrated successful domain-specific adaptations and cross-domain innovations, supporting the feasibility of adapting queer pedagogy to sociolinguistics."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "Adoption of computational linguistics methods such as machine learning and statistical models to analyze and visualize large corpora, alongside tools like concordancing, frequency analysis, and text visualisation.\n\n**Explanation:** Computational linguistics methods facilitate the processing and analysis of large corpora through automation and advanced statistical techniques. By using machine learning for pattern detection and employing visualization tools for clear representation, researchers and practitioners can efficiently analyze translation data, uncover linguistic patterns, and gain insights into translation characteristics quickly. These tools handle the data-intensive nature of corpus-based translation studies, addressing both quantitative and qualitative aspects effectively.",
      "status": "SUCCESS",
      "title": "QueerLing: A Computational Framework for Analyzing the Intersection of Linguistic and LGBTQ+ Identities in Language Revitalization",
      "abstract": "Background: The intersection of cultural/linguistic identities and LGBTQ+ experiences remains underexplored in sociolinguistics, particularly in the context of language revitalization efforts. Current methods lack comprehensive frameworks to integrate queer identities into the analysis of linguistic patterns and practices. Gap: While computational linguistics methods such as machine learning and statistical models excel at processing and analyzing large corpora, they have not been tailored to address the unique sociolinguistic challenges posed by the intersection of linguistic and LGBTQ+ identities. Proposed Method: We propose 'QueerLing', a novel computational framework that integrates machine learning-based pattern detection, concordancing, and text visualization with sociolinguistic theories of identity and language use. By incorporating domain-specific annotations and leveraging transfer learning from queer-focused corpora, QueerLing aims to uncover nuanced patterns in language use that reflect the interplay between cultural/linguistic identities and LGBTQ+ experiences. Expected Result: This framework is expected to provide a robust, scalable, and data-driven approach to analyze and visualize the intersection of queer identities and language revitalization efforts, offering new insights and fostering inclusivity in sociolinguistic research.",
      "modification": "Incorporate domain-specific annotations for LGBTQ+ identities and utilize transfer learning from queer-focused corpora to adapt computational linguistics methods to the sociolinguistic context.",
      "reasoning": "The candidate method, computational linguistics techniques such as machine learning and statistical models, is fundamentally compatible with the limitation. These methods excel at processing large corpora and uncovering patterns, which aligns with the need to analyze the intersection of cultural/linguistic identities and LGBTQ+ experiences. However, the gap lies in the lack of domain-specific adaptation to sociolinguistic contexts involving queer identities. By introducing domain-specific annotations and leveraging transfer learning from queer-focused corpora, the method can be tailored to address this limitation. This approach is inspired by evolutionary patterns in the development of domain-specific models in biomedical NLP, where transfer learning and domain-specific pretraining were used to overcome limitations of general NLP models.",
      "rationale": "Step 1 Analysis: The mathematical and algorithmic properties of computational linguistics methods, such as machine learning for pattern detection and statistical models for frequency analysis, are compatible with the limitation. These methods are designed to handle large datasets and uncover patterns, which is essential for analyzing the intersection of cultural/linguistic identities and LGBTQ+ experiences. However, the current methods lack domain-specific adaptation to sociolinguistic contexts, particularly those involving queer identities. Step 2 Analysis: The gap lies in the absence of domain-specific annotations and the lack of training on queer-focused corpora. The bridging variable is the integration of domain-specific annotations for LGBTQ+ identities and the use of transfer learning from queer-focused corpora. This modification would enable the computational methods to capture the nuanced interplay between linguistic and queer identities. Step 3 Analysis: Drawing inspiration from evolutionary patterns in biomedical NLP, where domain-specific pretraining and transfer learning addressed the limitations of general NLP models, we propose 'QueerLing'. This framework adapts computational linguistics methods to the sociolinguistic context by incorporating domain-specific annotations and leveraging transfer learning. The expected result is a robust, scalable framework that provides new insights into the intersection of queer identities and language revitalization efforts. Decision Chain: Limitation ‚Üí Compatibility of computational linguistics methods ‚Üí Identification of gap (lack of domain-specific adaptation) ‚Üí Bridging variable (domain-specific annotations and transfer learning) ‚Üí Proposed solution (QueerLing). Evidence: The success of domain-specific pretraining and transfer learning in biomedical NLP demonstrates the feasibility of adapting computational methods to specific sociolinguistic contexts. By applying similar logic, we can address the limitation and advance research in this underexplored area."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "Instruction finetuning, especially with an increased number of tasks and chain-of-thought data, improves generalization capabilities.\n\n**Explanation:** By finetuning language models with instruction-based datasets and integrating chain-of-thought (CoT) data, models like Flan-PaLM are able to better understand and solve tasks by reasoning through step-by-step instructions. This enhances their ability to tackle unseen tasks, as they learn more flexible inferencing strategies via instructions and reasoning patterns.",
      "status": "SUCCESS",
      "title": "Queer-Inclusive Sociolinguistic Frameworks via Instruction-Finetuned Language Models",
      "abstract": "Background: The intersection of cultural/linguistic identities and LGBTQ+ experiences remains underexplored in sociolinguistic research, particularly in the context of language revitalization efforts. Current methods struggle to incorporate queer identities effectively, necessitating more comprehensive frameworks. Gap: While instruction-finetuned language models with chain-of-thought reasoning have demonstrated enhanced generalization capabilities, they have not been explicitly adapted to address nuanced sociolinguistic intersections like those involving LGBTQ+ identities. Proposed Method: We propose an adaptation of instruction finetuning by curating a task-specific dataset that includes sociolinguistic scenarios involving queer identities, cultural contexts, and language revitalization efforts. This dataset will be augmented with chain-of-thought reasoning prompts to enable the model to infer and reason about complex sociocultural dynamics. Expected Result: The adapted model is expected to provide deeper insights into the intersection of queer identities and linguistic/cultural revitalization, thereby offering a novel computational framework for inclusive sociolinguistic research.",
      "modification": "Curate a task-specific dataset focused on queer identities, cultural/linguistic intersections, and language revitalization, and integrate chain-of-thought reasoning prompts tailored to sociolinguistic contexts.",
      "reasoning": "Step 1: Compatibility Analysis - Instruction finetuning with chain-of-thought reasoning is theoretically compatible with the limitation. The method's ability to generalize across tasks and reason through complex instructions aligns with the need to address nuanced sociolinguistic intersections. The computational complexity of instruction finetuning is manageable, and the method's flexibility allows for domain-specific adaptations. Thus, the method is compatible. Step 2: Gap Identification - The primary gap is the lack of domain-specific data and reasoning prompts that capture the intersection of queer identities, cultural/linguistic contexts, and language revitalization. The bridging variable is the creation of a task-specific dataset and reasoning framework tailored to this intersection. Step 3: Idea Drafting - Drawing from evolutionary patterns, such as the domain-specific adaptations seen in BioBERT and clinical NLP models, we propose a similar adaptation strategy. By curating a dataset and reasoning framework specific to the sociolinguistic domain, we can extend the capabilities of instruction-finetuned models to address this limitation.",
      "rationale": "The reasoning process begins with the compatibility analysis of instruction finetuning for addressing the limitation. The method's ability to generalize across tasks and reason through step-by-step instructions makes it suitable for tackling the nuanced intersection of queer identities and linguistic/cultural contexts. The computational complexity and flexibility of the method further support its applicability. The gap analysis identifies the absence of domain-specific data and reasoning prompts as the primary barrier. Drawing from evolutionary patterns, such as the adaptation of BioBERT for biomedical tasks, we propose a similar strategy: curating a task-specific dataset and reasoning framework. This approach leverages the method's strengths while addressing the limitation's unique requirements. The proposed idea, 'Queer-Inclusive Sociolinguistic Frameworks via Instruction-Finetuned Language Models,' outlines a structured plan to adapt instruction finetuning for sociolinguistic research, offering a novel computational framework for inclusive exploration of queer identities in language revitalization efforts."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "Utilizing Natural Language Processing (NLP) methods such as text classification, topic modelling, event extraction, and text scaling to automate the extraction of relevant information from large volumes of text.\n\n**Explanation:** NLP methods provide computational tools to automatically analyze text and extract key information, such as sentiment, stance, and topics, which can inform policymakers. By automating the data processing, NLP reduces the reliance on manual coding and allows for efficient analysis of vast amounts of textual data. This enables policymakers to use evidence-based insights extracted from text data to make informed decisions.",
      "status": "SUCCESS",
      "title": "QueerLens-NLP: Bridging Cultural and LGBTQ+ Identities in Language Revitalization through Domain-Specific NLP Models",
      "abstract": "Background: The intersection of cultural/linguistic identities and LGBTQ+ experiences remains underexplored in sociolinguistic research. Current methods struggle to incorporate queer identities effectively into language revitalization efforts, necessitating more comprehensive frameworks. Gap: While NLP methods such as text classification and topic modeling offer powerful tools for analyzing large text corpora, they lack domain-specific adaptations to address the nuanced intersection of cultural and queer identities. Proposed Method: We propose 'QueerLens-NLP,' a domain-specific NLP framework pre-trained on curated datasets representing diverse cultural and LGBTQ+ narratives. By integrating domain-specific pretraining with fine-tuned models for sociolinguistic tasks, QueerLens-NLP aims to extract, classify, and analyze text data that reflect the intersection of these identities. Expected Result: This approach will enable more inclusive and effective language revitalization efforts by providing insights into the representation and integration of queer identities within cultural and linguistic contexts.",
      "modification": "Develop a domain-specific pretraining approach using curated datasets that represent the intersection of cultural/linguistic identities and LGBTQ+ experiences. Fine-tune the model for sociolinguistic tasks such as language revitalization and identity representation analysis.",
      "reasoning": "Step 1: Analyze Compatibility: NLP methods such as text classification, topic modeling, and event extraction are computationally efficient and capable of processing large text corpora. However, their general-purpose nature limits their ability to address the nuanced intersection of cultural/linguistic identities and LGBTQ+ experiences. The underlying assumptions of these methods‚Äîsuch as reliance on general-purpose language models‚Äîare incompatible with the domain-specific requirements of this research limitation. To address this, domain-specific adaptations are necessary. Step 2: Identify the Gap: The primary gap lies in the lack of domain-specific pretraining that captures the intersection of cultural and queer identities. The bridging variable is the creation of a curated dataset that includes diverse narratives reflecting these intersections, combined with pretraining and fine-tuning techniques tailored to sociolinguistic tasks. This approach aligns with evolutionary patterns observed in the development of domain-specific NLP models, such as BioBERT, which addressed similar limitations in the biomedical domain. Step 3: Draft the Idea: Drawing inspiration from the evolutionary paths of domain-specific NLP models, we propose 'QueerLens-NLP,' a framework that adapts NLP methods to the sociolinguistic domain. The core innovation is the integration of domain-specific pretraining on curated datasets with fine-tuning for tasks such as language revitalization and identity representation analysis. This approach is expected to provide actionable insights for policymakers and researchers, enabling more inclusive and effective language revitalization efforts.",
      "rationale": "Step 1 Analysis: NLP methods like text classification and topic modeling are computationally efficient and suitable for analyzing large text corpora. However, their general-purpose nature limits their ability to address the nuanced intersection of cultural/linguistic identities and LGBTQ+ experiences. The underlying assumptions of these methods‚Äîsuch as reliance on general-purpose language models‚Äîare incompatible with the domain-specific requirements of this research limitation. For example, general NLP models may fail to capture the unique linguistic markers and cultural nuances present in texts representing queer identities within specific cultural contexts. Step 2 Analysis: The gap lies in the lack of domain-specific pretraining that captures the intersection of cultural and queer identities. The bridging variable is the creation of a curated dataset that includes diverse narratives reflecting these intersections, combined with pretraining and fine-tuning techniques tailored to sociolinguistic tasks. This approach aligns with evolutionary patterns observed in the development of domain-specific NLP models, such as BioBERT, which addressed similar limitations in the biomedical domain by pretraining on domain-specific corpora. Step 3 Analysis: Drawing inspiration from the evolutionary paths of domain-specific NLP models, we propose 'QueerLens-NLP,' a framework that adapts NLP methods to the sociolinguistic domain. The core innovation is the integration of domain-specific pretraining on curated datasets with fine-tuning for tasks such as language revitalization and identity representation analysis. This approach is expected to provide actionable insights for policymakers and researchers, enabling more inclusive and effective language revitalization efforts. Decision Chain: The limitation involves the intersection of cultural/linguistic identities and LGBTQ+ experiences, which is underexplored. General NLP methods are insufficient due to their lack of domain-specific adaptations. By creating a domain-specific NLP framework with curated datasets and tailored pretraining, we can address this gap effectively. Evidence: The evolutionary paths of domain-specific NLP models, such as BioBERT, demonstrate the efficacy of pretraining on domain-specific corpora to address limitations in general-purpose models. This logic supports the feasibility of 'QueerLens-NLP' as a solution to the identified research limitation."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "Use of Vector Space Models (VSMs) to represent semantics through mathematical structures like matrices and tensors.\n\n**Explanation:** VSMs represent documents, words, and semantics as vectors in a multidimensional space, where similarities in meaning are expressed as vector similarities. This mathematical representation allows computers to process text based on statistical patterns of word usage, moving from frequency counts of terms to capturing their semantic relevance.",
      "status": "SUCCESS",
      "title": "Queer-Linguistic Embedding Models: Integrating Cultural and LGBTQ+ Identities in Vector Space Representations",
      "abstract": "Background: Current sociolinguistic research struggles to address the intersection of cultural/linguistic identities and LGBTQ+ experiences, particularly in the context of language revitalization. Existing methods lack comprehensive frameworks to incorporate queer identities effectively. Gap: While Vector Space Models (VSMs) are powerful tools for representing semantics, they have not been adapted to encode the nuanced sociolinguistic intersections of cultural and queer identities. Proposed Method: We propose a novel adaptation of VSMs, termed Queer-Linguistic Embedding Models (QLEMs), which incorporate sociolinguistic metadata, including cultural and LGBTQ+ identity markers, into the vector space. By leveraging domain-specific corpora and metadata-driven embeddings, QLEMs aim to capture the interplay between linguistic and queer identities. Expected Result: This approach will provide a robust computational framework for analyzing and revitalizing languages in ways that inclusively represent LGBTQ+ experiences, addressing a critical gap in sociolinguistic research.",
      "modification": "Incorporate sociolinguistic metadata, including cultural and LGBTQ+ identity markers, into the vector space representation to create domain-specific embeddings.",
      "reasoning": "Step 1: Compatibility Analysis - Vector Space Models (VSMs) are mathematically and algorithmically suitable for representing semantics in a multidimensional space. They are capable of capturing complex relationships between words and concepts based on statistical patterns. However, their standard implementation does not inherently encode sociolinguistic metadata, such as cultural or LGBTQ+ identity markers. This limitation can be addressed by adapting the VSM framework to include metadata-driven embeddings. Thus, VSMs are fundamentally compatible with the problem but require specific modifications. Step 2: Gap Identification - The primary gap lies in the inability of standard VSMs to represent sociolinguistic intersections, such as those between cultural and queer identities. The bridging variable is the integration of sociolinguistic metadata into the vector space representation. This requires the development of domain-specific corpora annotated with cultural and LGBTQ+ identity markers, as well as algorithms to incorporate this metadata into the embedding process. Step 3: Idea Drafting - Drawing inspiration from evolutionary patterns in domain-specific NLP models, such as BioBERT, we propose Queer-Linguistic Embedding Models (QLEMs). These models extend VSMs by incorporating sociolinguistic metadata, enabling them to capture the nuanced interplay between linguistic and queer identities. This innovation builds on the success of domain-specific adaptations in NLP, applying similar principles to sociolinguistics. The expected outcome is a computational framework that inclusively represents LGBTQ+ experiences, addressing a critical gap in language revitalization efforts.",
      "rationale": "The reasoning process begins with an analysis of the compatibility of Vector Space Models (VSMs) with the given limitation. VSMs are well-suited for representing semantics mathematically but lack inherent mechanisms to encode sociolinguistic metadata. This limitation can be overcome by adapting the VSM framework to include metadata-driven embeddings. The gap lies in the need for a method to represent the intersection of cultural and LGBTQ+ identities within the vector space. Drawing from evolutionary patterns in domain-specific NLP models, such as BioBERT, which successfully adapted general NLP models to specific domains, we propose a similar adaptation for sociolinguistics. The bridging variable is the integration of sociolinguistic metadata into the vector space representation. This requires the development of annotated corpora and algorithms to incorporate this metadata. The proposed Queer-Linguistic Embedding Models (QLEMs) extend VSMs by incorporating sociolinguistic metadata, enabling them to capture the nuanced interplay between linguistic and queer identities. This innovation addresses the critical gap in sociolinguistic research, providing a computational framework for inclusive language revitalization efforts. The decision chain follows logically from the limitation to the compatibility of VSMs, the identification of the gap, and the proposed solution, supported by evidence from the evolutionary patterns of domain-specific NLP models."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "The generative framework proposed uses a single, shared natural language output space to perform joint sequence labeling and sentence-level classification, allowing the incorporation of label semantics and knowledge sharing across tasks.\n\n**Explanation:** By framing sequence labeling as a conditional sequence generation problem, the model uses natural language expressions as labels, enriching the semantic understanding of labels. This approach allows the model to learn more efficiently from limited examples because it leverages shared semantic context inherent in natural language, thus reducing the gap between different tasks and enabling effective knowledge transfer.",
      "status": "SUCCESS",
      "title": "Integrating Cultural and Queer Linguistic Identities: A Generative Framework for Sociolinguistic Diversity",
      "abstract": "Background: Current sociolinguistic research struggles to address the intersection of cultural/linguistic identities and LGBTQ+ experiences, particularly in the context of language revitalization efforts. This gap arises due to the lack of comprehensive frameworks that can effectively incorporate queer identities into linguistic studies. Gap: While generative frameworks have shown promise in joint sequence labeling and classification tasks, they have not been adapted to sociolinguistic contexts that require nuanced understanding of cultural and queer identities. Proposed Method: We propose an adaptation of the generative framework for joint sequence labeling and sentence-level classification, tailored to sociolinguistic diversity. By leveraging natural language expressions as labels, the model can encode semantic nuances of queer and cultural identities, enabling effective knowledge transfer across tasks and domains. Expected Result: This approach is expected to provide a robust framework for studying the intersection of cultural and queer identities, facilitating more inclusive language revitalization efforts and advancing sociolinguistic research.",
      "modification": "Adapt the generative framework to include sociolinguistic-specific label semantics and task-specific pretraining on datasets that encode queer and cultural identity nuances.",
      "reasoning": "The reasoning follows a structured analysis of the compatibility, gap, and solution: Step 1 (Compatibility Analysis): The generative framework's ability to perform joint sequence labeling and sentence-level classification aligns well with the need to handle complex sociolinguistic data. Its use of natural language expressions as labels provides a semantic richness that is crucial for capturing the intersectionality of cultural and queer identities. The method's knowledge-sharing capability across tasks is particularly suited for addressing the limited data available in this domain. Step 2 (Gap Identification): The primary gap lies in the lack of sociolinguistic-specific adaptations in the generative framework. To bridge this, the model needs to incorporate label semantics that reflect queer and cultural identities. Additionally, task-specific pretraining on relevant datasets is required to ensure the model captures the nuances of these identities. Step 3 (Idea Drafting): Drawing inspiration from evolutionary patterns in domain-specific NLP models, the proposed idea adapts the generative framework to sociolinguistic contexts. This involves integrating sociolinguistic-specific label semantics and leveraging task-specific pretraining. The core innovation is the adaptation of a generative framework to encode and transfer knowledge about queer and cultural identities, enabling a more inclusive approach to language revitalization and sociolinguistic research.",
      "rationale": "Step 1 Analysis: The generative framework is compatible with the limitation as it supports joint sequence labeling and classification, which are essential for analyzing complex sociolinguistic data. Its use of natural language expressions as labels enriches semantic understanding, making it suitable for capturing the intersectionality of queer and cultural identities. The method's ability to share knowledge across tasks is particularly advantageous given the limited data in this domain. Step 2 Analysis: The gap lies in the lack of sociolinguistic-specific adaptations. To address this, the framework must be modified to include label semantics that reflect queer and cultural identities. Task-specific pretraining on datasets that encode these nuances is also necessary. Drawing from evolutionary patterns, such as the adaptation of BioBERT for domain-specific tasks, we propose a similar adaptation for sociolinguistic contexts. Step 3 Analysis: The idea builds on the generative framework's strengths and adapts it to sociolinguistic research. By integrating sociolinguistic-specific label semantics and leveraging task-specific pretraining, the proposed method addresses the limitation effectively. The core innovation is the adaptation of the generative framework to encode and transfer knowledge about queer and cultural identities, enabling a more inclusive approach to language revitalization and sociolinguistic research. Decision Chain: Limitation ‚Üí Compatibility of generative framework ‚Üí Gap in sociolinguistic adaptation ‚Üí Proposed solution. Evidence: The evolutionary patterns show that domain-specific adaptations of NLP models have been successful in addressing similar limitations. Applying this logic to the generative framework ensures its feasibility and relevance."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "Deep learning-based models that employ neural networks like CNNs, RNNs, and Transformers to improve the representation and classification of text.\n\n**Explanation:** Deep learning models utilize complex architectures that enable the extraction of more nuanced features from text data. CNNs can capture spatial hierarchies in text, RNNs can capture sequential dependencies, and Transformers utilize attention mechanisms to consider the context of words within a text. This allows for more effective learning and classification compared to classical methods which might treat text as a simple bag-of-words or fail to capture dependencies.",
      "status": "SUCCESS",
      "title": "Queer-Linguistic Transformers: A Deep Learning Framework for Integrating Cultural and LGBTQ+ Identities in Language Revitalization",
      "abstract": "Background: Current sociolinguistic research struggles to address the intersection of cultural/linguistic identities and LGBTQ+ experiences, and lacks comprehensive frameworks for integrating queer identities into language revitalization efforts. Gap: Existing deep learning models, while powerful in text representation and classification, have not been adapted to capture the nuanced interplay of cultural and queer identities in language. Proposed Method: We propose 'Queer-Linguistic Transformers,' a domain-specific adaptation of Transformer-based models, pre-trained on curated datasets that include queer cultural narratives, multilingual corpora, and language revitalization texts. By incorporating domain-specific pretraining and fine-tuning with attention mechanisms tailored to sociolinguistic contexts, the model aims to capture the intersectionality of identities and provide actionable insights for language revitalization. Expected Result: This framework is expected to improve the representation of LGBTQ+ experiences in sociolinguistic research, offering a robust tool for integrating queer identities into language revitalization efforts and advancing inclusivity in computational linguistics.",
      "modification": "Domain-specific pretraining on curated queer and multilingual sociolinguistic datasets, combined with attention mechanism adaptations to emphasize intersectional identity features.",
      "reasoning": "Step 1: Compatibility Analysis - Deep learning models like Transformers are well-suited for capturing complex relationships in text due to their attention mechanisms and ability to model context. The limitation involves understanding nuanced intersections of cultural and LGBTQ+ identities, which requires models capable of learning from diverse and intersectional datasets. Transformers, with their adaptability, can be tailored to this task by pretraining on relevant domain-specific corpora. Therefore, the method is compatible. Step 2: Gap Identification - The primary gap lies in the lack of domain-specific pretraining and fine-tuning for the sociolinguistic and queer identity intersection. The bridging variable is the creation of curated datasets that include queer cultural narratives, multilingual corpora, and language revitalization texts. Additionally, attention mechanisms must be adapted to emphasize intersectional identity features. Step 3: Idea Drafting - Drawing from the evolutionary paths, particularly the domain-specific adaptations seen in BioBERT and clinical BERT models, the proposed method follows a similar trajectory by tailoring Transformers to a new domain. The innovation lies in combining domain-specific pretraining with attention mechanism adaptations to address the unique challenges of sociolinguistic and queer identity intersections.",
      "rationale": "Step 1 Analysis: Deep learning models, particularly Transformers, are designed to capture complex relationships in text through their attention mechanisms. These models are computationally intensive but highly adaptable to domain-specific tasks. The limitation requires understanding the intersection of cultural/linguistic identities and LGBTQ+ experiences, which involves learning from diverse and intersectional datasets. Given that Transformers have been successfully adapted for domain-specific tasks (e.g., BioBERT for biomedical text), they are theoretically compatible with this limitation. Step 2 Analysis: The gap lies in the absence of domain-specific pretraining and fine-tuning for sociolinguistic and queer identity intersections. The bridging variable is the creation of curated datasets that include queer cultural narratives, multilingual corpora, and language revitalization texts. Additionally, attention mechanisms must be adapted to emphasize intersectional identity features, ensuring the model captures the nuanced interplay of cultural and queer identities. Step 3 Analysis: Inspired by the evolutionary paths where domain-specific adaptations of Transformers (e.g., BioBERT, clinical BERT) addressed limitations in specialized fields, the proposed method applies a similar adaptation strategy to sociolinguistics. The innovation lies in combining domain-specific pretraining with attention mechanism adaptations to address the unique challenges of sociolinguistic and queer identity intersections. Decision Chain: The limitation requires nuanced text representation ‚Üí Transformers are compatible due to their contextual modeling capabilities ‚Üí Gap identified as lack of domain-specific pretraining and attention mechanism adaptations ‚Üí Solution proposed as 'Queer-Linguistic Transformers' with curated datasets and tailored attention mechanisms. Evidence: The success of domain-specific adaptations like BioBERT and clinical BERT supports the feasibility of this approach, as these models overcame similar challenges in representing specialized knowledge domains."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "The use of pre-trained language models (PLMs) which are pre-trained on a language modeling task using large unlabelled corpora, followed by fine-tuning for specific NLP tasks.\n\n**Explanation:** PLMs learn generic, latent representations of language from extensive unlabelled data, capturing common language nuances which can be shared across different NLP tasks, reducing the need for task-specific training while allowing efficient adaptation through fine-tuning.",
      "status": "SUCCESS",
      "title": "QueerLingoBERT: A Domain-Specific Pre-trained Language Model for Integrating Queer and Cultural Linguistic Identities",
      "abstract": "Background: Current sociolinguistic research struggles to address the intersection of cultural/linguistic identities and LGBTQ+ experiences, particularly in the context of language revitalization efforts. This gap is attributed to the lack of comprehensive frameworks and datasets that incorporate queer identities effectively. Gap: While pre-trained language models (PLMs) have demonstrated success in capturing domain-specific nuances, they have not been adapted to sociolinguistic contexts that require sensitivity to queer and cultural identities. Proposed Method: We propose QueerLingoBERT, a domain-specific pre-trained language model fine-tuned on curated datasets that represent LGBTQ+ experiences across diverse cultural and linguistic contexts. By leveraging transfer learning and domain-specific pretraining, QueerLingoBERT aims to bridge the gap in understanding and revitalizing languages with queer-inclusive frameworks. Expected Result: This approach is expected to significantly enhance the ability of NLP models to address sociolinguistic themes involving queer identities and cultural diversity, providing a robust tool for future research and practical applications in language revitalization.",
      "modification": "Develop a domain-specific pre-trained language model (QueerLingoBERT) by fine-tuning on curated datasets that capture LGBTQ+ experiences across diverse cultural and linguistic contexts.",
      "reasoning": "Step 1: Analyze Compatibility. Pre-trained language models (PLMs) are designed to capture latent language representations from large corpora, making them highly adaptable to various NLP tasks through fine-tuning. However, their effectiveness in addressing sociolinguistic themes such as queer and cultural identities depends on the availability of relevant domain-specific data and the ability to represent nuanced, intersectional identities. The computational complexity of PLMs is manageable with modern hardware, and their transfer learning capabilities align well with the need for domain adaptation. Thus, PLMs are fundamentally compatible with the limitation. Step 2: Identify the Gap. The primary gap lies in the lack of domain-specific pretraining on datasets that represent LGBTQ+ experiences within diverse cultural and linguistic contexts. Existing PLMs are trained on general-purpose corpora, which may not adequately capture the intersectionality of queer and cultural identities. The bridging variable is the creation of a curated dataset and the subsequent fine-tuning of a PLM to specialize in this domain. Step 3: Draft the Idea. Drawing inspiration from the evolutionary paths of domain-specific PLMs like BioBERT, we propose QueerLingoBERT, a model pre-trained on curated datasets that integrate queer and cultural linguistic identities. The core innovation is the development of a domain-specific PLM that addresses the intersection of queer and cultural identities, enabling more inclusive and effective language revitalization efforts.",
      "rationale": "The reasoning process begins with analyzing the compatibility of PLMs with the given limitation. PLMs are inherently flexible and capable of capturing complex linguistic patterns, making them suitable for addressing sociolinguistic themes. However, the limitation highlights a specific gap: the lack of focus on queer and cultural identities in existing models. Drawing from the evolutionary context, where domain-specific PLMs like BioBERT were developed to address limitations in general-purpose models, we identify the need for a similar approach in this domain. The bridging variable is the creation of a curated dataset that represents LGBTQ+ experiences across diverse cultural and linguistic contexts, which can be used to fine-tune a PLM. This approach is inspired by the success of domain-specific pretraining in biomedical and clinical NLP tasks, as demonstrated in the evolutionary paths. The proposed QueerLingoBERT model builds on this logic, aiming to provide a robust tool for integrating queer and cultural linguistic identities into sociolinguistic research and language revitalization efforts. The expected outcome is a significant advancement in the ability of NLP models to address these complex and underexplored themes."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "McEnery and Hardie proposed a structured approach to corpus linguistics, involving both corpus-based and corpus-driven methodologies.\n\n**Explanation:** The corpus-based approach uses deductive reasoning where corpus data supports pre-existing theories, while the corpus-driven approach uses inductive reasoning where corpus data itself drives the formation of new linguistic theories. This dual methodology provides a framework that systematically handles large text data, allowing researchers to either validate existing theories or develop new insights directly from data, thus enhancing the robustness of linguistic research.",
      "status": "SUCCESS",
      "title": "Queer-Linguistic Corpus Framework: Integrating Cultural and LGBTQ+ Identities in Language Revitalization Research",
      "abstract": "Background: Current sociolinguistic research faces challenges in addressing the intersection of cultural/linguistic identities and LGBTQ+ experiences, as well as integrating queer identities into language revitalization efforts. Existing frameworks lack the methodological depth to handle the nuanced interplay of these dimensions. Gap: While McEnery and Hardie's structured corpus linguistics approach offers robust tools for analyzing large-scale linguistic data, it does not explicitly address the intersectionality of cultural and queer identities or provide a framework for language revitalization. Proposed Method: We propose the 'Queer-Linguistic Corpus Framework,' an extension of the corpus-driven and corpus-based methodologies, tailored to incorporate intersectional sociolinguistic dimensions. This framework introduces a 'Queer-Cultural Annotation Schema' as the bridging variable, enabling the systematic tagging and analysis of linguistic data that reflects both cultural and LGBTQ+ identities. Expected Result: The proposed framework is expected to generate new insights into the intersection of cultural and queer identities, while also providing actionable strategies for incorporating LGBTQ+ perspectives into language revitalization efforts.",
      "modification": "Develop a 'Queer-Cultural Annotation Schema' to systematically tag and analyze linguistic data that reflects the intersection of cultural and LGBTQ+ identities. This schema will enable the corpus-driven approach to generate new theories and insights specific to this intersection, while also supporting the corpus-based validation of existing theories in language revitalization.",
      "reasoning": "Step 1: Analyze Compatibility: McEnery and Hardie's structured corpus linguistics approach is inherently flexible and capable of handling large-scale linguistic data. Its dual methodology (corpus-based and corpus-driven) aligns well with the need to explore both existing theories and new insights. However, the method does not inherently address the intersectionality of cultural and LGBTQ+ identities, nor does it provide a framework for language revitalization. Despite this, its adaptability and data-driven nature make it a strong candidate for extension to address these gaps. Step 2: Identify the Gap: The primary gap lies in the absence of a mechanism to systematically capture and analyze the intersection of cultural and LGBTQ+ identities within linguistic data. Additionally, there is no existing framework within the method to address the specific needs of language revitalization efforts that incorporate queer identities. The bridging variable is the development of a 'Queer-Cultural Annotation Schema,' which will enable the tagging and analysis of relevant sociolinguistic dimensions. Step 3: Draft the Idea: Drawing from the evolutionary context, where domain-specific adaptations of general methodologies have successfully addressed limitations (e.g., BioBERT for biomedical text), we propose a similar adaptation for corpus linguistics. The 'Queer-Linguistic Corpus Framework' extends the structured corpus linguistics approach by introducing a tailored annotation schema. This innovation allows for the systematic study of the intersection of cultural and LGBTQ+ identities and provides a foundation for integrating these insights into language revitalization frameworks. The core innovation is the 'Queer-Cultural Annotation Schema,' which bridges the gap between the method's capabilities and the research limitation.",
      "rationale": "The research limitation highlights two key challenges: the lack of focus on the intersection of cultural/linguistic identities and LGBTQ+ experiences, and the need for frameworks to integrate queer identities into language revitalization efforts. McEnery and Hardie's structured corpus linguistics approach is a strong candidate due to its dual methodology, which supports both hypothesis-driven and data-driven research. However, the method lacks specific tools for addressing intersectionality and language revitalization. Drawing from the evolutionary context, where domain-specific adaptations (e.g., BioBERT for biomedical text) successfully addressed similar limitations, we propose a tailored extension of the method. The 'Queer-Linguistic Corpus Framework' introduces a 'Queer-Cultural Annotation Schema' as the bridging variable. This schema enables the systematic tagging and analysis of linguistic data that reflects the intersection of cultural and LGBTQ+ identities. By leveraging the corpus-driven approach, the framework can generate new theories and insights, while the corpus-based approach validates these findings within the context of language revitalization. This adaptation aligns with the evolutionary logic of extending general methodologies to address specific domain challenges, making it a feasible and innovative solution to the research limitation."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "Brain-inspired AI leveraging principles of the human brain, such as neuroplasticity, multimodality processing, and efficient architecture designs, like neuromorphic computing and attention mechanisms.\n\n**Explanation:** By mimicking the human brain's ability to integrate multimodal information, adapt through neuroplasticity, and efficiently process data, brain-inspired AI can develop systems that are more flexible and capable of generalizing across tasks. This replication of brain principles allows AI systems to handle complex, real-world problems with adaptability similar to human cognition, essential for achieving AGI.",
      "status": "SUCCESS",
      "title": "Culturally-Informed Brain-Inspired AI for Integrating LGBTQ+ and Linguistic Identities in Language Revitalization",
      "abstract": "Background: Current sociolinguistic research struggles to address the intersection of cultural/linguistic identities and LGBTQ+ experiences, particularly in the context of language revitalization efforts. Existing frameworks lack the adaptability and multimodal integration needed to effectively incorporate queer identities into revitalization strategies. Gap: The challenge lies in developing a comprehensive framework that can model the complex interplay of cultural, linguistic, and queer identities while being adaptable to underexplored sociolinguistic contexts. Proposed Method: We propose a brain-inspired AI framework leveraging neuroplasticity, multimodal processing, and attention mechanisms to create culturally-informed models. These models will integrate diverse sociolinguistic and identity-related data, enabling the development of inclusive language revitalization strategies. Expected Result: This approach is expected to produce adaptive, context-sensitive models that can address the intersectionality of LGBTQ+ and linguistic identities, providing a novel tool for inclusive sociolinguistic research and practice.",
      "modification": "Incorporate culturally-informed training data and identity-specific embeddings into the brain-inspired AI framework, enabling it to model the intersectionality of LGBTQ+ and linguistic identities.",
      "reasoning": "Step 1: Compatibility Analysis: Brain-inspired AI is inherently adaptable due to its principles of neuroplasticity and multimodal processing. These properties align well with the need to model the intersectionality of cultural, linguistic, and queer identities, which requires flexible and context-sensitive systems. Additionally, attention mechanisms can help focus on specific sociolinguistic features, making the method computationally efficient and theoretically suitable. Thus, the method is compatible with the limitation. Step 2: Gap Identification: The primary gap is the lack of culturally-informed and identity-specific data integration within the brain-inspired AI framework. To bridge this gap, the 'Bridging Variable' is the inclusion of culturally-informed training data and identity-specific embeddings. These modifications will allow the AI to model the nuanced intersectionality of LGBTQ+ and linguistic identities. Step 3: Idea Drafting: Drawing from the evolutionary patterns, such as the domain-specific adaptations seen in BioBERT, we propose a similar adaptation strategy. By incorporating identity-specific embeddings and culturally-informed data, the brain-inspired AI can be tailored to address the specific sociolinguistic challenge. The core innovation lies in adapting brain-inspired AI to sociolinguistic contexts by integrating cultural and identity-specific dimensions.",
      "rationale": "Step 1 Analysis: The mathematical and algorithmic properties of brain-inspired AI, such as neuroplasticity and multimodal processing, are well-suited for addressing the complexity of sociolinguistic phenomena. These properties allow the AI to adapt to diverse data types and contexts, which is critical for modeling the intersectionality of LGBTQ+ and linguistic identities. The computational efficiency of neuromorphic designs and attention mechanisms further supports its applicability in resource-constrained settings, such as underexplored sociolinguistic domains. Step 2 Analysis: The gap lies in the lack of integration of culturally-informed and identity-specific data within the current AI frameworks. Drawing from evolutionary patterns, such as the domain-specific adaptations in BioBERT, we identify the need to incorporate identity-specific embeddings and culturally-informed training data as the 'Bridging Variable.' This modification will enable the AI to model the nuanced interplay of cultural, linguistic, and queer identities. Step 3 Analysis: Inspired by the evolutionary paths, particularly the adaptation of general NLP models to domain-specific contexts, we propose a similar approach for brain-inspired AI. By integrating culturally-informed data and identity-specific embeddings, the AI can be tailored to address the specific sociolinguistic challenge. The proposed method builds on the adaptability and multimodal capabilities of brain-inspired AI, extending its application to inclusive sociolinguistic research. Decision Chain: Limitation ‚Üí Compatibility of brain-inspired AI ‚Üí Gap in data integration ‚Üí Solution through culturally-informed adaptations. Evidence: The success of domain-specific adaptations in NLP, such as BioBERT, supports the feasibility of tailoring brain-inspired AI to sociolinguistic contexts. This approach aligns with the principles of neuroplasticity and multimodal processing, ensuring its effectiveness in addressing the limitation."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "ÂºÄÂèë‰∫Ü‰∏ÄÁßçÊñπÊ≥ïÊù•ÊäìÂèñÂ§ßÈáèÁöÑÁΩëÁªúÊï∞ÊçÆÂπ∂ËøõË°åËØ≠Ë®ÄÂ§ÑÁêÜÔºå‰ΩøÂæóËøô‰∫õÊï∞ÊçÆÊõ¥ÈÄÇÂêà‰∫éËØ≠Ë®ÄÁ†îÁ©∂ÂíåËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°„ÄÇ\n\n**Explanation:** ÈÄöËøáÂØπ‰ªéÁΩëÁªúÊäìÂèñÁöÑÊï∞ÊçÆËøõË°åËØ≠Ë®ÄÂ≠¶Â§ÑÁêÜÔºåÂ¶ÇËØçÂΩ¢ËøòÂéü„ÄÅÊ†áËÆ∞ÂåñÂíåÂè•Ê≥ïÂàÜÊûêÔºåÂ¢ûÂº∫‰∫ÜÊï∞ÊçÆÁöÑË¥®ÈáèÂíåÂÆûÁî®ÊÄß„ÄÇËøô‰ΩøÂæóÁ†îÁ©∂‰∫∫ÂëòËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞Âú®Â§ßËßÑÊ®°Êï∞ÊçÆÈõÜ‰∏äËøõË°åÂÆûÈ™åÂíåÂàÜÊûêÔºå‰ªéËÄåÊîØÊåÅÊõ¥Ê∑±ÂÖ•ÁöÑËØ≠Ë®ÄÁ†îÁ©∂ÂíåÂ∫îÁî®ÂºÄÂèë„ÄÇ",
      "status": "SUCCESS",
      "title": "QueerLingua: A Domain-Specific Language Model for Integrating LGBTQ+ and Cultural-Linguistic Identities in Language Revitalization",
      "abstract": "Background: Current sociolinguistic research often overlooks the intersection of LGBTQ+ identities and cultural/linguistic diversity, particularly in the context of language revitalization. Existing methods lack the capacity to address the nuanced interplay between these dimensions due to limited data and inadequate frameworks. Gap: While web data scraping and linguistic processing techniques have been successfully used to enhance large-scale language research, they have not been adapted to address the specific challenges of integrating queer identities into language revitalization efforts. Proposed Method: We propose 'QueerLingua,' a domain-specific language model pre-trained on curated web-scraped data that emphasizes LGBTQ+ experiences and cultural-linguistic diversity. By leveraging advanced linguistic preprocessing techniques (e.g., tokenization, lemmatization, and syntactic parsing) and domain-specific data augmentation, QueerLingua aims to provide a comprehensive framework for analyzing and supporting language revitalization efforts that inclusively represent queer identities. Expected Result: This approach is expected to bridge the gap in sociolinguistic research by offering a scalable, data-driven framework that integrates LGBTQ+ perspectives into language revitalization, fostering inclusivity and cultural preservation.",
      "modification": "Develop a domain-specific pretraining pipeline that focuses on LGBTQ+ and cultural-linguistic data, incorporating targeted data augmentation and linguistic preprocessing to address the intersectional challenges.",
      "reasoning": "Step 1: Compatibility Analysis: The candidate method, which involves web data scraping and linguistic preprocessing, is theoretically compatible with addressing the limitation. The method's ability to process large-scale data and enhance its linguistic quality aligns with the need for comprehensive frameworks to integrate LGBTQ+ identities into language revitalization. However, the current method does not explicitly focus on domain-specific challenges, such as the intersection of queer identities and cultural-linguistic diversity. This indicates that while the foundational capabilities are present, the method requires adaptation to address the specific sociolinguistic context. Step 2: Gap Identification: The gap lies in the lack of domain-specific focus and the absence of curated datasets that represent LGBTQ+ experiences within diverse cultural and linguistic contexts. The bridging variable is the development of a domain-specific pretraining pipeline that incorporates curated LGBTQ+ and cultural-linguistic data, along with targeted data augmentation techniques to enhance representation and inclusivity. This approach draws inspiration from the evolutionary paths of domain-specific language models like BioBERT, which successfully addressed domain-specific challenges through targeted pretraining. Step 3: Idea Drafting: The proposed idea, 'QueerLingua,' builds on the candidate method by adapting it to the specific sociolinguistic challenge. The core innovation is the integration of LGBTQ+ and cultural-linguistic data into a domain-specific language model, leveraging advanced linguistic preprocessing and data augmentation techniques. This approach is expected to provide a scalable framework for inclusive language revitalization efforts, addressing the identified research limitation.",
      "rationale": "The reasoning process begins with analyzing the compatibility of the candidate method with the research limitation. The method's ability to process large-scale web data and enhance its linguistic quality aligns with the need for comprehensive frameworks in sociolinguistic research. However, the method lacks a domain-specific focus, which is critical for addressing the intersection of LGBTQ+ identities and cultural-linguistic diversity. Drawing from the evolutionary paths of domain-specific language models like BioBERT, which overcame similar limitations in biomedical NLP through targeted pretraining, we identify the need for a domain-specific pretraining pipeline as the bridging variable. This pipeline would focus on LGBTQ+ and cultural-linguistic data, incorporating targeted data augmentation and linguistic preprocessing to address the intersectional challenges. The proposed idea, 'QueerLingua,' represents a novel application of domain-specific language modeling to sociolinguistic research, with the potential to significantly advance the integration of queer identities into language revitalization efforts. The complete reasoning path demonstrates the feasibility of adapting the candidate method to address the research limitation, supported by evidence from the evolutionary context of domain-specific language models."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "Inject lightweight, task-specific adapter layers into a pre-trained NMT model.\n\n**Explanation:** By only fine-tuning small adapter layers rather than the entire NMT model, the approach reduces parameter inefficiency and simplifies model maintenance. Adapters allow a single model to be adapted to multiple languages and domains simultaneously without requiring separate models for each task.",
      "status": "SUCCESS",
      "title": "Adapter-Enhanced Neural Machine Translation for Integrating Cultural and LGBTQ+ Linguistic Dimensions in Language Revitalization",
      "abstract": "Background: Current sociolinguistic research and language revitalization efforts often fail to adequately address the intersection of cultural/linguistic identities and LGBTQ+ experiences. This gap limits the inclusivity and effectiveness of language preservation initiatives. Existing neural machine translation (NMT) models, while powerful, are not inherently equipped to handle such nuanced sociolinguistic dimensions. Gap: The primary limitation lies in the lack of task-specific mechanisms within NMT models to incorporate and adapt to the intersectional complexities of cultural and LGBTQ+ linguistic identities. Proposed Method: We propose injecting lightweight, task-specific adapter layers into a pre-trained NMT model. These adapter layers will be fine-tuned on curated datasets that emphasize the intersection of cultural/linguistic identities and LGBTQ+ experiences, as well as language revitalization contexts. By leveraging the modularity and efficiency of adapter layers, the method enables the model to learn and generalize across multiple sociolinguistic domains without requiring extensive retraining. Expected Result: This approach is expected to provide a scalable, efficient, and inclusive framework for addressing the intersectional challenges in sociolinguistic research and language revitalization, thereby advancing both theoretical understanding and practical applications.",
      "modification": "Curate a specialized dataset that captures the intersection of cultural/linguistic identities and LGBTQ+ experiences, and fine-tune the adapter layers specifically on this dataset to enable the NMT model to address these nuanced sociolinguistic dimensions.",
      "reasoning": "Step 1: Analyze Compatibility. The candidate method of injecting lightweight, task-specific adapter layers into a pre-trained NMT model is theoretically compatible with the limitation. Adapter layers are designed to specialize pre-trained models for specific tasks or domains without requiring full retraining, which aligns well with the need to address the intersectional complexities of cultural/linguistic identities and LGBTQ+ experiences. The computational efficiency of adapter layers also ensures scalability, which is crucial for language revitalization efforts that often operate with limited resources. Step 2: Identify the Gap. The primary gap lies in the lack of a specialized dataset and task-specific fine-tuning to address the intersection of cultural/linguistic identities and LGBTQ+ experiences. The bridging variable here is the creation of a curated dataset that captures these nuanced sociolinguistic dimensions, enabling the adapter layers to learn and generalize effectively. Step 3: Draft the Idea. The proposed method involves injecting task-specific adapter layers into a pre-trained NMT model and fine-tuning them on a curated dataset that emphasizes the intersection of cultural/linguistic identities and LGBTQ+ experiences. This modular approach leverages the efficiency and adaptability of adapter layers to address the identified limitation, providing a scalable and inclusive solution for sociolinguistic research and language revitalization.",
      "rationale": "The limitation highlights the challenge of addressing the intersection between cultural/linguistic identities and LGBTQ+ experiences in sociolinguistic research and language revitalization. The candidate method, injecting lightweight, task-specific adapter layers into a pre-trained NMT model, is compatible with this limitation due to its modularity, efficiency, and ability to specialize in specific tasks or domains. Drawing from the evolutionary patterns, particularly the 'Star' and 'Chain' paths, we observe that domain-specific adaptations and fine-tuning on specialized datasets have been successful in addressing similar limitations in other fields, such as biomedical NLP. Applying this logic, the proposed solution involves curating a specialized dataset that captures the intersectional sociolinguistic dimensions and fine-tuning the adapter layers on this dataset. This approach not only addresses the identified gap but also aligns with successful strategies observed in the evolutionary paths, such as leveraging domain-specific pretraining and modular adaptations. The expected result is a scalable, efficient, and inclusive framework that advances both theoretical understanding and practical applications in sociolinguistic research and language revitalization."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles to fully address the intersection between cultural/linguistic identities and LGBTQ+ experiences, as relatively little is known about this area compared to other sociolinguistic themes.\n- There is limited exploration in our approach regarding how language revitalisation efforts can incorporate queer identities effectively, indicating a need for more comprehensive frameworks in future research.",
      "method": "Integrate Large Language Models (LLMs) like ChatGPT and GPT-4 into recommender systems, leveraging their strong language understanding and reasoning capabilities.\n\n**Explanation:** LLMs have been trained on extensive text data, developing robust language understanding and generation abilities, as well as impressive generalization and reasoning skills. By using LLMs to model complex user-item interactions and to capture textual side information, the recommender systems can better understand and adapt to user's preferences and generalize across various recommendation tasks, including unseen scenarios, thus addressing the limitations of DNN-based methods.",
      "status": "SUCCESS",
      "title": "Queer-Inclusive Sociolinguistic Frameworks: Leveraging Large Language Models for Intersectional Cultural and Linguistic Identity Analysis",
      "abstract": "Background: Current sociolinguistic research struggles to address the intersection of cultural/linguistic identities and LGBTQ+ experiences, as well as to integrate queer identities into language revitalization efforts. This gap highlights the need for comprehensive frameworks that can handle the complexity of these intersections. Gap: Existing methods, including traditional sociolinguistic approaches, lack the computational power and nuanced understanding of language required to address these challenges. Proposed Method: We propose leveraging Large Language Models (LLMs) such as GPT-4 to model and analyze the intersection of cultural/linguistic identities and LGBTQ+ experiences. By fine-tuning LLMs on curated datasets that emphasize queer-inclusive sociolinguistic contexts, we aim to create a framework that captures the nuanced interplay of identity, culture, and language. Expected Result: This approach is expected to provide a robust, scalable, and inclusive framework for sociolinguistic research, enabling deeper insights into underexplored intersections and informing language revitalization efforts that incorporate queer identities effectively.",
      "modification": "Fine-tune LLMs on curated datasets that emphasize queer-inclusive sociolinguistic contexts, and develop a framework to analyze the intersection of cultural/linguistic identities and LGBTQ+ experiences.",
      "reasoning": "Step 1: Analyze Compatibility. Large Language Models (LLMs) like GPT-4 are inherently designed to understand and generate complex language patterns, making them suitable for analyzing sociolinguistic phenomena. Their ability to generalize across diverse datasets and contexts aligns well with the need to study the intersection of cultural/linguistic identities and LGBTQ+ experiences. However, the current training of LLMs on general datasets may not sufficiently capture the nuanced sociolinguistic and queer-inclusive contexts required for this research. Step 2: Identify the Gap. The primary gap lies in the lack of domain-specific fine-tuning of LLMs to address queer-inclusive sociolinguistic contexts. The Bridging Variable here is the creation of a curated dataset that emphasizes the intersectionality of cultural/linguistic identities and LGBTQ+ experiences. Additionally, a framework needs to be developed to guide the analysis and application of LLM outputs in this domain. Step 3: Draft the Idea. Building on the compatibility analysis and gap identification, the proposed method involves fine-tuning LLMs on curated datasets and developing a sociolinguistic framework to analyze these intersections. This approach draws from evolutionary patterns in domain-specific adaptations of LLMs, such as BioBERT's success in biomedical NLP, to address the limitations of general models in specialized domains.",
      "rationale": "Step 1 Analysis: Large Language Models (LLMs) like GPT-4 are well-suited for tasks requiring deep language understanding and reasoning. Their ability to generalize across diverse datasets and contexts makes them compatible with the need to study complex sociolinguistic phenomena. However, the limitation lies in their general training, which may not adequately capture the nuanced interplay of cultural/linguistic identities and LGBTQ+ experiences. This is a domain where specialized knowledge and contextual understanding are crucial. Step 2 Analysis: The gap is the lack of domain-specific fine-tuning and frameworks for queer-inclusive sociolinguistic analysis. Drawing from the evolutionary patterns in the biomedical NLP domain, where models like BioBERT were fine-tuned on domain-specific data to overcome limitations of general NLP models, a similar approach can be applied here. The Bridging Variable is the creation of a curated dataset that emphasizes queer-inclusive sociolinguistic contexts and the development of a framework to guide the analysis. Step 3 Analysis: The proposed idea involves fine-tuning LLMs on curated datasets and developing a sociolinguistic framework to analyze the intersection of cultural/linguistic identities and LGBTQ+ experiences. This approach is inspired by the evolutionary success of domain-specific adaptations of LLMs, such as BioBERT, and aims to address the identified limitations by leveraging the strengths of LLMs in language understanding and reasoning. Decision Chain: Limitation ‚Üí Compatibility Analysis ‚Üí Gap Identification ‚Üí Solution. Evidence: The success of domain-specific adaptations of LLMs, such as BioBERT in biomedical NLP, supports the feasibility of this approach. By applying similar logic to sociolinguistic research, we can address the identified limitations effectively."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles with the scarcity of labelled training data which limits the performance and generalisation of deep learning models despite advances in overcoming this challenge.",
      "method": "The paper provides a comprehensive overview of efficient parallel training, model compression methods such as knowledge distillation and model pruning, and parameter-efficient tuning techniques like LoRA and Prompt Learning to reduce computational and memory overhead.\n\n**Explanation:** Parallel training techniques help distribute the computational load across multiple GPUs, making it feasible to handle large-scale data efficiently. Techniques like model compression reduce the size of models without significant loss in performance, making them easier to deploy on limited hardware. Parameter-efficient tuning allows fine-tuning of LLMs by adjusting only a subset of parameters, significantly reducing the cost associated with full model tuning. These solutions collectively facilitate the practical training and deployment of LLMs by lowering resource requirements.",
      "status": "SUCCESS",
      "title": "Adaptive Semi-Supervised Model Compression for Scarce Labelled Data in Deep Learning",
      "abstract": "Background: Deep learning models often struggle with the scarcity of labelled training data, which limits their performance and generalisation. While advances in parallel training, model compression, and parameter-efficient tuning have reduced computational and memory overhead, these methods do not directly address the challenge of limited labelled data. Gap: Existing methods focus on computational efficiency but lack mechanisms to leverage unlabelled data effectively or adapt to scenarios with minimal supervision. Proposed Method: We propose Adaptive Semi-Supervised Model Compression (ASSMC), a novel framework that integrates semi-supervised learning with model compression techniques like knowledge distillation. By leveraging unlabelled data through pseudo-labelling and incorporating domain-specific priors, ASSMC enhances model performance in low-data regimes while maintaining computational efficiency. Expected Result: This approach is expected to improve the generalisation of deep learning models under labelled data scarcity, offering a scalable and efficient solution for resource-constrained environments.",
      "modification": "Integrate semi-supervised learning techniques, such as pseudo-labelling or self-training, into the model compression pipeline to leverage unlabelled data effectively.",
      "reasoning": "The candidate method focuses on computational efficiency through parallel training, model compression, and parameter-efficient tuning. However, the limitation explicitly concerns the scarcity of labelled training data, which impacts model performance and generalisation. While the candidate method reduces resource requirements, it does not inherently address the lack of labelled data. To bridge this gap, semi-supervised learning techniques can be integrated into the model compression framework. Evolutionary patterns show that domain-specific adaptations and leveraging unlabelled data have been effective in overcoming similar limitations in biomedical NLP. By combining semi-supervised learning with knowledge distillation, the proposed method can utilise unlabelled data to improve model performance while maintaining computational efficiency.",
      "rationale": "Step 1: Compatibility Analysis - The candidate method includes techniques like parallel training, model compression, and parameter-efficient tuning, which are computationally efficient and scalable. However, these methods do not directly address the scarcity of labelled data, which is the core limitation. Semi-supervised learning, on the other hand, is well-suited for scenarios with limited labelled data as it leverages unlabelled data to improve model performance. Thus, the candidate method is partially compatible but requires adaptation. Step 2: Gap Identification - The key gap is the lack of mechanisms to leverage unlabelled data within the candidate method. The bridging variable is the integration of semi-supervised learning techniques, such as pseudo-labelling or self-training, into the model compression pipeline. This adaptation aligns with the evolutionary patterns observed in biomedical NLP, where domain-specific adaptations and unlabelled data utilisation have been successful. Step 3: Idea Drafting - The proposed title and abstract reflect the integration of semi-supervised learning with model compression to address the limitation. The core innovation is the combination of semi-supervised learning and model compression to enhance model performance in low-data regimes. Decision Chain: Limitation ‚Üí Candidate Method Compatibility ‚Üí Gap Identification ‚Üí Solution. Evidence: Evolutionary paths highlight the effectiveness of domain-specific adaptations and leveraging unlabelled data in overcoming similar limitations. The proposed method builds on these insights to address the scarcity of labelled data while maintaining computational efficiency."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles with the scarcity of labelled training data which limits the performance and generalisation of deep learning models despite advances in overcoming this challenge.",
      "method": "Adoption of computational linguistics methods such as machine learning and statistical models to analyze and visualize large corpora, alongside tools like concordancing, frequency analysis, and text visualisation.\n\n**Explanation:** Computational linguistics methods facilitate the processing and analysis of large corpora through automation and advanced statistical techniques. By using machine learning for pattern detection and employing visualization tools for clear representation, researchers and practitioners can efficiently analyze translation data, uncover linguistic patterns, and gain insights into translation characteristics quickly. These tools handle the data-intensive nature of corpus-based translation studies, addressing both quantitative and qualitative aspects effectively.",
      "status": "SUCCESS",
      "title": "Leveraging Semi-Supervised Learning and Domain-Specific Pretraining for Enhanced Low-Resource Corpus Analysis",
      "abstract": "Background: The scarcity of labeled training data poses a significant challenge to the performance and generalization of deep learning models, particularly in domains that require specialized knowledge. Computational linguistics methods, such as machine learning and statistical models, have shown promise in analyzing large corpora and uncovering linguistic patterns. However, these methods often rely on extensive labeled datasets, which are not always available in low-resource scenarios. Gap: Current computational linguistics approaches lack mechanisms to effectively leverage unlabeled data and domain-specific knowledge to address the limitations posed by insufficient labeled data. Proposed Method: We propose a novel framework that integrates semi-supervised learning techniques with domain-specific pretraining to enhance the utility of computational linguistics methods in low-resource settings. By employing self-training and pseudo-labeling strategies, combined with pretraining on domain-specific corpora, the method aims to maximize the use of both labeled and unlabeled data. Expected Result: The proposed approach is expected to improve the performance and generalization of computational linguistics models in low-resource scenarios, enabling more effective analysis and visualization of large corpora.",
      "modification": "Integration of semi-supervised learning techniques (e.g., self-training, pseudo-labeling) and domain-specific pretraining to leverage both labeled and unlabeled data.",
      "reasoning": "Step 1: Compatibility Analysis - The candidate method, which involves computational linguistics techniques like machine learning, statistical models, and visualization tools, is well-suited for analyzing and processing large corpora. However, the limitation explicitly highlights the challenge of scarce labeled data, which these methods do not inherently address. Without labeled data, machine learning models struggle to generalize effectively, and statistical models may fail to capture nuanced patterns. Thus, while the candidate method is compatible in terms of its ability to process large corpora, it requires additional mechanisms to handle the scarcity of labeled data effectively. Step 2: Gap Identification - The gap lies in the candidate method's reliance on labeled data for training. To bridge this gap, the method needs to incorporate semi-supervised learning techniques, such as self-training and pseudo-labeling, which can utilize unlabeled data to augment the training process. Additionally, domain-specific pretraining on unlabeled corpora can help the model acquire specialized knowledge, further mitigating the impact of limited labeled data. The bridging variable is the integration of semi-supervised learning and domain-specific pretraining into the computational linguistics framework. Step 3: Idea Drafting - The proposed idea builds on the candidate method by addressing its limitation in handling scarce labeled data. The core innovation lies in the combination of semi-supervised learning techniques with domain-specific pretraining, enabling the method to leverage both labeled and unlabeled data effectively. This approach aligns with the evolutionary patterns observed in the provided context, where domain-specific pretraining has been successfully used to overcome similar limitations in other fields. The proposed framework is expected to enhance the performance and generalization of computational linguistics models in low-resource scenarios, making it a feasible and impactful solution.",
      "rationale": "Step 1 Analysis: The candidate method, which includes machine learning and statistical models for analyzing large corpora, is theoretically compatible with the task of processing and visualizing data. However, its reliance on labeled data limits its applicability in low-resource scenarios. Computational complexity is not a primary concern here, as the focus is on data availability rather than processing power. The method's underlying assumptions about data availability and quality are not fully aligned with the limitation, indicating a need for adaptation. Step 2 Analysis: The gap is the method's inability to effectively utilize unlabeled data, which is often abundant even in low-resource settings. To address this, semi-supervised learning techniques such as self-training and pseudo-labeling can be integrated into the framework. These techniques allow the model to iteratively label and learn from unlabeled data, effectively increasing the size of the training set. Additionally, domain-specific pretraining can be employed to imbue the model with specialized knowledge, further enhancing its performance in the target domain. The bridging variable is the integration of these techniques into the computational linguistics framework. Step 3 Analysis: Drawing from the evolutionary context, where domain-specific pretraining has been used to overcome limitations in other fields, the proposed idea adapts this approach to the current limitation. The title, abstract, and core innovation are crafted to reflect the integration of semi-supervised learning and domain-specific pretraining, providing a clear and compelling solution to the problem of scarce labeled data. Decision Chain: Limitation ‚Üí Candidate Method Compatibility ‚Üí Identification of Gap (reliance on labeled data) ‚Üí Bridging Variable (semi-supervised learning and domain-specific pretraining) ‚Üí Proposed Solution. Evidence: The evolutionary paths highlight the successful use of domain-specific pretraining to address similar limitations in other fields, supporting the feasibility of the proposed solution."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles with the scarcity of labelled training data which limits the performance and generalisation of deep learning models despite advances in overcoming this challenge.",
      "method": "Utilization of large language models (LLMs) in medicine to deeply understand and semantically reason with medical texts, comprehend medical terminology, and integrate diverse medical data sources.\n\n**Explanation:** LLMs leverage deep learning and NLP techniques to process complex medical semantics and terminology, ensuring accurate retrieval and processing of medical information. By integrating diverse data sources such as medical literature and clinical guidelines, LLMs provide comprehensive and accurate medical information support. Their ability to update with the latest research further enhances the precision and timeliness of information delivery.",
      "status": "SUCCESS",
      "title": "Adaptive Semi-Supervised Pretraining of Large Language Models for Enhanced Medical Data Utilization",
      "abstract": "Background: The scarcity of labeled training data poses a significant challenge to the performance and generalization of deep learning models in medical applications. While large language models (LLMs) have demonstrated remarkable capabilities in processing complex semantics and integrating diverse data sources, their reliance on supervised learning limits their utility in low-resource settings. Gap: Current LLMs do not effectively leverage unlabeled data in the medical domain to address the challenge of labeled data scarcity. Proposed Method: We propose an adaptive semi-supervised pretraining framework for LLMs that incorporates self-supervised learning on large-scale unlabeled medical texts and fine-tuning with limited labeled data. This approach leverages the semantic reasoning capabilities of LLMs while reducing their dependence on labeled datasets. Expected Result: The proposed method is expected to significantly improve the performance and generalization of LLMs in medical applications, particularly in scenarios with limited labeled data, by effectively utilizing the vast amount of available unlabeled medical information.",
      "modification": "Incorporate a semi-supervised learning framework into the pretraining process of LLMs, combining self-supervised learning on unlabeled medical texts with fine-tuning on limited labeled data.",
      "reasoning": "Step 1: Analyze Compatibility - The candidate method, LLMs, is inherently capable of processing complex medical semantics and integrating diverse data sources. However, their reliance on supervised learning limits their effectiveness in scenarios with scarce labeled data. The computational complexity of LLMs is high, but this is not a fundamental incompatibility for addressing the limitation. Thus, the method is compatible with the limitation but requires adaptation to reduce its dependence on labeled data. Step 2: Identify the Gap - The key gap is the lack of a mechanism to effectively utilize unlabeled medical data to mitigate the scarcity of labeled data. The bridging variable is the incorporation of a semi-supervised learning framework that combines self-supervised learning on unlabeled data with fine-tuning on limited labeled data. This approach aligns with the evolutionary patterns observed in the development of domain-specific models like BioBERT, which adapted pretraining strategies to address domain-specific challenges. Step 3: Draft the Idea - The proposed method builds on the semantic reasoning capabilities of LLMs by introducing a semi-supervised pretraining framework. This innovation leverages the vast amount of unlabeled medical data to enhance the utility of LLMs in low-resource settings. The expected outcome is improved performance and generalization of LLMs in medical applications, addressing the limitation of labeled data scarcity.",
      "rationale": "The limitation of labeled data scarcity is a well-documented challenge in the application of deep learning models to medical tasks. The candidate method, LLMs, offers a promising foundation due to their ability to process complex semantics and integrate diverse data sources. However, their reliance on supervised learning limits their applicability in low-resource settings. By analyzing the evolutionary patterns in this field, such as the development of BioBERT and domain-specific pretraining strategies, it is evident that adapting pretraining methods to leverage domain-specific data can address similar challenges. The proposed modification introduces a semi-supervised learning framework that combines self-supervised learning on unlabeled medical texts with fine-tuning on limited labeled data. This approach aligns with the observed evolution of methods in this domain and addresses the identified gap. The resulting idea, 'Adaptive Semi-Supervised Pretraining of Large Language Models for Enhanced Medical Data Utilization,' is expected to improve the performance and generalization of LLMs in medical applications, particularly in scenarios with limited labeled data. This comprehensive reasoning demonstrates the feasibility and potential impact of the proposed solution."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles with the scarcity of labelled training data which limits the performance and generalisation of deep learning models despite advances in overcoming this challenge.",
      "method": "Instruction finetuning, especially with an increased number of tasks and chain-of-thought data, improves generalization capabilities.\n\n**Explanation:** By finetuning language models with instruction-based datasets and integrating chain-of-thought (CoT) data, models like Flan-PaLM are able to better understand and solve tasks by reasoning through step-by-step instructions. This enhances their ability to tackle unseen tasks, as they learn more flexible inferencing strategies via instructions and reasoning patterns.",
      "status": "SUCCESS",
      "title": "Adaptive Instruction Finetuning with Domain-Specific Data Augmentation for Low-Label Scenarios",
      "abstract": "Background: Deep learning models often struggle with the scarcity of labeled training data, which limits their performance and generalization capabilities. Instruction finetuning, combined with chain-of-thought reasoning, has shown promise in improving generalization by enabling models to learn flexible inferencing strategies. However, its application in low-label data scenarios remains underexplored. Gap: While instruction finetuning enhances generalization, it does not directly address the challenge of limited labeled data, which is critical for improving performance in resource-scarce domains. Proposed Method: We propose an adaptive instruction finetuning framework that integrates domain-specific data augmentation techniques to generate synthetic labeled data. By leveraging task-specific augmentation strategies and incorporating these into instruction-based finetuning with chain-of-thought reasoning, the model can better generalize to unseen tasks in low-label scenarios. Expected Result: This approach is expected to significantly enhance the performance and generalization of deep learning models in resource-scarce domains, bridging the gap between instruction finetuning and low-label data challenges.",
      "modification": "Integrate domain-specific data augmentation techniques into the instruction finetuning process to generate synthetic labeled data and enhance model training in low-label scenarios.",
      "reasoning": "Step 1: Compatibility Analysis: Instruction finetuning, particularly with chain-of-thought reasoning, is designed to enhance generalization by teaching models to reason through tasks step-by-step. This aligns well with the need for improved generalization in low-label data scenarios. However, the method does not inherently address the scarcity of labeled data, as it relies on the availability of diverse instruction-based datasets. Computationally, the method is feasible, as instruction finetuning has been successfully applied to large-scale models like Flan-PaLM. Thus, the method is compatible but requires adaptation to address the specific limitation of labeled data scarcity. Step 2: Gap Identification: The key gap lies in the lack of a mechanism to overcome the scarcity of labeled data. The bridging variable is the integration of domain-specific data augmentation techniques into the instruction finetuning process. By generating synthetic labeled data tailored to specific tasks or domains, the method can be adapted to address the limitation. This approach is inspired by evolutionary patterns in research, where domain-specific adaptations (e.g., BioBERT) have successfully addressed similar challenges. Step 3: Idea Drafting: The proposed method, 'Adaptive Instruction Finetuning with Domain-Specific Data Augmentation for Low-Label Scenarios,' combines instruction finetuning with task-specific data augmentation. The core innovation is the integration of augmentation strategies into the finetuning process, enabling models to learn from synthetic data and improve generalization in low-label scenarios. This builds on the evolutionary logic of adapting general methods to domain-specific challenges, as seen in the progression from BioBERT to domain-specific pretraining approaches.",
      "rationale": "The reasoning process begins with analyzing the compatibility of instruction finetuning with the limitation of labeled data scarcity. Instruction finetuning enhances generalization but does not directly address the lack of labeled data, necessitating adaptation. Drawing from evolutionary patterns, such as the adaptation of general NLP models to domain-specific challenges (e.g., BioBERT), we identify the integration of domain-specific data augmentation as the bridging variable. This modification allows the method to generate synthetic labeled data, addressing the limitation while leveraging the strengths of instruction finetuning. The proposed idea builds on this insight, presenting a framework that combines augmentation with finetuning to improve performance in low-label scenarios. The decision chain follows logically from the limitation (scarcity of labeled data) to the method's compatibility (instruction finetuning enhances generalization) to the gap (lack of data augmentation) and finally to the solution (adaptive finetuning with augmentation). The evidence from evolutionary paths supports this approach, as similar adaptations have successfully addressed related challenges in NLP research."
    },
    {
      "limitation": "**Limitations extracted from paper sections:**\n\n- Our method struggles with the scarcity of labelled training data which limits the performance and generalisation of deep learning models despite advances in overcoming this challenge.",
      "method": "Utilizing Natural Language Processing (NLP) methods such as text classification, topic modelling, event extraction, and text scaling to automate the extraction of relevant information from large volumes of text.\n\n**Explanation:** NLP methods provide computational tools to automatically analyze text and extract key information, such as sentiment, stance, and topics, which can inform policymakers. By automating the data processing, NLP reduces the reliance on manual coding and allows for efficient analysis of vast amounts of textual data. This enables policymakers to use evidence-based insights extracted from text data to make informed decisions.",
      "status": "SUCCESS",
      "title": "Semi-Supervised NLP Framework for Augmenting Labeled Data in Deep Learning Models",
      "abstract": "Background: Deep learning models often struggle with the scarcity of labeled training data, which limits their performance and generalization capabilities. This challenge persists despite advancements in data augmentation and transfer learning. Gap: Current NLP methods, while effective in automating text analysis, do not directly address the creation of high-quality labeled datasets to mitigate the labeled data scarcity problem. Proposed Method: We propose a semi-supervised NLP framework that leverages topic modeling, text classification, and event extraction to generate pseudo-labeled data from large unlabeled text corpora. By combining these pseudo-labels with a small set of manually labeled data, the framework employs a self-training loop to iteratively refine the model's performance. Expected Result: This approach is expected to significantly enhance the labeled dataset size and diversity, thereby improving the performance and generalization of deep learning models in low-resource settings.",
      "modification": "Introduce a semi-supervised learning mechanism that combines pseudo-label generation from NLP methods with iterative self-training to address labeled data scarcity.",
      "reasoning": "Step 1: Analyze Compatibility. The candidate method, which involves NLP techniques like text classification, topic modeling, and event extraction, is compatible with the limitation of labeled data scarcity. These methods can process large text corpora to extract meaningful patterns and information. However, the current formulation of the method focuses on automating information extraction rather than directly addressing the creation of labeled datasets. Step 2: Identify the Gap. The limitation requires a mechanism to generate or augment labeled datasets, while the candidate method focuses on extracting insights from text. The bridging variable is the introduction of a semi-supervised learning mechanism that uses NLP methods to generate pseudo-labels from unlabeled data. This pseudo-labeled data can then be iteratively refined through self-training. Step 3: Draft the Idea. Drawing inspiration from the evolutionary paths, particularly the use of domain-specific pretraining and iterative refinement in NLP, the proposed method adapts these principles to labeled data augmentation. The core innovation is the integration of semi-supervised learning with NLP techniques to address labeled data scarcity.",
      "rationale": "The reasoning follows a structured chain of thought: 1. Limitation: The scarcity of labeled training data limits the performance of deep learning models. 2. Compatibility: NLP methods like text classification and topic modeling can process large text corpora, making them suitable for generating pseudo-labels. However, the current method does not directly address labeled data creation. 3. Gap Identification: The key gap is the lack of a mechanism to generate labeled data. The bridging variable is the introduction of a semi-supervised learning framework that combines pseudo-label generation with iterative self-training. 4. Evolutionary Context: Learning from the evolution of domain-specific pretraining in NLP, the proposed method adapts the idea of iterative refinement and domain-specific adaptation to labeled data augmentation. 5. Proposed Solution: A semi-supervised NLP framework that generates pseudo-labeled data from large text corpora and refines it through self-training. 6. Expected Outcome: The method is expected to enhance the size and diversity of labeled datasets, improving the performance and generalization of deep learning models in low-resource settings. This reasoning is supported by the evolutionary patterns, where iterative refinement and domain-specific adaptations have successfully addressed similar challenges in NLP."
    }
  ],
  "pools": {
    "unsolved_limitations": 287,
    "candidate_methods": 25
  }
};

                // ÊåâÁ±ªÂûãÂàÜÁªÑËæπÊï∞ÊçÆ
                const edgesByType = {};
                edgesData.forEach(edge => {
                    if (!edgesByType[edge.type]) {
                        edgesByType[edge.type] = [];
                    }
                    edgesByType[edge.type].push(edge);
                });

                // ÂàõÂª∫ËäÇÁÇπËΩ®Ëøπ
                const nodeTrace = {
                    x: nodesData.map(n => n.x),
                    y: nodesData.map(n => n.y),
                    mode: 'markers+text',
                    marker: {
                        size: nodesData.map(n => n.size),
                        color: nodesData.map(n => n.color),
                        line: { width: 2, color: 'white' },
                        colorscale: 'Viridis'
                    },
                    text: nodesData.map(n => n.label),
                    textposition: 'middle center',
                    textfont: { size: 10, color: 'black' },
                    customdata: nodesData,
                    hovertemplate: '<b>%{customdata.title}</b><extra></extra>',
                    type: 'scatter',
                    name: 'Paper Nodes'
                };

                // Chart layout configuration
                const layout = {
                    title: '',
                    showlegend: false,
                    hovermode: 'closest',
                    margin: { l: 0, r: 0, b: 40, t: 0 },
                    xaxis: {
                        title: 'Publication Year',
                        showgrid: true,
                        gridcolor: 'lightgray',
                        range: [1979, 2026]
                    },
                    yaxis: {
                        title: 'Paper Distribution',
                        showgrid: true,
                        gridcolor: 'lightgray',
                        showticklabels: false
                    },
                    plot_bgcolor: 'white',
                    paper_bgcolor: 'white'
                };

                // ========== Utility Functions ==========
                // Create edge traces (generic function to eliminate duplicate logic)
                function createEdgeTraces(styleMap) {
                    const traces = [];
                    Object.keys(edgesByType).forEach(type => {
                        const edges = edgesByType[type];
                        const style = styleMap.get(type) || {
                            color: edges[0].color,
                            width: edges[0].width,
                            dash: 'solid'
                        };

                        const edgeX = [];
                        const edgeY = [];

                        edges.forEach(edge => {
                            const fromNode = nodesData.find(n => n.id === edge.from);
                            const toNode = nodesData.find(n => n.id === edge.to);
                            if (fromNode && toNode) {
                                edgeX.push(fromNode.x, toNode.x, null);
                                edgeY.push(fromNode.y, toNode.y, null);
                            }
                        });

                        if (edgeX.length > 0) {
                            traces.push({
                                x: edgeX,
                                y: edgeY,
                                mode: 'lines',
                                line: {
                                    width: style.width,
                                    color: style.color,
                                    dash: style.dash
                                },
                                hoverinfo: 'none',
                                showlegend: false,
                                type: 'scatter'
                            });
                        }
                    });
                    return traces;
                }

                // Update graph (generic function)
                function updateGraph(edgeTraces, nodeColors, nodeLineStyle = null) {
                    const nodeUpdate = {
                        ...nodeTrace,
                        marker: {
                            ...nodeTrace.marker,
                            color: nodeColors,
                            line: nodeLineStyle || { width: 2, color: 'white' }
                        }
                    };

                    Plotly.react('graph', [...edgeTraces, nodeUpdate], layout);
                }

                // ========== Initialize Graph ==========
                // Create initial edge styles (using originally defined colors)
                const initialEdgeStyle = new Map();
                Object.keys(edgesByType).forEach(type => {
                    const firstEdge = edgesByType[type][0];
                    initialEdgeStyle.set(type, {
                        color: firstEdge.original_color,
                        width: firstEdge.width,
                        dash: firstEdge.dash
                    });
                });

                const initialEdgeTraces = createEdgeTraces(initialEdgeStyle);
                updateGraph(initialEdgeTraces, nodesData.map(n => n.color));

                // ========== Event Handlers ==========
                document.getElementById('graph').on('plotly_click', function(data) {
                    if (data.points?.[0]?.customdata) {
                        const node = data.points[0].customdata;
                        const nodeIndex = data.points[0].pointIndex;
                        showPaperDetails(node);
                        highlightClickedNodeAndEdges(nodeIndex, node);
                    }
                });


                // ========== ÂäüËÉΩÂáΩÊï∞ ==========
                // Ê†áÁ≠æÈ°µÂàáÊç¢ÂáΩÊï∞
                function switchTab(event, tabId) {
                    // ÁßªÈô§ÊâÄÊúâactiveÁ±ª
                    document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));
                    document.querySelectorAll('.tab-content').forEach(content => content.classList.remove('active'));

                    // Ê∑ªÂä†activeÁ±ªÂà∞ÂΩìÂâçÊ†áÁ≠æ
                    event.target.classList.add('active');
                    document.getElementById(tabId).classList.add('active');

                    // ÂàáÊç¢Âà∞"üìÑ Paper Details"Êàñ"üí° Research Ideas"Ê†áÁ≠æÊó∂ÔºåÈáçÁΩÆÂõæË∞±È´ò‰∫Æ
                    if (tabId === 'paper-tab' || tabId === 'ideas-tab') {
                        resetGraphHighlight();
                        console.log(`Switched to ${tabId}, graph highlight reset`);
                    }

                    // Ê†πÊçÆÊ†áÁ≠æÈ°µIDÂä†ËΩΩÁõ∏Â∫îÂÜÖÂÆπ
                    if (tabId === 'survey-tab') {
                        renderDeepSurvey();
                    } else if (tabId === 'ideas-tab') {
                        renderResearchIdeas();
                    }
                }

                // Ê∏≤Êüìüìù Deep Survey (Êñ∞ÁâàÊï∞ÊçÆÁªìÊûÑ)
                function renderDeepSurvey() {
                    const surveyTab = document.getElementById('survey-tab');

                    if (!deepSurveyData || Object.keys(deepSurveyData).length === 0) {
                        surveyTab.innerHTML = '<div class="placeholder">No üìù Deep Survey data available</div>';
                        return;
                    }

                    let html = '<div style="padding:20px;">';

                    // ÊëòË¶Å‰ø°ÊÅØ (Êñ∞ÁªìÊûÑ)
                    if (deepSurveyData.summary) {
                        html += `
                            <div class="stats">
                                <h4 style="margin-top:0;">üìä Survey Summary</h4>
                                <div class="stat-item"><span>Original Papers:</span><span>${deepSurveyData.summary.original_papers || 0}</span></div>
                                <div class="stat-item"><span>Pruned Papers:</span><span>${deepSurveyData.summary.pruned_papers || 0}</span></div>
                                <div class="stat-item"><span>Evolutionary Threads:</span><span>${deepSurveyData.summary.total_threads || 0}</span></div>
                            </div>
                        `;
                    }

                    // Ââ™ÊûùÁªüËÆ°‰ø°ÊÅØ
                    if (deepSurveyData.pruning_stats) {
                        const stats = deepSurveyData.pruning_stats;
                        const retentionRate = (stats.retention_rate * 100).toFixed(1);
                        html += `
                            <div class="stats" style="margin-top:15px; background:#fff3cd;">
                                <h4 style="margin-top:0;">‚úÇÔ∏è Graph Pruning Statistics</h4>
                                <div class="stat-item"><span>Retention Rate:</span><span>${retentionRate}%</span></div>
                                <div class="stat-item"><span>Seed Papers:</span><span>${stats.seed_papers || 0}</span></div>
                                <div class="stat-item"><span>Strong Edges:</span><span>${stats.strong_edges || 0}</span></div>
                                <div class="stat-item"><span>Weak Edges Removed:</span><span>${stats.weak_edges_removed || 0}</span></div>
                            </div>
                        `;
                    }

                    // ÊºîÂåñË∑ØÂæÑ (Threads)
                    const threads = deepSurveyData.survey_report?.threads || deepSurveyData.evolutionary_paths || [];
                    if (threads.length > 0) {
                        html += `
                            <div style="display:flex; justify-content:space-between; align-items:center; margin-top:20px;">
                                <h3 style="color:#2c3e50; margin:0;">üßµ Key Evolutionary Threads</h3>
                                <button id="resetGraphBtn" onclick="resetGraphHighlight()"
                                    style="padding:5px 12px; background:#6c757d; color:white; border:none; border-radius:4px; cursor:pointer; font-size:12px;">
                                    üîÑ Reset Graph
                                </button>
                            </div>
                        `;
                        threads.forEach((thread, index) => {
                            const threadTitle = thread.title || thread.thread_name || `Thread ${index + 1}`;
                            const patternType = thread.pattern_type || thread.thread_type || 'Unknown Type';
                            const paperCount = thread.papers ? thread.papers.length : 0;
                            const narrative = thread.narrative || 'No narrative text available';

                            // ÂÆö‰πâ‰∏∞ÂØåÁöÑÈ¢úËâ≤Ë∞ÉËâ≤ÊùøÔºàÊåâÊïÖ‰∫ãÁ∫øÁ¥¢ÂºïÂàÜÈÖçÔºâ
                            const colorPalette = [
                                '#E74C3C',  // Á∫¢Ëâ≤ - Thread 0
                                '#3498DB',  // ËìùËâ≤ - Thread 1
                                '#2ECC71',  // ÁªøËâ≤ - Thread 2
                                '#F39C12',  // Ê©ôËâ≤ - Thread 3
                                '#9B59B6',  // Á¥´Ëâ≤ - Thread 4
                                '#1ABC9C',  // ÈùíËâ≤ - Thread 5
                                '#E67E22',  // Ê∑±Ê©ôËâ≤ - Thread 6
                                '#95A5A6',  // ÁÅ∞Ëâ≤ - Thread 7
                                '#34495E',  // Ê∑±ËìùÁÅ∞ - Thread 8
                                '#16A085'   // Ê∑±ÈùíËâ≤ - Thread 9
                            ];

                            // Ê†πÊçÆÊïÖ‰∫ãÁ∫øÁ¥¢ÂºïÂàÜÈÖçÈ¢úËâ≤Ôºà‰øùËØÅÊØèÊù°ÊïÖ‰∫ãÁ∫øÈ¢úËâ≤ÂîØ‰∏ÄÔºâ
                            let borderColor = colorPalette[index % colorPalette.length];
                            let highlightColor = borderColor;

                            // Êî∂ÈõÜËØ•ÊïÖ‰∫ãÁ∫øÁöÑÊâÄÊúâËÆ∫ÊñáID
                            const paperIds = thread.papers ? thread.papers.map(p => p.paper_id) : [];

                            html += `
                                <div class="epoch-card" style="border-left-color:${borderColor}; cursor:pointer; transition:all 0.3s;"
                                     onclick="highlightThread(${index}, '${highlightColor}')"
                                     onmouseover="this.style.backgroundColor='#f8f9fa'"
                                     onmouseout="this.style.backgroundColor='white'">
                                    <h4>
                                        ${threadTitle}
                                        <span style="float:right; font-size:12px; color:#666; font-weight:normal;">
                                            ${patternType}
                                        </span>
                                    </h4>
                                    <p><strong>üìö Number of Papers:</strong> ${paperCount}</p>
                                    ${thread.total_citations ? `<p><strong>üìä Total Citations:</strong> ${thread.total_citations}</p>` : ''}
                                    <p style="font-size:11px; color:#666; margin-top:8px;">
                                        üí° <em>Click this card to highlight all papers in this thread on the left graph</em>
                                    </p>

                                    <details style="margin-top:10px;">
                                        <summary style="cursor:pointer; color:#3498DB; font-weight:bold;">üìñ View Evolutionary Narrative</summary>
                                        <div style="margin-top:10px; padding:10px; background:#f8f9fa; border-radius:5px; line-height:1.6; white-space:pre-wrap;">
                                            ${narrative}
                                        </div>
                                    </details>

                                    ${thread.papers && thread.papers.length > 0 ? `
                                        <details style="margin-top:10px;">
                                            <summary style="cursor:pointer; color:#2ECC71; font-weight:bold;">üìÑ View Paper List</summary>
                                            <ul style="margin-top:10px; padding-left:20px;">
                                                ${thread.papers.map((p, pIndex) => `
                                                    <li style="margin:5px 0; cursor:pointer; color:#2980b9; transition:color 0.2s;"
                                                        onclick="event.stopPropagation(); showPaperFromThread('${p.paper_id}');"
                                                        onmouseover="this.style.color='#3498db'; this.style.textDecoration='underline';"
                                                        onmouseout="this.style.color='#2980b9'; this.style.textDecoration='none';"
                                                        title="Click to view üìÑ Paper Details and highlight in graph">
                                                        <strong>${p.title}</strong>
                                                        (${p.year || 'N/A'}, ÂºïÁî®: ${p.cited_by_count || 0})
                                                    </li>
                                                `).join('')}
                                            </ul>
                                        </details>
                                    ` : ''}
                                </div>
                            `;
                        });
                    }

                    // ÁªºËø∞Êä•ÂëäÊëòË¶Å
                    if (deepSurveyData.survey_report?.abstract) {
                        html += `
                            <div style="margin-top:20px; padding:15px; background:#e8f4f8; border-left:4px solid #3498DB; border-radius:5px;">
                                <h4 style="margin:0 0 10px 0; color:#2c3e50;">üìù Survey Abstract</h4>
                                <p style="line-height:1.6; color:#333; margin:0;">${deepSurveyData.survey_report.abstract}</p>
                            </div>
                        `;
                    }

                    html += '</div>';
                    surveyTab.innerHTML = html;
                }

                // Ê∏≤Êüìüí° Research Ideas
                function renderResearchIdeas() {
                    const ideasTab = document.getElementById('ideas-tab');

                    if (!researchIdeasData || Object.keys(researchIdeasData).length === 0) {
                        ideasTab.innerHTML = '<div class="placeholder">No üí° Research Ideas data available</div>';
                        return;
                    }

                    let html = '<div style="padding:20px;">';

                    // ÁªüËÆ°‰ø°ÊÅØ
                    html += `
                        <div class="stats">
                            <h4 style="margin-top:0;">üí° Idea Generation Statistics</h4>
                            <div class="stat-item"><span>Total Ideas:</span><span>${researchIdeasData.total_ideas || 0}</span></div>
                            <div class="stat-item"><span>Successful Ideas:</span><span>${researchIdeasData.successful_ideas || 0}</span></div>
                            ${researchIdeasData.pools ? `
                                <div class="stat-item"><span>Unsolved Limitations:</span><span>${researchIdeasData.pools.unsolved_limitations || 0}</span></div>
                                <div class="stat-item"><span>Candidate Methods:</span><span>${researchIdeasData.pools.candidate_methods || 0}</span></div>
                            ` : ''}
                        </div>
                    `;

                    // ÂàõÊÑèÂàóË°®
                    if (researchIdeasData.ideas && researchIdeasData.ideas.length > 0) {
                        html += '<h3 style="color:#2c3e50; margin-top:20px;">üí° Research Ideas List</h3>';
                        researchIdeasData.ideas.forEach((idea, index) => {
                            const statusClass = idea.status === 'SUCCESS' ? 'status-success' : 'status-incompatible';
                            const statusText = idea.status === 'SUCCESS' ? '‚úì Feasible' : '‚úó Incompatible';

                            html += `
                                <div class="idea-card">
                                    <h4>
                                        Idea ${index + 1}: ${idea.title || 'Unnamed Idea'}
                                        <span class="status-badge ${statusClass}">${statusText}</span>
                                    </h4>
                                    ${idea.abstract ? `
                                        <p style="margin:10px 0; line-height:1.6; color:#444;">
                                            <strong>Abstract:</strong> ${idea.abstract}
                                        </p>
                                    ` : ''}
                                    ${idea.modification ? `
                                        <p style="margin:8px 0; padding:10px; background:#f8f9fa; border-radius:4px;">
                                            <strong>üîß Key Innovation:</strong> ${idea.modification}
                                        </p>
                                    ` : ''}
                                    ${idea.reasoning ? `
                                        <details style="margin-top:10px;">
                                            <summary style="cursor:pointer; color:#3498DB;"><strong>View reasoning process</strong></summary>
                                            <p style="margin-top:8px; font-size:12px; color:#666; white-space:pre-wrap;">${idea.reasoning}</p>
                                        </details>
                                    ` : ''}
                                </div>
                            `;
                        });
                    }

                    html += '</div>';
                    ideasTab.innerHTML = html;
                }

                function showPaperDetails(node) {
                    const authorsText = node.authors.slice(0, 5).join(', ') +
                                      (node.authors.length > 5 ? ' etc.' : '');

                    // ÊûÑÂª∫RAGÂàÜÊûêÈÉ®ÂàÜÁöÑHTML
                    let ragAnalysisHTML = '';
                    if (node.rag_problem || node.rag_method || node.rag_limitation || node.rag_future_work) {
                        ragAnalysisHTML = `
                            <div class="paper-info" style="background:#e8f4f8; padding:15px; border-radius:8px; margin-top:15px;">
                                <h4 style="margin:0 0 10px 0; color:#1a73e8; font-size:15px;">üß† In-depth Analysis of Multi-Agent Systems</h4>
                                ${node.analysis_method ? `<p style="font-size:12px; color:#666; margin-bottom:10px;"><strong>Analysis Method:</strong> ${node.analysis_method.toUpperCase()}</p>` : ''}
                                ${node.sections_extracted ? `<p style="font-size:12px; color:#666; margin-bottom:10px;"><strong>Extracted Sections:</strong> ${node.sections_extracted} sections</p>` : ''}
                            </div>
                            ${node.rag_problem ? `
                            <div class="paper-info" style="border-left:3px solid #FF6B6B; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#FF6B6B; font-size:14px;">üìã Problem</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_problem}</p>
                            </div>
                            ` : ''}
                            ${node.rag_method ? `
                            <div class="paper-info" style="border-left:3px solid #4ECDC4; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#4ECDC4; font-size:14px;">üí° Method</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_method}</p>
                            </div>
                            ` : ''}
                            ${node.rag_limitation ? `
                            <div class="paper-info" style="border-left:3px solid #FFA500; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#FFA500; font-size:14px;">‚ö†Ô∏è Limitation</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_limitation}</p>
                            </div>
                            ` : ''}
                            ${node.rag_future_work ? `
                            <div class="paper-info" style="border-left:3px solid #9B59B6; padding-left:10px;">
                                <h4 style="margin:0 0 8px 0; color:#9B59B6; font-size:14px;">üîÆ Future Work</h4>
                                <p style="font-size:13px; line-height:1.6; color:#333;">${node.rag_future_work}</p>
                            </div>
                            ` : ''}
                        `;
                    }

                    document.getElementById('paper-tab').innerHTML = `
                        <div class="stats">
                            <h4 style="margin-top:0;">Graph Statistics</h4>
                            <div class="stat-item"><span>Total Papers:</span><span>226</span></div>
                            <div class="stat-item"><span>Citation Relationships:</span><span>282</span></div>
                            <div class="stat-item"><span>Time Span:</span><span>1980 - 2025</span></div>
                        </div>
                        <div class="paper-info">
                            <h3>${node.title}</h3>
                            <p><strong>Authors:</strong> ${authorsText}</p>
                            <p><strong>Year:</strong> ${node.year}</p>
                            <p><strong>Cited by:</strong> ${node.cited_by_count}</p>
                            <p><strong>Journal/Conference:</strong> ${node.venue || 'Unknown'}</p>
                            <p><strong>Paper ID:</strong> ${node.id}</p>
                        </div>
                        ${ragAnalysisHTML}
                    `;
                }

                function highlightClickedNodeAndEdges(nodeIndex, node) {
                    // Âè™ÊîπÂèòËäÇÁÇπÈ¢úËâ≤ - Ë¢´ÁÇπÂáªÁöÑËäÇÁÇπÈ´ò‰∫Æ
                    const nodeColors = nodesData.map((n, i) =>
                        i === nodeIndex ? '#FF4444' : n.color);

                    // Ëæπ‰øùÊåÅÂéüÂßãÊ†∑Âºè‰∏çÂèò
                    updateGraph(initialEdgeTraces, nodeColors);
                }

                // ========== ÊºîÂåñÊïÖ‰∫ãÁ∫øÈ´ò‰∫ÆÂäüËÉΩ ==========
                function highlightThread(threadIndex, highlightColor) {
                    // Ëé∑ÂèñÁ∫øÁ®ãÊï∞ÊçÆ
                    const threads = deepSurveyData.survey_report?.threads || deepSurveyData.evolutionary_paths || [];
                    if (threadIndex >= threads.length) return;

                    const thread = threads[threadIndex];
                    const threadPaperIds = new Set(thread.papers?.map(p => p.paper_id) || []);

                    console.log(`Highlighting Thread ${threadIndex}: ${thread.title}, containing ${threadPaperIds.size} papers`);

                    // Êõ¥Êñ∞ËäÇÁÇπÈ¢úËâ≤ÂíåÂ§ßÂ∞è
                    const newColors = [];
                    const newSizes = [];
                    const newLineStyles = [];

                    nodesData.forEach((node, index) => {
                        if (threadPaperIds.has(node.id)) {
                            // È´ò‰∫ÆÊòæÁ§∫Ôºö‰øùÊåÅËäÇÁÇπÂéüÊú¨È¢úËâ≤ÔºåÊîæÂ§ß1.5ÂÄç
                            newColors.push(node.color);
                            newSizes.push(node.size * 1.5);
                            newLineStyles.push({ width: 3, color: node.color });
                        } else {
                            // ÂÖ∂‰ªñËäÇÁÇπÔºöÂèòÁÅ∞ÔºåÁº©Â∞èÂà∞0.5ÂÄç
                            newColors.push('#D3D3D3');
                            newSizes.push(node.size * 0.5);
                            newLineStyles.push({ width: 1, color: '#CCCCCC' });
                        }
                    });

                    // Êõ¥Êñ∞ÂõæË∞±
                    const highlightedNodeTrace = {
                        ...nodeTrace,
                        marker: {
                            ...nodeTrace.marker,
                            size: newSizes,
                            color: newColors,
                            line: newLineStyles
                        }
                    };

                    // Ëæπ‰πüË∞ÉÊï¥ÈÄèÊòéÂ∫¶ÔºàÈ´ò‰∫ÆÊïÖ‰∫ãÁ∫øÂÜÖÁöÑËæπÔºâ
                    const highlightedEdgeTraces = createEdgeTracesWithHighlight(threadPaperIds, highlightColor);

                    Plotly.react('graph', [...highlightedEdgeTraces, highlightedNodeTrace], layout);

                    // ÊªöÂä®ÂõæË∞±Âà∞È´ò‰∫ÆÂå∫Âüü
                    document.getElementById('graph').scrollIntoView({ behavior: 'smooth', block: 'center' });
                }

                function createEdgeTracesWithHighlight(highlightedNodeIds, highlightColor) {
                    const traces = [];
                    Object.keys(edgesByType).forEach(type => {
                        const edges = edgesByType[type];
                        const style = initialEdgeStyle.get(type) || {
                            color: edges[0].color,
                            width: edges[0].width,
                            dash: 'solid'
                        };

                        // ÂàÜÁ¶ªÈ´ò‰∫ÆËæπÂíåÈùûÈ´ò‰∫ÆËæπ
                        const highlightedEdgeX = [];
                        const highlightedEdgeY = [];
                        const dimmedEdgeX = [];
                        const dimmedEdgeY = [];

                        edges.forEach(edge => {
                            const fromNode = nodesData.find(n => n.id === edge.from);
                            const toNode = nodesData.find(n => n.id === edge.to);
                            if (fromNode && toNode) {
                                // Ê£ÄÊü•ÊòØÂê¶ÊòØÈ´ò‰∫ÆÊïÖ‰∫ãÁ∫øÁöÑËæπ
                                const isHighlighted = highlightedNodeIds.has(edge.from) && highlightedNodeIds.has(edge.to);

                                if (isHighlighted) {
                                    highlightedEdgeX.push(fromNode.x, toNode.x, null);
                                    highlightedEdgeY.push(fromNode.y, toNode.y, null);
                                } else {
                                    dimmedEdgeX.push(fromNode.x, toNode.x, null);
                                    dimmedEdgeY.push(fromNode.y, toNode.y, null);
                                }
                            }
                        });

                        // Ê∑ªÂä†È´ò‰∫ÆËæπtraceÔºà‰øùÊåÅÂéüÂßãÈ¢úËâ≤ÔºåÂè™Âä†Á≤óÔºâ
                        if (highlightedEdgeX.length > 0) {
                            traces.push({
                                x: highlightedEdgeX,
                                y: highlightedEdgeY,
                                mode: 'lines',
                                line: {
                                    width: style.width * 1.8,
                                    color: style.color,  // ‰ΩøÁî®ÂéüÂßãÈ¢úËâ≤Ôºå‰∏çÊîπÂèò
                                    dash: style.dash
                                },
                                opacity: 1.0,
                                hoverinfo: 'none',
                                showlegend: false,
                                type: 'scatter'
                            });
                        }

                        // Ê∑ªÂä†ÂèòÁÅ∞Ëæπtrace
                        if (dimmedEdgeX.length > 0) {
                            traces.push({
                                x: dimmedEdgeX,
                                y: dimmedEdgeY,
                                mode: 'lines',
                                line: {
                                    width: style.width * 0.4,
                                    color: '#E0E0E0',
                                    dash: style.dash
                                },
                                opacity: 0.2,
                                hoverinfo: 'none',
                                showlegend: false,
                                type: 'scatter'
                            });
                        }
                    });
                    return traces;
                }

                function resetGraphHighlight() {
                    console.log('Resetting graph highlight');
                    // ÊÅ¢Â§çÂéüÂßãÈ¢úËâ≤ÂíåÂ§ßÂ∞è
                    updateGraph(initialEdgeTraces, nodesData.map(n => n.color));
                }

                // ‰ªéüìù Deep SurveyÁöÑËÆ∫ÊñáÂàóË°®‰∏≠ÁÇπÂáªËÆ∫ÊñáÔºåÊòæÁ§∫ËØ¶ÊÉÖÂπ∂È´ò‰∫Æ
                function showPaperFromThread(paperId) {
                    console.log('Clicked paper from üìù Deep Survey:', paperId);

                    // Êü•ÊâæÂØπÂ∫îÁöÑËäÇÁÇπÁ¥¢Âºï
                    const nodeIndex = nodesData.findIndex(n => n.id === paperId);

                    if (nodeIndex === -1) {
                        console.warn('Êú™Âú®ÂõæË∞±‰∏≠ÊâæÂà∞ËÆ∫Êñá:', paperId);
                        alert('ËØ•ËÆ∫Êñá‰∏çÂú®ÂΩìÂâçÊòæÁ§∫ÁöÑÂõæË∞±ËäÇÁÇπ‰∏≠');
                        return;
                    }

                    const node = nodesData[nodeIndex];

                    // ÂàáÊç¢Âà∞üìÑ Paper DetailsÊ†áÁ≠æÈ°µ
                    const paperTab = document.querySelector('.tab[onclick*="paper-tab"]');
                    if (paperTab) {
                        paperTab.click();
                    }

                    // ÊòæÁ§∫üìÑ Paper Details
                    showPaperDetails(node);

                    // Êü•ÊâæËØ•ËÆ∫ÊñáÊâÄÂ±ûÁöÑÊºîÂåñÊïÖ‰∫ãÁ∫ø
                    const threads = deepSurveyData.survey_report?.threads || deepSurveyData.evolutionary_paths || [];
                    let threadIndex = -1;

                    for (let i = 0; i < threads.length; i++) {
                        const thread = threads[i];
                        const paperIds = thread.papers?.map(p => p.paper_id) || [];
                        if (paperIds.includes(paperId)) {
                            threadIndex = i;
                            break;
                        }
                    }

                    // È´ò‰∫ÆÊï¥‰∏™ÊïÖ‰∫ãÁ∫øÔºàËäÇÁÇπ‰øùÊåÅÂéüËâ≤Ôºâ
                    if (threadIndex !== -1) {
                        console.log(`This paper belongs to thread ${threadIndex}, highlighting the entire thread`);
                        highlightThread(threadIndex, null);  // ‰º†nullÂõ†‰∏∫‰∏çÂÜçÈúÄË¶ÅhighlightColor
                    } else {
                        // Â¶ÇÊûúÊ≤°ÊúâÊâæÂà∞ÊïÖ‰∫ãÁ∫øÔºåÂõûÈÄÄÂà∞ÂçïËäÇÁÇπÈ´ò‰∫Æ
                        console.log('This paper does not belong to any thread, highlighting single node only');
                        highlightClickedNodeAndEdges(nodeIndex, node);
                    }

                    console.log('Displayed üìÑ Paper Details and highlighted node:', node.title);
                }


                function updateHoverPosition(event) {
                    const hoverDiv = document.getElementById('hoverTitle');
                    if (hoverDiv) {
                        hoverDiv.style.left = (event.clientX + 10) + 'px';
                        hoverDiv.style.top = (event.clientY - 30) + 'px';
                    }
                }
            </script>
            
        </body>
        </html>
        